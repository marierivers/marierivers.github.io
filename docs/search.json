[
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Posts",
    "section": "",
    "text": "The Importance of Meaningfully Open Snow Data\n\n\n\n\n\n\nopen science\n\n\n\nCommunicating climate and water resource data to a wider audience\n\n\n\n\n\nJun 7, 2022\n\n\nMarie Rivers\n\n\n\n\n\n\n\n\n\n\n\n\nHow to make a dumbbell schedule with R\n\n\n\n\n\n\nR\n\n\ndata visualization\n\n\n\nA modern update to the Gantt chart\n\n\n\n\n\nFeb 8, 2022\n\n\nMarie Rivers\n\n\n\n\n\n\n\n\n\n\n\n\nDoes Environmental Quality Influence Where People Live?\n\n\n\n\n\n\nR\n\n\n\nPopulation Change vs. the EPA Environmental Quality Index (EQI): a statistical analysis\n\n\n\n\n\nDec 2, 2021\n\n\nMarie Rivers\n\n\n\n\n\n\n\n\n\n\n\n\nHow I made this visualization\n\n\n\n\n\n\nR\n\n\ndata visualization\n\n\n\nVisualization of Alaska household languages data\n\n\n\n\n\nNov 3, 2021\n\n\nMarie Rivers\n\n\n\n\n\n\n\n\n\n\n\n\nDo you learn about the chicken or the egg first\n\n\n\n\n\n\nPython\n\n\nR\n\n\nMEDS\n\n\n\nIf you are new to coding, what first steps should you take?\n\n\n\n\n\nOct 17, 2021\n\n\nMarie Rivers\n\n\n\n\n\n\n\n\n\n\n\n\nA New Perspective on Data\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nMEDS\n\n\n\nlife beyond excel spreadsheets\n\n\n\n\n\nAug 18, 2012\n\n\nMarie Rivers\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "presentations/power_outage_presentation.html#background",
    "href": "presentations/power_outage_presentation.html#background",
    "title": "Houston Power Outages",
    "section": "Background",
    "text": "Background\n\n\nIn February 2021 the Houston, TX metropolitan area experienced wide scale power outages due to electrical infrastructure failure during winter storms and extreme cold temperatures\n\n\n\n1.4 million customers without power"
  },
  {
    "objectID": "presentations/power_outage_presentation.html#approach",
    "href": "presentations/power_outage_presentation.html#approach",
    "title": "Houston Power Outages",
    "section": "Approach",
    "text": "Approach\nUse geospatial and statistical methods to quantify:\n\nNumber of residential homes without power\nSocioeconomic differences of areas with and without power\n\n…by using data from:\n\nSatellite imagery of nighttime lights\nOpenStreetMaps roadways and buidings\nCensus tract level race, age and income variables"
  },
  {
    "objectID": "presentations/power_outage_presentation.html#project-overview",
    "href": "presentations/power_outage_presentation.html#project-overview",
    "title": "Houston Power Outages",
    "section": "Project overview",
    "text": "Project overview\n\n\n\n\nLoad satellite imagery\n\n\n\n\nCreate blackout mask\n\n\n\n\nIdentify residential buildings within blackout area\n\n\n\n\nSpatially join census data to blackout areas\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n+144,000 households without power"
  },
  {
    "objectID": "presentations/power_outage_presentation.html#section",
    "href": "presentations/power_outage_presentation.html#section",
    "title": "Houston Power Outages",
    "section": "",
    "text": "Percent white\n\n\n\n\n\nPercent bipoc"
  },
  {
    "objectID": "presentations/power_outage_presentation.html#statistical-analysis",
    "href": "presentations/power_outage_presentation.html#statistical-analysis",
    "title": "Houston Power Outages",
    "section": "Statistical Analysis",
    "text": "Statistical Analysis\nLinear regression models of percent of households without power vs. census variables\n\n\n\n\n\n\n\nRace\n\npercent white\npercent black\npercent Native American\npercent Asian\npercent Hispanic / Latino\n\n\n\nAge\n\n65 and older\nchildren under 18\n\n\n\nIncome\n\npercent households below poverty\nmedian income"
  },
  {
    "objectID": "presentations/power_outage_presentation.html#statistical-analysis-1",
    "href": "presentations/power_outage_presentation.html#statistical-analysis-1",
    "title": "Houston Power Outages",
    "section": "Statistical Analysis",
    "text": "Statistical Analysis\n\n\ngraph\ncode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# linear regression model\nmodel_pct_white &lt;- lm(data = blackout_census_data, pct_houses_that_lost_power ~ pct_white)\n\n#plot\nplot_model_pct_white &lt;- ggplot(data = blackout_census_data, aes(x = pct_white, y = pct_houses_that_lost_power)) +\n  geom_point(size = 0.5) +\n  geom_smooth(method = lm, formula = y~x, se = FALSE) +\n  theme_classic() +\n  labs(x = \"% white\", y = \"% of houses that lost power\",\n       title = \"Linear regression of % households without power vs. % population white\")\nplot_model_pct_white"
  },
  {
    "objectID": "presentations/power_outage_presentation.html#statistical-analysis-2",
    "href": "presentations/power_outage_presentation.html#statistical-analysis-2",
    "title": "Houston Power Outages",
    "section": "Statistical Analysis",
    "text": "Statistical Analysis\n\n\ngraph\ncode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# linear regression model\nmodel_pct_black &lt;- lm(data = blackout_census_data, pct_houses_that_lost_power ~ pct_black)\n\n# plot\nplot_model_pct_black &lt;- ggplot(data = blackout_census_data, aes(x = pct_black, y = pct_houses_that_lost_power)) +\n  geom_point(size = 0.5) +\n  geom_smooth(method = lm, formula = y~x, se = FALSE) +\n  theme_classic() +\n  labs(x = \"% black\", y = \"% of houses that lost power\",\n       title = \"Linear regression of % households without power vs. % population black\")\nplot_model_pct_black"
  },
  {
    "objectID": "presentations/power_outage_presentation.html#results",
    "href": "presentations/power_outage_presentation.html#results",
    "title": "Houston Power Outages",
    "section": "Results",
    "text": "Results"
  },
  {
    "objectID": "presentations/power_outage_presentation.html#conclusions",
    "href": "presentations/power_outage_presentation.html#conclusions",
    "title": "Houston Power Outages",
    "section": "Conclusions",
    "text": "Conclusions\n\n\n\nWhile race, age and income accounted for small portions of the overall variance in residential power outages, this analysis suggests some racial and economic inequality.\nElectric utilities should evaluate infrastructure and asset management plans in areas with higher proportions of people of color and poverty\nThere is a need for more equitable responses to natural disasters\n\n\n image source1\n\n\n\n\nhttps://marierivers.github.io/blackout_analysis/\n\n\nhttps://appliedsciences.nasa.gov/our-impact/news/extreme-winter-weather-causes-us-blackouts"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "posts/2021-12-02-environmental-quality-population-change/index.html",
    "href": "posts/2021-12-02-environmental-quality-population-change/index.html",
    "title": "Does Environmental Quality Influence Where People Live?",
    "section": "",
    "text": "This post discusses a statistical analysis used to answer the question: Does environmental quality influence where people live in the United States?"
  },
  {
    "objectID": "posts/2021-12-02-environmental-quality-population-change/index.html#background",
    "href": "posts/2021-12-02-environmental-quality-population-change/index.html#background",
    "title": "Does Environmental Quality Influence Where People Live?",
    "section": "Background",
    "text": "Background\nThe Environmental Quality Index (EQI), developed by the U.S. Environmental Protection Agency (EPA) provides a county level snapshop of environmental conditions throughout the country. EPA first released EQIs for the period 2000-2005 and updated these indexes for 2006-2010. This statistical evaluation focuses on the 2006-2010 EQI. The purpose of the EQI is to use (1) as an indicator of ambient conditions/exposure in environmental health and modeling and (2) as a covariate to adjust for ambient conditions in environmental models (EPA 2020). Previous studies have used the EQI to evaluate relationships between environmental quality and public health outcomes such as cancer incidence, asthma, obesity, and infant mortality.\nThe EQI is developed from five domains each with identified environmental constructs as shown in Table 1. Each county has an overall environmental index and a domain specific index. Indexes were also stratified by rural-urban continuum codes (RUCCs) for counties classified as metropolitan urbanized, non-metro urbanized, less urbanized, and thinly populated.\n\n\n\nEQI Environmental Domains and Constructs\n\n\nDomain\nConstructs\n\n\n\n\nair\ncriteria air pollutants and hazardous air pollutants\n\n\nwater\noverall water quality, general water contamination, domestic use, atmospheric deposition, drought, chemical contamination, and drinking water quality\n\n\nland\nagriculture, pesticides, facilities, radon, and mining activity\n\n\nbuilt\nroads, highway/road safety, commuting behavior, housing environment, walkability, and green space\n\n\nsociodemographic\ncrime, socioeconomic, political character, and creative class representation\n\n\n\n\n\n\n\n\nLimitations\nWhile the EQI can identify counties with higher environmental burdens, it may not identify environmental injustices at the local community level. The EQI cannot quantify environmental exposure for individuals and reflects only outside environmental conditions, not indoor conditions. The EQI can be used to identify locations for future research, but is not intended for regulatory purposes or as a diagnostics tool. Due to changes in methodology and datasets, the 2000-2005 and 2006-2010 EQIs should not be directly compared."
  },
  {
    "objectID": "posts/2021-12-02-environmental-quality-population-change/index.html#the-data",
    "href": "posts/2021-12-02-environmental-quality-population-change/index.html#the-data",
    "title": "Does Environmental Quality Influence Where People Live?",
    "section": "The Data",
    "text": "The Data\n\nEnvironmental Quality Index\nTo develop the EQI, variables were identified from available data to represent each environmental domain and assessed for collinearity so redundant variables could be excluded. Variables were standardized based on geographic space or on a per capita rate, as appropriate and transformations such as log-transformations were performed as needed based on the normality of each variable. Data gaps were evaluated to distinguish between missing data and meaningful zeros. Where applicable, spatial kriging was used to interpolate values when data was not available for all counties. Principal component analysis was used to aggregate variables into domain specific indexes. The domain indexes were then aggregated into overall indexes for each county. A result of this method is that each domain does not equally influence the overall EQI value for a given county. The EQI is developed to be normally distributed with mean=0 and standard deviation=1. Higher EQI values correspond with worse environmental quality. Lower (more negative) EQI values correspond with better environmental quality.\n\n\nCensus Population Data\nCounty level population data was obtained from the U.S. Census Bureau’s county intercensal datasets for 2000-2010. Percent population change was calculated for 2006-2010 then winsorized to remove outliers above the 99.9th percentile (ie. counties with population change above 28.5%)."
  },
  {
    "objectID": "posts/2021-12-02-environmental-quality-population-change/index.html#statistical-analysis",
    "href": "posts/2021-12-02-environmental-quality-population-change/index.html#statistical-analysis",
    "title": "Does Environmental Quality Influence Where People Live?",
    "section": "Statistical Analysis",
    "text": "Statistical Analysis\nKalawao County, Hawaii had the lowest EQI value (highest environmental quality) and greatest decrease in population (-17.4%). Falls Church, Virginia had the highest EQI (lowest environmental quality) and a population change of 15.4% which falls is the 99th percentile. The summary statistics in Table 2 suggest that population growth is correlated with worse environmental characteristics.\n\n\n\nEQI summary statistics for counties based on population change between 2006 and 2010\n\n\nPopulation Change\nMin\nMax\nMean\nStandard Deviation\nVariance\nNumber of Counties\n\n\n\n\nnegative\n-5.88\n2.59\n-0.24\n0.93\n0.87\n1088\n\n\npositive\n-5.05\n2.85\n0.13\n1.01\n1.02\n2052\n\n\nall counties\n-5.88\n2.85\n0.00\n1.00\n1.00\n3140\n\n\n\n\n\n\n\n\nHypothesis Testing\nA different means test was completed to determine if mean environmental quality was statistically different for counties that experienced positive vs. negative population change between 2006-2010. The figure below shows a histogram of EQI values for counties with negative and positive population change.\n\n\n\n\n\n\n\n\n\nnull hypothesis: There is no difference in mean EQI for counties with positive and negative population change.\n\\[H_{0}: \\mu_{posPopChange} - \\mu_{negPopChange} = 0\\]\nalternative hypothesis: There is a difference in mean EQI for counties with positive and negative population change.\n\\[H_{A}: \\mu_{posPopChange} - \\mu_{negPopChange} \\neq 0\\]\n\\[\\text{point estimate} = \\mu_{posPopChange} - \\mu_{negPopChange} =0.131 - -0.245 = 0.376\\]\nThe standard error for the difference in means is:\n\\[SE = \\sqrt{\\frac{s_1^2}{n_1} + \\frac{s^2_2}{n_2}} = \\sqrt{\\frac{1.009^2}{2052} + \\frac{0.933^2}{1088}} = 0.036\\]\nThe z-score for hypothesis testing is:\n\\[z = \\frac{\\text{point estimate - null}}{SE} = \\frac{0.376 - 0}{0.036} = 10.443\\]\nThe p-value, the probability of getting a point estimate at least as extreme as calculated if the null hypothesis were true, is:\n\\[p\\text {-value }=\\operatorname{Pr}(Z&lt;-|z| \\text { or } Z&gt;|z|)=2 * \\operatorname{Pr}(Z&gt;|z|) = 1.5809578\\times 10^{-25}\\]\nSince the p-value is &lt; 0.001 we reject the null that there is no difference in EQI for counties with positive population change versus negative population change. There is a statistically significant difference (at the 0.1% significance level) in EQI across the two population change groups. The 95% confidence interval ranges from 0.31 to 0.45. This means that there is a 95% chance that this interval includes the true difference in mean EQI between counties with positive and negative percent population change.\n\n\nLinear Regression\nLinear regression was used to model the relationship between population change and environmental quality using the overall EQI value and each domain specific EQI to determine if a particular domain was a stronger predictor of population change.\n\\[\\text{percent population change}_i=\\beta_{0}+\\beta_{1} \\cdot EQI_i + \\varepsilon_i\\]\n\n\n\n\n\n\n\n\n\nFirst, hypothesis testing was used to test whether the slope coefficient for the percent population change rate is equal to zero or not.\nnull hypothesis: The slope coefficient is equal to zero\n\\[H_{0}: \\beta_{1} = 0\\] alternative hypothesis: The slope coefficient is NOT equal to zero\n\\[H_{A}: \\beta_{1} \\neq 0\\] ::: {.cell}\n:::\nThe point estimate, \\(\\beta_1\\) = 0.991 and the standard error, SE = 0.077.\n\\[z = \\frac{\\text{point estimate - null}}{SE} = \\frac{{0.991 - 0}}{0.077} = 12.817\\]\n\\[p\\text {-value }=\\operatorname{Pr}(Z&lt;-|z| \\text { or } Z&gt;|z|)=2 * \\operatorname{Pr}(Z&gt;|z|) = 1.0849376\\times 10^{-36}\\]\nSince the p-value for the slope coefficient was &lt; 0.001, we reject the null hypothesis that EQI has no influence on population change at the 0.1% level. There is a statistically significant relationship between EQI and percent population change and the coefficient is significantly different from zero. Based on value of \\(\\beta_1\\), for each one unit increase in EQI, the percent population change increases by 0.991. The 95% confidence interval for the slope coefficient ranges from 0.84 to 1.143. This means that there is a 95% chance that this interval includes the true county level rate of change for percent population change for each one unit change in EQI.\n\n\nDomain Specific Linear Models\n\\[\\text{percent population change}_i=\\beta_{0,domain}+\\beta_{1,domain} \\cdot EQI_{i,domain} + \\varepsilon_i\\] ::: {.cell}\n:::\n\n\n\n\n\n\n\n\n\nWhile a partly manual method was used above, statistical functions in R were used to test for the significance of domain models. Table 3 presents coefficients for each domain specific model. The numbers in [brackets] are the 95% confidence intervals for each estimated coefficient.\n\n\n\nSummary of EQI Domain Slope Coefficients\n\n\ncoefficient\noverall EQI\nair model\nwater model\nland model\nbuilt model\nsociodem model\n\n\n\n\nIntercept\n1.868 ***\n1.869 ***\n1.868 ***\n1.868 ***\n1.867 ***\n1.868 ***\n\n\n\n[1.716, 2.019]\n[1.717, 2.020]\n[1.714, 2.023]\n[1.713, 2.024]\n[1.713, 2.021]\n[1.718, 2.019]\n\n\nEQI\n0.991 ***\n\n\n\n\n\n\n\n\n[0.840, 1.143]\n\n\n\n\n\n\n\nair EQI\n\n0.998 ***\n\n\n\n\n\n\n\n\n[0.847, 1.150]\n\n\n\n\n\n\nwater EQI\n\n\n0.485 ***\n\n\n\n\n\n\n\n\n[0.331, 0.640]\n\n\n\n\n\nland EQI\n\n\n\n-0.314 ***\n\n\n\n\n\n\n\n\n[-0.469, -0.158]\n\n\n\n\nbuilt EQI\n\n\n\n\n0.635 ***\n\n\n\n\n\n\n\n\n[0.481, 0.790]\n\n\n\nsociodem EQI\n\n\n\n\n\n1.068 ***\n\n\n\n\n\n\n\n\n[0.917, 1.219]\n\n\nn\n3140\n3140\n3140\n3140\n3140\n3140\n\n\nR2\n0.050\n0.051\n0.012\n0.005\n0.020\n0.058\n\n\n\n\n\n\n\nThe figure below provides a visual comparison of each model result. The bold portion of the line represents the 90% confidence interval and the full line represents the 95% confidence interval for each estimate.\n\n\n\n\n\n\n\n\n\nThe p-value on the slope coefficient was &lt; 0.001 for all domain specific linear models which indicates a statistically significant relationship at the 0.01% level. Based on the \\(R^2\\) values and slope coefficients, the air and sociodemographic domains account for most of the overall relationship between population change and EQI. All domains except land are positively correlated with population change. Since higher EQI values indicate poorer environmental quality, these models show that population increased more in counties with worse environmental conditions. For a one unit increase in sociodemogrpahic EQI, the percent population change increases by 1.068. For a one unit increase in air EQI, the percent population change increases by 0.998. The \\(R^2\\) terms represent the variance in percent population change that can be explained by EQI. For the overall EQI value, 5% of the variance in percent population change is explained by environmental conditions. The sociodemographic EQI explains 5.8% of the variance in population change while the air EQI explains 5.1%."
  },
  {
    "objectID": "posts/2021-12-02-environmental-quality-population-change/index.html#conclusions",
    "href": "posts/2021-12-02-environmental-quality-population-change/index.html#conclusions",
    "title": "Does Environmental Quality Influence Where People Live?",
    "section": "Conclusions",
    "text": "Conclusions\nThe identified relationships between population change and environmental quality are noteworthy for their public health and environmental justice implications. Positive population trends in areas with worse environmental conditions could result in increased incidences of cancer, asthma, obesity, and infant mortality. While this project did not evaluate economic variables, locations with higher environmental quality could also have higher living costs which drive people to move to more affordable places. If economic factors contribute to population growth in counties with poor environmental quality, then this could negatively affect the health of vulnerable populations and perpetuate social inequalities. Further analysis could evaluate trends in mean household income to determine if there is growing income inequality between counties with better and worse environmental quality. Economic variables and other factors influencing demographic shifts from rural areas to cities may be stronger predictors of population change than environmental quality.\nData availability:\nEPA Datasets and files from the EQI county data from 2006-2010\nU.S. Census Bureau County Intercensal Tables: 2000-2010"
  },
  {
    "objectID": "posts/2021-12-02-environmental-quality-population-change/index.html#references",
    "href": "posts/2021-12-02-environmental-quality-population-change/index.html#references",
    "title": "Does Environmental Quality Influence Where People Live?",
    "section": "References",
    "text": "References\n\nU.S. EPA. Environmental Quality Index - Technical Report (2006-2010) (Final, 2020). U.S. Environmental Protection Agency, Washington, DC, EPA/600/R-20/367, 2020.\nU.S. Census Bureau. County Intercensal Datasets: 2000-2010. https://www.census.gov/data/datasets/time-series/demo/popest/intercensal-2000-2010-counties.html"
  },
  {
    "objectID": "posts/2021-08-18-a-new-perspective-on-data/index.html",
    "href": "posts/2021-08-18-a-new-perspective-on-data/index.html",
    "title": "A New Perspective on Data",
    "section": "",
    "text": "Throughout my career as an environmental engineer, the bulk of my experience with data had been with excel spreadsheets and modeling software. Spreadsheets were my primary tool to complete bulk calculations and create graphs. I’ve collected many tips and tricks and consider my excel skills to be very strong. When I want to try something I’ve never tried before, I have a reasonably good intuition for if it’s something excel is capable of and how to find the solution using help features or internet searches.\nDuring college I used excel for everything inside and outside of class. My classmates and I would joke about using solver to answer everything from mundane domestic questions to meaning of life type queries. While I haven’t used solver since finding the optimal 2012 US Olympic gymnastics team based on various scoring scenarios, spreadsheets are still prevalent in my life:\n\n– Planning a group vacation…shared Google sheet!\n– Outlining an epic fantasy novel…no need for scrivener\n– Any major life decision…better back that choice up with excel\n\nAnd my works days were full of engineering calcs, budgets, cost estimates, asset management analyses, and graphs. From summary statistics, if statements, and well placed $ signs to conditional formatting and pivot tables, I thought my data storage, analysis, and visualization skills were top of their game…until I discovered the Masters of Environmental Data Science (MEDS) program at UC Santa Barbara’s Bren school of Environmental Science & Management. I started to get a hunch that there were other alternatives when working with data.\nSince starting the MEDS program, I’ve begun to expand my data storage, analysis, and visualization outlook. I was very impressed with an article we read in class Data Organization in Spreadsheet by Karl W. Broman & Kara H. Woo. (Karl W. Broman & Kara H. Woo (2018) Data Organization in Spreadsheets, The American Statistician, 72:1, 2-10, DOI: 10.1080/00031305.2017.1375989). This paper articulates many of the logistical challenges I’ve faced with data and presented useful recommendations. I’ve worked with or created spreadsheets where each of the basic spreadsheet principles were not used.\nI have received and passed along complicated spreadsheets with equations linked to cells in tabs throughout the workbook. While my coworkers and I attempted to used consistent file names (even if only consistent with their own files) these efforts often fall short when unexpected complexities arise or deadlines get close. When you are desperately trying to get a deliverable out the door, intermediate file names are the least of your priorities. Challenges associated with combining or separating dates and addresses and entering zip codes or ID numbers that begin with 0 were expected parts of data manipulation. I’ve received data with no documentation for column headers, acronyms, or units; from these experiences I began to use perhaps overly lengthy variable names, often with a top row merged over several variables.\nThe concept of Tidy data was completely knew to me. At first, this data structure seemed drawn-out and a little redundant, but as we’ve worked with more data in class I now see the advantages. Taking the time to use functions such as pivot_wider and pivot_longer to get data in Tide format ultimately gives your subsequent analysis more flexibility and ease.\nAs I continue with the MEDS program, I’m looking forward to gaining the same intuition for R and Python that I have with excel. I’m sure this journey will be frustrating and time consuming, but know that learning new ways to store, analyze, and visual data will be rewarding!\nP.S. I never fully appreciated CSV files\n\n\n\nCitationBibTeX citation:@online{rivers2012,\n  author = {Rivers, Marie},\n  title = {A {New} {Perspective} on {Data}},\n  date = {2012-08-18},\n  url = {https://marierivers.github.io/posts/2021-08-18-a-new-perspective-on-data/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nRivers, Marie. 2012. “A New Perspective on Data.” August\n18, 2012. https://marierivers.github.io/posts/2021-08-18-a-new-perspective-on-data/."
  },
  {
    "objectID": "posts/2021-10-17-do-you-learn-about-the-chicken-or-the-egg-first/index.html",
    "href": "posts/2021-10-17-do-you-learn-about-the-chicken-or-the-egg-first/index.html",
    "title": "Do you learn about the chicken or the egg first",
    "section": "",
    "text": "When I decided to apply for an Environmental Data Science program I had zero coding experience. I knew that Python was a common language, but had never heard about R until learning about the UCSB Environmental Data Science program. Before starting classes I tried to learn more about this new world I’d be entering. I enrolled in an Intro to Programming class that focused on Python. In the first class we learned background info and advantages/disadvantages of C, Java, and Python, what a compiler and an interpreter are, general python syntax, and different data types (integers/floats/Booleans/strings). Over the next few weeks, we received formal definitions for value, variable, constant, expression, data strictures (arrays, queues, stacks, lists) and logical operators…All this info was useful, but we didn’t actually jump into much coding. The class dedicated more time to theoretical content than hands on coding.\nA big take away from the class wasn’t any specific coding knowledge, but rather an idea of the general mindset required to successfully learn how to code. I experienced the problem solving component of coding and the potentially frustrating cycle trying, failing, trying again, tweaking, consulting other references, and approaching from a different direction. More important than the lessons, I confirmed that I enjoy this type of work, my mind thinks in the logical way that code often requires, and I have the patience to work through debugging. An aspiring coder can’t be daunted when things don’t work the first, second, or third time. I also completed enough Codecademy R tutorials to confirm that yep, this coding thing was for me. I skimmed a few reference docs and watched a few intro R YouTube videos, but I didn’t have a great understanding of how R was used beyond tutorial world. A lot of this early context didn’t stick.\nOnce I arrived at UCSB, we started using R (and a little Python) straight away. From the beginning, our classes emphasized the importance of reproducible workflows, documenting code with comments, and version control. We spent more time typing code than just looking at code. Our classes include the right amount of repetition to retain essential content and appreciate its use in a variety of contexts.\nThere is no “right” way to being to learn how to code. Starting is the most important part. Don’t try to gain some arbitrary minimum knowledge base before starting. The first time you use a new package or function, you’re not going to remember all the details. The first times you read reference books, package documentations or blog posts they won’t make much sense. The more you learn, the more your capacity to ingest new information will increase. You will never know everything, but you will gain a larger understanding of what is possible (and where to go to learn how to do it). AND…you’ll start to understand R jokes. The best way to learn is by doing. This grad program has a good balance of formal instruction and, more importantly, plenty of opportunities to figure things out for yourself.\n\n\n\nCitationBibTeX citation:@online{rivers2021,\n  author = {Rivers, Marie},\n  title = {Do You Learn about the Chicken or the Egg First},\n  date = {2021-10-17},\n  url = {https://marierivers.github.io/posts/2021-10-17-do-you-learn-about-the-chicken-or-the-egg-first/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nRivers, Marie. 2021. “Do You Learn about the Chicken or the Egg\nFirst.” October 17, 2021. https://marierivers.github.io/posts/2021-10-17-do-you-learn-about-the-chicken-or-the-egg-first/."
  },
  {
    "objectID": "code_samples/protecting-whales-from-ships/index.html",
    "href": "code_samples/protecting-whales-from-ships/index.html",
    "title": "Protecting Whales from Ships",
    "section": "",
    "text": "In the waters off the Caribbean island nation of Dominica, whale habitat and marine traffic overlap spatially, putting whale populations in danger. This analysis identifies a speed reduction zone off the island of Dominica for the purpose of reducing the occurrence of ships striking whales and quantifies the impact of reduced travel speeds on marine traffic."
  },
  {
    "objectID": "code_samples/protecting-whales-from-ships/index.html#load-data",
    "href": "code_samples/protecting-whales-from-ships/index.html#load-data",
    "title": "Protecting Whales from Ships",
    "section": "Load data",
    "text": "Load data\n\n# Join folder path and filename \nfp3 = os.path.join(input_folder2, \"station1249.csv\")\n\n\n# Read file using gpd.read_file()\nvessels = gpd.read_file(fp3)\n\n\ntype(vessels)\n\n&lt;class 'geopandas.geodataframe.GeoDataFrame'&gt;\n\n\n\nvessels.head()\n\n  field_1       MMSI        LON       LAT            TIMESTAMP geometry\n0       0  233092000  -61.84788  15.23238  2015-05-22 13:53:26     None\n1       1  255803280  -61.74397  15.96114  2015-05-22 13:52:57     None\n2       2  329002300  -61.38968  15.29744  2015-05-22 13:52:32     None\n3       3  257674000  -61.54395   16.2334  2015-05-22 13:52:24     None\n4       4  636092006  -61.52401  15.81954  2015-05-22 13:51:23     None\n\n\nSimilar to the whale sighting data, set the geometry and coordinate reference system for the vessel data.\n\n# bootstrap the geometries\nvessel_points = gpd.points_from_xy(vessels['LON'], vessels['LAT'])\nvessel_gdf = gpd.GeoDataFrame(vessels, geometry=vessel_points)\n\n\n# project the dataset into an appropriate CRS\nvessel_gdf = vessel_gdf.set_crs(epsg=4326)\nvessel_gdf = vessel_gdf.to_crs(epsg=proj_area_crs)\n\n\nvessel_gdf.crs\n\n&lt;Derived Projected CRS: EPSG:2002&gt;\nName: Dominica 1945 / British West Indies Grid\nAxis Info [cartesian]:\n- E[east]: Easting (metre)\n- N[north]: Northing (metre)\nArea of Use:\n- name: Dominica - onshore.\n- bounds: (-61.55, 15.14, -61.2, 15.69)\nCoordinate Operation:\n- name: British West Indies Grid\n- method: Transverse Mercator\nDatum: Dominica 1945\n- Ellipsoid: Clarke 1880 (RGS)\n- Prime Meridian: Greenwich\n\n\n\nvessel_gdf['TIMESTAMP'] = pd.to_datetime(vessel_gdf['TIMESTAMP'])\n\n\nvessel_gdf.head()\n\n  field_1       MMSI  ...           TIMESTAMP                        geometry\n0       0  233092000  ... 2015-05-22 13:53:26  POINT (415373.315 1683307.035)\n1       1  255803280  ... 2015-05-22 13:52:57  POINT (426434.345 1763918.193)\n2       2  329002300  ... 2015-05-22 13:52:32  POINT (464555.392 1690588.725)\n3       3  257674000  ... 2015-05-22 13:52:24  POINT (447770.634 1794068.620)\n4       4  636092006  ... 2015-05-22 13:51:23  POINT (450006.361 1748297.844)\n\n[5 rows x 6 columns]\n\n\n\n# plot vessel points against Dominca outline and speed reduction zone\nbase = dominica.plot(facecolor='none', edgecolor='black', linewidth=3, figsize=(15, 15))\nvessel_gdf.plot(ax=base, markersize = 3)\nspeed_reduction_zone.plot(ax=base, edgecolor='red', linewidth=2)\n\n\n\n\n\n\n\n\n\n# spatially subset AIS data to only include vessels within identified whale habitat\nvessels_in_whale_habitat = vessel_gdf.sjoin(speed_reduction_zone, how=\"inner\")\nvessels_in_whale_habitat\n\n       field_1       MMSI  ...                        geometry index_right\n2            2  329002300  ...  POINT (464555.392 1690588.725)           0\n7            7  338143127  ...  POINT (463892.452 1694650.397)           0\n13          13  329002300  ...  POINT (464555.389 1690589.831)           0\n15          15  338143015  ...  POINT (463910.683 1694655.978)           0\n16          16  338143127  ...  POINT (463697.964 1694341.275)           0\n...        ...        ...  ...                             ...         ...\n617252  238717  329002300  ...  POINT (453901.647 1709712.916)           0\n617253  238718  338143015  ...  POINT (463915.972 1694683.643)           0\n617255  238720  338143127  ...  POINT (463905.177 1694705.734)           0\n617259  238724  377907247  ...  POINT (464023.288 1691613.624)           0\n617261  238726  329002300  ...  POINT (454741.236 1707161.130)           0\n\n[167411 rows x 7 columns]"
  },
  {
    "objectID": "code_samples/protecting-whales-from-ships/index.html#calculate-distance-and-speed",
    "href": "code_samples/protecting-whales-from-ships/index.html#calculate-distance-and-speed",
    "title": "Protecting Whales from Ships",
    "section": "Calculate distance and speed",
    "text": "Calculate distance and speed\n\n# plot of only vessel points within speed reduction zone\nbase = dominica.plot(facecolor='none', linewidth=3, figsize=(15, 15))\nspeed_reduction_zone.plot(ax=base, facecolor='none', edgecolor='red', linewidth=3)\nvessels_in_whale_habitat.plot(ax=base, markersize = 0.5, facecolor='black')\nctx.add_basemap(ax=base, crs=dominica.crs.to_string())\nplt.show()\n\n\n\n\n\n\n\n\n\n# sort vessel dataframe by MMSI and time\nvessels_in_whale_habitat = vessels_in_whale_habitat.sort_values(by=['MMSI', 'TIMESTAMP'])\nvessels_in_whale_habitat\n\n       field_1       MMSI  ...                        geometry index_right\n235025  235025  203106200  ...  POINT (462476.396 1680935.224)           0\n235018  235018  203106200  ...  POINT (462283.995 1681393.698)           0\n235000  235000  203106200  ...  POINT (461936.769 1682722.187)           0\n234989  234989  203106200  ...  POINT (461798.818 1683708.377)           0\n234984  234984  203106200  ...  POINT (461654.150 1683997.765)           0\n...        ...        ...  ...                             ...         ...\n259103  259103  983191049  ...  POINT (465250.372 1690066.434)           0\n259094  259094  983191049  ...  POINT (465243.965 1690054.249)           0\n258954  258954  983191049  ...  POINT (465226.597 1690121.667)           0\n258930  258930  983191049  ...  POINT (465242.895 1690053.140)           0\n258206  258206  983191049  ...  POINT (465272.964 1690049.908)           0\n\n[167411 rows x 7 columns]\n\n\n\n# create a copy of the vessel dataframe and shift each observation down one row using `shift()`\nvessels_shift = vessels_in_whale_habitat.copy(deep=True).shift(periods=1)\n\n/Users/marierivers/Library/r-miniconda/lib/python3.9/site-packages/geopandas/array.py:1406: UserWarning: CRS not set for some of the concatenation inputs. Setting output's CRS as Dominica 1945 / British West Indies Grid (the single non-null crs provided).\n  warnings.warn(\n\n\n\n# rename shifted column names\nvessels_shift = vessels_shift.rename(columns={\"field_1\": \"field_1_shift\", \"MMSI\": \"MMSI_shift\", \"LON\": \"LON_shift\", \"LAT\": \"LAT_shift\", \"TIMESTAMP\": \"TIMESTAMP_shift\", \"geometry\": \"geometry_shift\", \"index_right\": \"index_right_shift\"})\n\n\n# join original dataframe with the shifted copy using `join()`\nvessels_shift_join = vessels_in_whale_habitat.join(vessels_shift).sort_values(by=['MMSI', 'TIMESTAMP'])\n\n\n# drop all rows in the joined dataframe in which the MMSI of the left is not the same as the one on the right\nvessels_keep = vessels_shift_join.drop(vessels_shift_join[vessels_shift_join['MMSI'] != vessels_shift_join['MMSI_shift']].index)\n\n\n# set the geometry column\nvessels_keep = vessels_keep.set_geometry(\"geometry\")\nvessels_keep2 = vessels_keep.set_geometry(\"geometry_shift\")\n\n\n# calculate distance between each observation\nvessels_keep['distance_m'] = vessels_keep.distance(vessels_keep2)\n\n\n# calculate time difference between each observation to the next\nvessels_keep['time'] = vessels_keep['TIMESTAMP'] - vessels_keep['TIMESTAMP_shift']\n\n\n# calculate speed\nmeters_per_nm = 1852\n\nvessels_keep['speed_m_per_sec'] = vessels_keep['distance_m'] / vessels_keep['time'].dt.total_seconds()\nvessels_keep['speed_knots'] = vessels_keep['speed_m_per_sec'] * 60 * 60 / meters_per_nm\nvessels_keep['time_10knots_minutes'] = (vessels_keep['distance_m'] * 60 ) / ( meters_per_nm * 10 )\nvessels_keep['time_dif_minutes'] = vessels_keep['time_10knots_minutes'] - (vessels_keep['time'].dt.total_seconds() / 60 )\nvessels_keep\n\n       field_1       MMSI  ... time_10knots_minutes time_dif_minutes\n235018  235018  203106200  ...             1.610828        -0.889172\n235000  235000  203106200  ...             4.448540        -3.034793\n234989  234989  203106200  ...             3.226109        -1.773891\n234984  234984  203106200  ...             1.048164        -1.468503\n234972  234972  203106200  ...             1.394116        -3.589217\n...        ...        ...  ...                  ...              ...\n259103  259103  983191049  ...             0.043139        -5.940194\n259094  259094  983191049  ...             0.044599        -4.355401\n258954  258954  983191049  ...             0.225548       -55.824452\n258930  258930  983191049  ...             0.228202       -11.471798\n258206  258206  983191049  ...             0.097976      -248.585358\n\n[166255 rows x 20 columns]\n\n\n\nvessels_keep = vessels_keep.sort_values(by=['speed_knots'], ascending=False)\n\n\n# look at the vessels that would be affected by the speed reduction zone\nvessels_going_too_fast = vessels_keep.drop(vessels_keep[vessels_keep['time_dif_minutes'] &lt; 0].index)\nvessels_going_too_fast\n\n       field_1       MMSI  ... time_10knots_minutes time_dif_minutes\n585844  207309  341387000  ...             0.209101         0.209101\n67091    67091  227528210  ...             7.979167         6.245834\n66925    66925  228008600  ...            13.481637        10.498303\n499754  121219  329002300  ...             8.728323         6.711656\n546817  168282  329002300  ...            12.866650         9.849984\n...        ...        ...  ...                  ...              ...\n616429  237894  636091437  ...             0.000000         0.000000\n616427  237892  636091437  ...             0.000000         0.000000\n616422  237887  636091437  ...             0.000000         0.000000\n616419  237884  636091437  ...             0.000000         0.000000\n616408  237873  636091437  ...             0.000000         0.000000\n\n[21410 rows x 20 columns]\n\n\n\nshipping_impact_minutes = vessels_going_too_fast['time_dif_minutes'].sum()\nshipping_impact_days = round(shipping_impact_minutes / ( 60 * 24), 2)\nshipping_impact_days\n\n27.88\n\n\nA 10-knot reduced speed zone in the identified whale habitat will increase travel time by approximately 27.88 days."
  },
  {
    "objectID": "code_samples/buoy_data/index.html",
    "href": "code_samples/buoy_data/index.html",
    "title": "Buoy Data",
    "section": "",
    "text": "This analysis…\n\n# import Python packages\nimport pandas as pd\nimport numpy as np\nimport datetime\nimport time\nimport calendar\nimport netCDF4\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nfrom matplotlib import gridspec\nfrom matplotlib import cm\n\n\nRead data and metadata\n\nstn = '071'\n# CDIP Archived Dataset URL\ndata_url = 'http://thredds.cdip.ucsd.edu/thredds/dodsC/cdip/archive/' + stn + 'p1/' + stn + 'p1_historic.nc'\ndata = netCDF4.Dataset(data_url)\n\n\ndata\n\n&lt;class 'netCDF4._netCDF4.Dataset'&gt;\nroot group (NETCDF3_CLASSIC data model, file format DAP2):\n    naming_authority: edu.ucsd.cdip\n    keywords_vocabulary: Global Change Master Directory (GCMD) Earth Science Keywords\n    date_created: 2024-11-08T23:29:52Z\n    date_issued: 2024-11-08T23:29:52Z\n    date_modified: 2024-11-08T23:29:52Z\n    creator_name: Coastal Data Information Program, SIO/UCSD\n    creator_url: http://cdip.ucsd.edu\n    creator_email: www@cdip.ucsd.edu\n    creator_institution: Scripps Institution of Oceanography, UCSD\n    creator_country: USA\n    creator_sector: academic\n    publisher_name: Coastal Data Information Program, SIO/UCSD\n    publisher_url: http://cdip.ucsd.edu\n    publisher_email: www@cdip.ucsd.edu\n    publisher_country: USA\n    publisher_institution: Scripps Institution of Oceanography, UCSD\n    institution: Scripps Institution of Oceanography, University of California San Diego\n    project: Coastal Data Information Program (CDIP)\n    processing_level: QA/QC information available at http://cdip.ucsd.edu/documentation\n    standard_name_vocabulary: CF Standard Name Table v79\n    Conventions: ACDD-1.3, CF-1.8, IOOS-1.2\n    license: These data may be redistributed and used without restriction.\n    cdm_data_type: Station\n    featureType: timeSeries\n    ncei_template_version: NCEI_NetCDF_TimeSeries_Orthogonal_Template_v2.0\n    references: http://cdip.ucsd.edu/documentation\n    uuid: FE88B271-A3A2-4DB2-93EB-B9E8BBAB6DCE\n    title: Directional wave and sea surface temperature measurements collected in situ by Datawell Waverider buoys located near HARVEST, CA from 1991/10/22 to 2024/11/08.\n    summary: Directional wave and sea surface temperature measurements collected in situ by Datawell Waverider buoys located near HARVEST, CA from 1991/10/22 to 2024/11/08. This dataset includes publicly-released data only, excluding all records flagged bad by quality control procedures. A total of 466717 wave samples were analyzed for this area, where the water depth is approximately 183 to 558 meters.\n    keywords: EARTH SCIENCE, OCEANS, OCEAN WAVES, GRAVITY WAVES, WIND WAVES, SIGNIFICANT WAVE HEIGHT, WAVE FREQUENCY, WAVE PERIOD, WAVE SPECTRA, OCEAN TEMPERATURE, SEA SURFACE TEMPERATURE, WATER TEMPERATURE, OCEAN, PACIFIC OCEAN, EASTERN PACIFIC OCEAN\n    id: CDIP_071p1_19911022-20241108_historic\n    history: 2024-11-08T23:29:52Z: dataset created; user, program, arguments: uproc, wnc_append_to_hist v1.0, 071. If date_modified is after date_created, contact CDIP for details of changes.\n    comment: Multiple deployments may be included in this dataset. Please refer to the per-deployment datasets - as referenced by sourceFilename and the xxxSourceIndex variables - for more complete metadata. All values are decoded directly from the instruments in accordance with the manufacturers documentation EXCEPT for those with the attribute :additional_processing which describes further data handling performed by CDIP.\n    acknowledgment: CDIP is primarily supported by the U.S. Army Corps of Engineers (USACE). Station partner: CDBW ; Field operator: CDIP\n    metadata_link: http://cdip.ucsd.edu/metadata/071p1\n    infoUrl: http://cdip.ucsd.edu/metadata/071p1\n    contributor_name: CDIP, CDBW/USACE\n    contributor_role: station operation, station funding\n    geospatial_lat_min: 34.440964\n    geospatial_lat_max: 34.486687\n    geospatial_lat_units: degrees_north\n    geospatial_lat_resolution: 1e-04\n    geospatial_lon_min: -120.7961\n    geospatial_lon_max: -120.685455\n    geospatial_lon_units: degrees_east\n    geospatial_lon_resolution: 1e-04\n    geospatial_vertical_min: 0.0\n    geospatial_vertical_max: 0.0\n    geospatial_vertical_units: meters\n    geospatial_vertical_origin: sea surface\n    geospatial_vertical_positive: up\n    geospatial_vertical_resolution: 1.0\n    time_coverage_start: 1991-10-22T07:00:00Z\n    time_coverage_end: 2024-11-08T16:59:59Z\n    time_coverage_duration: P1724W3DT09H\n    time_coverage_resolution: PT30M\n    source: insitu observations\n    platform: metaPlatform\n    instrument: metaInstrumentation\n    DODS.strlen: 0\n    DODS.dimName: metaStationNameLength\n    dimensions(sizes): dwrTime(468936), gpsTime(433684), maxStrlen64(64), metaBoundsCount(2), metaDeployCount(23), sourceCount(23), sstTime(466624), waveFrequency(64), waveTime(466717)\n    variables(dimensions): |S1 sourceFilename(sourceCount, maxStrlen64), int32 waveTime(waveTime), int32 waveTimeBounds(waveTime, metaBoundsCount), int8 waveFlagPrimary(waveTime), int8 waveFlagSecondary(waveTime), float32 waveHs(waveTime), float32 waveTp(waveTime), float32 waveTa(waveTime), float32 waveDp(waveTime), float32 wavePeakPSD(waveTime), float32 waveTz(waveTime), int32 waveSourceIndex(waveTime), float32 waveFrequency(waveFrequency), float32 waveFrequencyBounds(waveFrequency, metaBoundsCount), int8 waveFrequencyFlagPrimary(waveFrequency), int8 waveFrequencyFlagSecondary(waveFrequency), float32 waveBandwidth(waveFrequency), int32 sstTime(sstTime), int32 sstTimeBounds(sstTime, metaBoundsCount), int8 sstFlagPrimary(sstTime), int8 sstFlagSecondary(sstTime), float32 sstSeaSurfaceTemperature(sstTime), int32 sstSourceIndex(sstTime), float32 sstReferenceTemp(sstTime), int32 gpsTime(gpsTime), int32 gpsTimeBounds(gpsTime, metaBoundsCount), int8 gpsStatusFlags(gpsTime), float32 gpsLatitude(gpsTime), float32 gpsLongitude(gpsTime), int32 gpsSourceIndex(gpsTime), int32 dwrTime(dwrTime), int32 dwrTimeBounds(dwrTime, metaBoundsCount), int32 dwrSourceIndex(dwrTime), int32 dwrBatteryLevel(dwrTime), float32 dwrZAccelerometerOffset(dwrTime), float32 dwrXAccelerometerOffset(dwrTime), float32 dwrYAccelerometerOffset(dwrTime), float32 dwrOrientation(dwrTime), float32 dwrInclination(dwrTime), int32 dwrBatteryWeeksOfLife(dwrTime), float32 metaDeployLatitude(metaDeployCount), float32 metaDeployLongitude(metaDeployCount), float32 metaWaterDepth(metaDeployCount), float32 metaDeclination(metaDeployCount), |S1 metaStationName(maxStrlen64), float32 metaStationLatitude(), float32 metaStationLongitude(), |S1 metaPlatform(maxStrlen64), |S1 metaInstrumentation(maxStrlen64), |S1 metaGridMapping(maxStrlen64), float32 waveEnergyDensity(waveTime, waveFrequency), float32 waveMeanDirection(waveTime, waveFrequency), float32 waveA1Value(waveTime, waveFrequency), float32 waveB1Value(waveTime, waveFrequency), float32 waveA2Value(waveTime, waveFrequency), float32 waveB2Value(waveTime, waveFrequency), float32 waveCheckFactor(waveTime, waveFrequency), float32 waveSpread(waveTime, waveFrequency), float32 waveM2Value(waveTime, waveFrequency), float32 waveN2Value(waveTime, waveFrequency)\n    groups: \n\n\n\ndata.summary\n\n'Directional wave and sea surface temperature measurements collected in situ by Datawell Waverider buoys located near HARVEST, CA from 1991/10/22 to 2024/11/08. This dataset includes publicly-released data only, excluding all records flagged bad by quality control procedures. A total of 466717 wave samples were analyzed for this area, where the water depth is approximately 183 to 558 meters.'\n\n\n\n# return all variables included in the dataset\nprint(data.variables.keys())\n\ndict_keys(['sourceFilename', 'waveTime', 'waveTimeBounds', 'waveFlagPrimary', 'waveFlagSecondary', 'waveHs', 'waveTp', 'waveTa', 'waveDp', 'wavePeakPSD', 'waveTz', 'waveSourceIndex', 'waveFrequency', 'waveFrequencyBounds', 'waveFrequencyFlagPrimary', 'waveFrequencyFlagSecondary', 'waveBandwidth', 'sstTime', 'sstTimeBounds', 'sstFlagPrimary', 'sstFlagSecondary', 'sstSeaSurfaceTemperature', 'sstSourceIndex', 'sstReferenceTemp', 'gpsTime', 'gpsTimeBounds', 'gpsStatusFlags', 'gpsLatitude', 'gpsLongitude', 'gpsSourceIndex', 'dwrTime', 'dwrTimeBounds', 'dwrSourceIndex', 'dwrBatteryLevel', 'dwrZAccelerometerOffset', 'dwrXAccelerometerOffset', 'dwrYAccelerometerOffset', 'dwrOrientation', 'dwrInclination', 'dwrBatteryWeeksOfLife', 'metaDeployLatitude', 'metaDeployLongitude', 'metaWaterDepth', 'metaDeclination', 'metaStationName', 'metaStationLatitude', 'metaStationLongitude', 'metaPlatform', 'metaInstrumentation', 'metaGridMapping', 'waveEnergyDensity', 'waveMeanDirection', 'waveA1Value', 'waveB1Value', 'waveA2Value', 'waveB2Value', 'waveCheckFactor', 'waveSpread', 'waveM2Value', 'waveN2Value'])\n\n\n\n# learn more about a variable including long name, units, valid min/max values\nprint(data['sstSeaSurfaceTemperature'])\n\n&lt;class 'netCDF4._netCDF4.Variable'&gt;\nfloat32 sstSeaSurfaceTemperature(sstTime)\n    long_name: sea surface temperature\n    units: Celsius\n    _FillValue: -999.99\n    standard_name: sea_surface_temperature\n    coordinates: metaStationLatitude metaStationLongitude\n    grid_mapping: metaGridMapping\n    valid_min: -5.0\n    valid_max: 46.15\n    ancillary_variables: sstFlagPrimary sstFlagSecondary\n    ncei_name: SEA SURFACE TEMPERATURE\n    cell_methods: sstTime: point\nunlimited dimensions: \ncurrent shape = (466624,)\nfilling off\n\n\n\n# Get SST timestamp variable \nsst_time_var = data.variables['sstTime']\n\n# Get SST variable \nsst = data.variables['sstSeaSurfaceTemperature'][:]\n\n# Get wave height timestamp variable\nwave_time_var = data.variables['waveTime']\n\n# Get wave height variable \nwave = data.variables['waveHs'][:]\n\n\n# Use num2date on sst_time_var\nsst_time = netCDF4.num2date(sst_time_var[:], sst_time_var.units, only_use_cftime_datetimes=False)\n\n\n# Make an empty pandas dataframe\nsst_df = pd.DataFrame()\n\n# Fill it with SST and the date time it was collected\nsst_df['date_time'] = sst_time\nsst_df['sst'] = sst\nsst_df\n\n                 date_time        sst\n0      1991-10-22 08:21:00  16.000000\n1      1991-10-22 08:51:00  16.000000\n2      1991-10-22 09:21:00  15.900000\n3      1991-10-22 09:51:00  15.900000\n4      1991-10-22 10:21:00  15.850000\n...                    ...        ...\n466619 2024-11-08 14:58:20  13.950001\n466620 2024-11-08 15:28:20  14.000000\n466621 2024-11-08 15:58:20  14.000000\n466622 2024-11-08 16:28:20  14.049999\n466623 2024-11-08 16:58:20  14.100000\n\n[466624 rows x 2 columns]\n\n\n\n\n\n\n\n\nCitationBibTeX citation:@online{rivers,\n  author = {Rivers, Marie},\n  title = {Buoy {Data}},\n  url = {https://marierivers.github.io/code_samples/buoy_data/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nRivers, Marie. n.d. “Buoy Data.” https://marierivers.github.io/code_samples/buoy_data/."
  },
  {
    "objectID": "code_samples/blackout-analysis-with-python-and-r/index.html",
    "href": "code_samples/blackout-analysis-with-python-and-r/index.html",
    "title": "Blackout Analysis with Python and R",
    "section": "",
    "text": "This analysis evaluates the spatial distribution of power outages in the area surrounding Houston, Texas caused by winter storms in February 2021 and explores which socioeconomic variables are most correlated with power outages. Code and outputs for the geospatial and statistical analyses can be view from the navigation bar at the top of this page."
  },
  {
    "objectID": "code_samples/blackout-analysis-with-python-and-r/index.html#read-in-data",
    "href": "code_samples/blackout-analysis-with-python-and-r/index.html#read-in-data",
    "title": "Blackout Analysis with Python and R",
    "section": "Read in data",
    "text": "Read in data\nRead in all data layers and clip to the region on interest.\n\nSatellite imagery\nNighttime light data was obtained from the Visible Infrared Imaging Radiometer Suite (VIIRS), located on the Suomi NPP satellite, which uses a low-light sensor to measure light emissions and reflections. Specifically, the daily at-sensor top of atmosphere (TOA) nighttime radiance (VNP46A1) product was used. This data is available at a 500 meter geographic linear latitude/longitude grid resolution. The processed data accounts for nighttime cloud masks, solar/viewing/lunar geometry values, aerosols, and snow cover. This analysis used the day/night band DNB_At_Sensor_Radiance_500m dataset within the VNP46A1 product.\nImagery from February 7th and February 16th was used to visualize pre- and post-storm conditions due to a lack of cloud cover. Imagery from two tiles was needed to cover the area of interest, which resulted in a total of four files used in the analysis.\n\nVNP46A1.A2021038.h08v05.001.2021039064328.h5\nVNP46A1.A2021038.h08v06.001.2021039064329.h5\nVNP46A1.A2021047.h08v05.001.2021048091106.h5\nVNP46A1.A2021047.h08v06.001.2021048091105.h5\n\n\n\n\nFile name convention\n\n\nfile name part\nexplanation\n\n\n\n\nVNP46A1\nname of data short product\n\n\nA2021038\nyear (ie. 2021) and day of year (ie. 038) that the imagery was acquired\n\n\nh08\nhorizontal tile number 8\n\n\nv05\nvertical tile number 5\n\n\n2021039064328\nyear (2021), day of year (039), and time (06:43:28 UTC) that the file contents were generated\n\n\nh5\nfile extension: HDF-EOS5 (hierarchical data format - earth observing system)\n\n\n\n\n\n\n\nThe functions below takes an HDFEOS file as input and, from that file, reads the DNB_at_Sensor_Radiance_500m dataset. The R code using the stars package with Python code uses GDAL and rasterio. The functions then reads the sinusoidal tile x/y positions, adjusts the dimensions, and sets the coordinate reference system.\n\nFuctions to read imagery data\n\nR Project RPython Python\n\n\n\n# function to read and process imagery files as rasters using the stars package\nread_dnb_file &lt;- function(file_name) {\n  \n  # HDF dataset that contains the night lights band\n  dataset_name &lt;- \"//HDFEOS/GRIDS/VNP_Grid_DNB/Data_Fields/DNB_At_Sensor_Radiance_500m\"\n  \n  # extract the horizontal and vertical tile coordinates from the metadata\n  # this information is a string of text\n  h_string &lt;- gdal_metadata(file_name)[199]\n  v_string &lt;- gdal_metadata(file_name)[219]\n  \n  # from the horizontal and vertical tile text, obtain the coordinate info as an integer\n  tile_h &lt;- as.integer(str_split(h_string, \"=\", simplify = TRUE)[[2]])\n  tile_v &lt;- as.integer(str_split(v_string, \"=\", simplify = TRUE)[[2]])\n  \n  # use tile coordinates to calculate a geographic bounding box\n  west &lt;- (10 * tile_h) - 180\n  north &lt;- 90 - (10 * tile_v)\n  east &lt;- west + 10\n  south &lt;- north - 10\n  \n  delta &lt;- 10 / 2400\n  \n  # read the dataset\n  dnb &lt;- read_stars(file_name, sub = dataset_name)\n  \n  # set the coordinate reference system\n  st_crs(dnb) &lt;- st_crs(4326)\n  st_dimensions(dnb)$x$delta &lt;- delta\n  st_dimensions(dnb)$x$offset &lt;- west\n  st_dimensions(dnb)$y$delta &lt;- -delta\n  st_dimensions(dnb)$y$offset &lt;- north\n  \n  return(dnb)\n}\n\n# load in files using the read_dnb function\ndnb_feb7_h08v05 &lt;- read_dnb_file(file_name = \"data/VNP46A1/VNP46A1.A2021038.h08v05.001.2021039064328.h5\")\ndnb_feb7_h08v06  &lt;- read_dnb_file(file_name = \"data/VNP46A1/VNP46A1.A2021038.h08v06.001.2021039064329.h5\")\ndnb_feb16_h08v05 &lt;- read_dnb_file(file_name = \"data/VNP46A1/VNP46A1.A2021047.h08v05.001.2021048091106.h5\")\ndnb_feb16_h08v06 &lt;- read_dnb_file(file_name = \"data/VNP46A1/VNP46A1.A2021047.h08v06.001.2021048091105.h5\")\n\n\n\n\n# code from: https://blackmarble.gsfc.nasa.gov/tools/OpenHDF5.py\ndef read_dnb_file_py (file_name):\n    rasterFiles = file_name\n    \n    #Get File Name Prefix\n    rasterFilePre = rasterFiles[8:16] + \"_\" + rasterFiles[17:23]\n    rasterPath = \"data/VNP46A1/\" + rasterFiles\n    fileExtension = \"_BBOX.tif\"\n    \n    # Open HDF file\n    hdflayer = gdal.Open(rasterPath, gdal.GA_ReadOnly)\n    \n    # Open raster layer\n    subhdflayer = hdflayer.GetSubDatasets()[4][0] # DNB_At_Sensor_Radiance_500m\n    rlayer = gdal.Open(subhdflayer, gdal.GA_ReadOnly)\n    \n    # Subset the Long Name\n    outputName = subhdflayer[92:]\n    outputNameNoSpace = outputName.strip().replace(\" \",\"_\").replace(\"/\",\"_\")\n    outputNameFinal = outputNameNoSpace + \"_\" + rasterFilePre + fileExtension\n    \n    # outputFolder = \"data/image\"\n    # outputRaster = outputFolder + outputNameFinal\n    outputFolder = \"data/\"\n    outputPre = \"image\"\n    outputRaster = outputFolder + outputPre + outputNameFinal\n    \n    # collect bounding box coordinates\n    HorizontalTileNumber = int(rlayer.GetMetadata_Dict()[\"HorizontalTileNumber\"])\n    VerticalTileNumber = int(rlayer.GetMetadata_Dict()[\"VerticalTileNumber\"])\n    \n    WestBoundCoord = (10*HorizontalTileNumber) - 180\n    NorthBoundCoord = 90-(10*VerticalTileNumber)\n    EastBoundCoord = WestBoundCoord + 10\n    SouthBoundCoord = NorthBoundCoord - 10\n    \n    # set crs\n    EPSG = \"-a_srs EPSG:4326\" #WGS84\n    \n    translateOptionText = EPSG+\" -a_ullr \" + str(WestBoundCoord) + \" \" + str(NorthBoundCoord) + \" \" + str(EastBoundCoord) + \" \" + str(SouthBoundCoord)\n    translateoptions = gdal.TranslateOptions(gdal.ParseCommandLine(translateOptionText))\n    gdal.Translate(outputRaster, rlayer, options=translateoptions)\n    \n    return(rasterio.open(outputRaster))\n\n# load in files using the read_dnb function\n# this saves geotif files of the data\ndnb_feb7_h08v05_py = read_dnb_file_py(file_name = \"VNP46A1.A2021038.h08v05.001.2021039064328.h5\")\ndnb_feb7_h08v06_py  = read_dnb_file_py(file_name = \"VNP46A1.A2021038.h08v06.001.2021039064329.h5\")\ndnb_feb16_h08v05_py = read_dnb_file_py(file_name = \"VNP46A1.A2021047.h08v05.001.2021048091106.h5\")\ndnb_feb16_h08v06_py = read_dnb_file_py(file_name = \"VNP46A1.A2021047.h08v06.001.2021048091105.h5\")\n\n\n\n\n\n\nMerge imagery files\nCombine the tiles from each day, like stitching together quilt squares. The R code uses st_mosaic() and the Python code uses rasterio.merge.\n\nR Project RPython Python\n\n\n\n# combined imagery before the storms\nfeb7_merged &lt;-  st_mosaic(dnb_feb7_h08v05, dnb_feb7_h08v06)\n\n# combined imagery after the storms\nfeb16_merged &lt;- st_mosaic(dnb_feb16_h08v05, dnb_feb16_h08v06)\n\n\n\n\n# combined imagery before the storms\nfeb7_merged_py, transform = merge([dnb_feb7_h08v05_py, dnb_feb7_h08v06_py])\n\n# combined imagery after the storms\nfeb16_merged_py, transform = merge([dnb_feb16_h08v05_py, dnb_feb16_h08v06_py])   \n\n\n\n\n\n\nExtract fill value from meta data and convert to NA in data arrays\n\nR Project RPython Python\n\n\n\n# extract fill value from metadata\nfill_value_string &lt;- gdal_metadata(\"data/VNP46A1/VNP46A1.A2021038.h08v05.001.2021039064328.h5\")[38]\nfill_value &lt;- as.integer(str_split(fill_value_string, \"=\", simplify = TRUE)[[2]])\n\n# convert fill value of 65535 to NA\nfeb7_merged[feb7_merged == fill_value] = NA\nfeb16_merged[feb16_merged == fill_value] = NA\n\n\n\n\nds = gdal.Open('data/VNP46A1/VNP46A1.A2021038.h08v05.001.2021039064328.h5', gdal.GA_ReadOnly)\nmeta = ds.GetMetadata()\nfill_value_py = int(meta['HDFEOS_GRIDS_VNP_Grid_DNB_Data_Fields_DNB_At_Sensor_Radiance_500m__FillValue'])\n\n# convert fill value to nan\nfeb7_merged_py = np.where(feb7_merged_py == fill_value_py, np.nan, feb7_merged_py)\nfeb7_merged_metadata = dnb_feb7_h08v05_py.meta.copy()\nfeb7_merged_metadata.update(width=feb7_merged_py.shape[2], height=feb7_merged_py.shape[1], transform=transform, dtype='float32')\nfeb7_merged_dataset = rasterio.io.MemoryFile().open(**feb7_merged_metadata)\nfeb7_merged_dataset.write(feb7_merged_py)\n\nfeb16_merged_py = np.where(feb16_merged_py == fill_value_py, np.nan, feb16_merged_py)\nfeb16_merged_metadata = dnb_feb16_h08v05_py.meta.copy()\nfeb16_merged_metadata.update(width=feb16_merged_py.shape[2], height=feb16_merged_py.shape[1], transform=transform, dtype='float64')\nfeb16_merged_dataset = rasterio.io.MemoryFile().open(**feb16_merged_metadata)\nfeb16_merged_dataset.write(feb16_merged_py)\n\n\n\n\nAccording to the metadata, the fill value is 65535.\n\n\nCrop to ROI\n\nR Project RPython Python\n\n\n\n# set region on interest\nroi_coordinates &lt;- st_polygon(list(rbind(c(-96.5,29), c(-96.5,30.5), c(-94.5,30.5), c(-94.5,29), c(-96.5,29))))\n\n# set coordinate reference system\nroi_sfc &lt;- st_sfc(roi_coordinates, crs = 4326)\n# crs 4326 matches the crs of the satellite imagery\n\nfeb7_roi &lt;- st_crop(feb7_merged, roi_sfc)\nfeb16_roi &lt;- st_crop(feb16_merged, roi_sfc)\n\n\n\n\nroi_coordinates_py = [(-96.5, 29), (-96.5, 30.5), (-94.5, 30.5), (-94.5, 29), (-96.5, 29)]\nmin_x, min_y = min(coord[0] for coord in roi_coordinates_py), min(coord[1] for coord in roi_coordinates_py)\nmax_x, max_y = max(coord[0] for coord in roi_coordinates_py), max(coord[1] for coord in roi_coordinates_py)\n\n# Get the window of the ROI\nwindow = feb7_merged_dataset.window(min_x, min_y, max_x, max_y)\n\n# Read the data within the ROI\nfeb7_roi_py = feb7_merged_dataset.read(window=window)\nfeb16_roi_py = feb16_merged_dataset.read(window=window)\n\n# Update the transform and profile\ntransform = feb7_merged_dataset.window_transform(window)\nprofile = feb7_merged_dataset.profile\nprofile.update({\n    'transform': transform,\n    'width': window.width,\n    'height': window.height,\n    'crs': feb7_merged_dataset.crs\n})\n   \n\n\n\n\n\n\nCode for leaflet map\nroi_leaflet &lt;- st_as_sf(roi_sfc)\nroi_leaflet &lt;- st_make_valid(roi_leaflet)\n\nleaflet(roi_leaflet) %&gt;%\n  setView(lat = 29.75, lng = -95.5, zoom = 8) %&gt;%\n  addProviderTiles(providers$OpenStreetMap) %&gt;% \n  addPolygons(fillColor = \"green\", fillOpacity = 0.5, weight = 2)\n\n\n\n\nRegion of Interest\n\n\n\n\n\n\nBefore the storms\n\n\n\n\n\n\n\n\n\n\n\n\nAfter the storms\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRoad and Building data\nBuilding and road data was obtained from OpenStreetMap. Subsets of the data were provided as GeoPackages (.gpkg) for the Houston metropolitan area. SQL queries were used to subset highways and major roads from the the road file and residential buildings from the building file.\nSince vehicles can be a significant source of observable nighttime light, a 200 meter highway buffer was removed from the blackout mask area. This step prevents areas that experienced reduced traffic from being identified as areas with power outages.\n\nR Project RPython Python\n\n\n\n# load in roads package and select specifically highways\nquery_roads &lt;- \"SELECT * FROM gis_osm_roads_free_1 WHERE fclass in ('motorway', 'motorway_link', 'primary', 'primary_link')\"\nhighways &lt;- st_read(\"data/gis_osm_roads_free_1.gpkg\", query = query_roads)\n\n# transform to the correct projection\nhighways_3083 &lt;- st_transform(highways, crs = 3083)\n\n# create a 200 meter buffer\nhighways_buffer_3083 &lt;- st_buffer(highways_3083, dist = 200) \nhighways_buffer_3083 &lt;- st_union(highways_buffer_3083)\nhighways_buffer_4326 &lt;- st_transform(highways_buffer_3083, crs = 4326)\nhighways_buffer_4326 &lt;- st_make_valid(highways_buffer_4326)\n\n\n\n\n# load in roads package and select specifically highways\nquery_roads_py = \"SELECT * \\\nFROM gis_osm_roads_free_1 \\\nWHERE fclass in ('motorway', 'motorway_link', 'primary', 'primary_link')\"\n\nhighways_py = gpd.read_file(\"data/gis_osm_roads_free_1.gpkg\", query = query_roads_py)\nhighways_py = highways_py[(highways_py['fclass'] == 'motorway') | (highways_py['fclass'] == 'motorway_link') | (highways_py['fclass'] == 'primary') | (highways_py['fclass'] == 'primary_link')]\nhighways_3083_py = highways_py.to_crs(3083)\n\nhighway_buffer_3083_py = highways_3083_py.copy()\nhighway_buffer_3083_py['geometry'] = highway_buffer_3083_py['geometry'].buffer(200)\nhighway_buffer_3083_py = gpd.GeoDataFrame(geometry=[highway_buffer_3083_py.geometry.unary_union], crs=3083)\n\n\n\n\nClose inspection of the building data shows many NA values in the type field. When looking at the data on the map, most NA values appear to be residential buildings, but some non-residential buildings such as schools are included.\n\nR Project RPython Python\n\n\n\n# read in buildings data and select only residential\nquery_buildings &lt;- \"SELECT * FROM gis_osm_buildings_a_free_1 WHERE (type IS NULL AND name IS NULL) OR type in ('residential', 'apartments', 'house', 'static_caravan', 'detached')\"\n\n# read buildings gpkg into object, and transform to correct projection\nbuildings &lt;- st_read(\"data/gis_osm_buildings_a_free_1.gpkg\", query = query_buildings)\nbuildings_3083 &lt;- st_transform(buildings, crs = 3083)\nbuildings_4326 &lt;- st_transform(buildings, crs = 4326)\n\n\n\n\nbuildings_4326_py = gpd.read_file('data/gis_osm_buildings_a_free_1.gpkg')\nbuildings_4326_py = buildings_4326_py[((buildings_4326_py['type'].isna()) & (buildings_4326_py['name'].isna())) | buildings_4326_py['type'].isin(['residential', 'apartments', 'house', 'static_caravan', 'detached'])]\n\nbuildings_3083_py = buildings_4326_py.to_crs(3083)\n\n\n\n\n\ndel(buildings_4326_py, highways_py, highways_3083_py, query_roads_py)\n\n\n\nCensus data\nData pertaining to race, age, and income demographics for Texas census tract was obtained from the U.S. Census Bureau’s American Community Survey. The data used in this analysis is from 2019 and was cropped to the region of interest. Specific variables evaluated include:\n\nRace\n\nwhite\nblack\nnative american\nhispanic / latino\n\nAge\n\n65 and older\nchildren under 18\n\nIncome\n\nhouseholds below poverty level\nmedian income\n\n\nWhile the race and age data provides the population of each variable for each census tract, these values were normalized by the total population of the census tract. The poverty data was normalized by number of households in each census tract.\n\n\n\n\n\n\nNote\n\n\n\nThe ACS data consists of the layers listed below, with each layer containing subsets of data as documented in the ACS Metadata.\n\n\n\nAmerican Commnity Surey Layer Names\n\n\n\n\n\n\n\n\n\nLayer name\n\n\n\n\nX01_AGE_AND_SEX\n\n\nX02_RACE\n\n\nX03_HISPANIC_OR_LATINO_ORIGIN\n\n\nX04_ANCESTRY\n\n\nX05_FOREIGN_BORN_CITIZENSHIP\n\n\nX06_PLACE_OF_BIRTH\n\n\nX07_MIGRATION\n\n\nX08_COMMUTING\n\n\nX09_CHILDREN_HOUSEHOLD_RELATIONSHIP\n\n\nX10_GRANDPARENTS_GRANDCHILDREN\n\n\nX11_HOUSEHOLD_FAMILY_SUBFAMILIES\n\n\nX12_MARITAL_STATUS_AND_HISTORY\n\n\nX13_FERTILITY\n\n\nX14_SCHOOL_ENROLLMENT\n\n\nX15_EDUCATIONAL_ATTAINMENT\n\n\nX16_LANGUAGE_SPOKEN_AT_HOME\n\n\n\n\n\n\nLayer name\n\n\n\n\nX17_POVERTY\n\n\nX18_DISABILITY\n\n\nX19_INCOME\n\n\nX20_EARNINGS\n\n\nX21_VETERAN_STATUS\n\n\nX22_FOOD_STAMPS\n\n\nX23_EMPLOYMENT_STATUS\n\n\nX25_HOUSING_CHARACTERISTICS\n\n\nX27_HEALTH_INSURANCE\n\n\nX28_COMPUTER_AND_INTERNET_USE\n\n\nX29_VOTING_AGE_POPULATION\n\n\nX99_IMPUTATION\n\n\nX24_INDUSTRY_OCCUPATION\n\n\nX26_GROUP_QUARTERS\n\n\nTRACT_METADATA_2019\n\n\nACS_2019_5YR_TRACT_48_TEXAS\n\n\n\n\n\n\n\n\n\n\n\n\n\nCensus tract geometry\n\nR Project RPython Python\n\n\n\n# read in census tract geometry\nacs_geoms &lt;- st_read(\"data/ACS_2019_5YR_TRACT_48_TEXAS.gdb\",\n                     layer = \"ACS_2019_5YR_TRACT_48_TEXAS\") %&gt;%\n  select(-(STATEFP:Shape_Area))\n\n\n\n\n# census tract geometry\nacs_geoms_py = gpd.read_file('data/ACS_2019_5YR_TRACT_48_TEXAS.gdb',\n                          layer = \"ACS_2019_5YR_TRACT_48_TEXAS\")\n\nacs_geoms_py_3083 = acs_geoms_py.to_crs(3083)\n\n\n\n\n\n\nAge and sex layer\n\nR Project RPython Python\n\n\n\n# read in variables of interest\nacs_age_sex &lt;- st_read(\"data/ACS_2019_5YR_TRACT_48_TEXAS.gdb\",\n                      layer = \"X01_AGE_AND_SEX\")\nacs_age_sex_df &lt;- acs_age_sex %&gt;%\n  select(GEOID) %&gt;%\n  mutate(total_pop_from_age_sex = acs_age_sex$B01001e1,\n         median_age = acs_age_sex$B01002e1,\n         pop_male_65_66 = acs_age_sex$B01001e20,\n         pop_male_67_to_69 = acs_age_sex$B01001e21,\n         pop_male_70_to_74 = acs_age_sex$B01001e22,\n         pop_male_75_to_79 = acs_age_sex$B01001e23,\n         pop_male_80_to_84 = acs_age_sex$B01001e24,\n         pop_male_85_and_over = acs_age_sex$B01001e25,\n         pop_female_65_66 = acs_age_sex$B01001e44,\n         pop_female_67_to_69 = acs_age_sex$B01001e45,\n         pop_female_70_to_74 = acs_age_sex$B01001e46,\n         pop_female_75_to_79 = acs_age_sex$B01001e47,\n         pop_female_80_to_84 = acs_age_sex$B01001e48,\n         pop_female_85_and_over = acs_age_sex$B01001e49,\n         pop_65_and_over = pop_male_65_66 + pop_male_67_to_69 + pop_male_70_to_74 + pop_male_75_to_79 + pop_male_80_to_84 + pop_male_85_and_over + pop_female_65_66 + pop_female_67_to_69 + pop_female_70_to_74 + pop_female_75_to_79 + pop_female_80_to_84 + pop_female_85_and_over,\n         pct_65_and_over = (pop_65_and_over / total_pop_from_age_sex) * 100) %&gt;%\n  select(GEOID, total_pop_from_age_sex, pop_65_and_over, pct_65_and_over)\n\n\n\n\n# data for layers of interest\nacs_age_sex_py = gpd.read_file('data/ACS_2019_5YR_TRACT_48_TEXAS.gdb',\n                            layer = \"X01_AGE_AND_SEX\")\n\nacs_age_sex_py = gpd.read_file('data/ACS_2019_5YR_TRACT_48_TEXAS.gdb',\n                            layer = \"X01_AGE_AND_SEX\")\nacs_age_sex_py = acs_age_sex_py.rename(columns={'B01001e1' : 'total_pop_from_age_sex', \n                                                'B01002e1' : 'median_age',\n                                                'B01001e20' : 'pop_male_65_66',\n                                                'B01001e21' : 'pop_male_67_to_69',\n                                                'B01001e22' : 'pop_male_70_to_74',\n                                                'B01001e23' : 'pop_male_75_to_79',\n                                                'B01001e24' : 'pop_male_80_to_84',\n                                                'B01001e25' : 'pop_male_85_and_over',\n                                                'B01001e44' : 'pop_female_65_66',\n                                                'B01001e45' : 'pop_female_67_to_69',\n                                                'B01001e46' : 'pop_female_70_to_74',\n                                                'B01001e47' : 'pop_female_75_to_79',\n                                                'B01001e48' : 'pop_female_80_to_84',\n                                                'B01001e49' : 'pop_female_85_and_over'})\nacs_age_sex_py['pop_65_and_over'] = acs_age_sex_py['pop_male_65_66'] + acs_age_sex_py['pop_male_67_to_69'] + acs_age_sex_py['pop_male_70_to_74'] + acs_age_sex_py['pop_male_75_to_79'] + acs_age_sex_py['pop_male_80_to_84'] + acs_age_sex_py['pop_male_85_and_over'] + acs_age_sex_py['pop_female_65_66'] + acs_age_sex_py['pop_female_67_to_69'] + acs_age_sex_py['pop_female_70_to_74'] + acs_age_sex_py['pop_female_75_to_79'] + acs_age_sex_py['pop_female_80_to_84'] + acs_age_sex_py['pop_female_85_and_over']\nacs_age_sex_py['pct_65_and_over'] = (acs_age_sex_py['pop_65_and_over'] / acs_age_sex_py['total_pop_from_age_sex']) * 100\nacs_age_sex_py = acs_age_sex_py[['GEOID', 'total_pop_from_age_sex', 'pop_65_and_over', 'pct_65_and_over']]\n\n\n\n\n\n\nRace layer\n\nR Project RPython Python\n\n\n\nacs_race &lt;- st_read(\"data/ACS_2019_5YR_TRACT_48_TEXAS.gdb\",\n                      layer = \"X02_RACE\")\nacs_race_df &lt;- acs_race %&gt;%\n  select(GEOID) %&gt;%\n  mutate(total_pop_from_race = acs_race$B02001e1,\n         pop_white = acs_race$B02001e2,\n         pct_white = (pop_white / total_pop_from_race) * 100,\n         pop_black = acs_race$B02001e3,\n         pct_black = (pop_black / total_pop_from_race) * 100,\n         pop_am_native = acs_race$B02001e4,\n         pct_am_native = (pop_am_native / total_pop_from_race) * 100,\n         pop_asian = acs_race$B02001e5,\n         pct_asian = (pop_asian / total_pop_from_race) * 100)\n\n\n\n\nacs_race_py = gpd.read_file('data/ACS_2019_5YR_TRACT_48_TEXAS.gdb',\n                         layer = \"X02_RACE\")\nacs_race_py = acs_race_py.rename(columns={'B02001e1' : 'total_pop_from_race',\n                                          'B02001e2' : 'pop_white'})\nacs_race_py['pct_white'] = (acs_race_py['pop_white'] / acs_race_py['total_pop_from_race']) * 100\nacs_race_py['pop_black'] = acs_race_py['B02001e3']\nacs_race_py['pct_black'] = (acs_race_py['pop_black'] / acs_race_py['total_pop_from_race']) * 100\nacs_race_py['pop_am_native'] = acs_race_py['B02001e4']\nacs_race_py['pct_am_native'] = (acs_race_py['pop_am_native'] / acs_race_py['total_pop_from_race']) * 100\nacs_race_py['pop_asian'] = acs_race_py['B02001e5']\nacs_race_py['pct_asian'] = (acs_race_py['pop_asian'] / acs_race_py['total_pop_from_race']) * 100\nacs_race_py = acs_race_py[['GEOID', 'total_pop_from_race', 'pop_white', 'pct_white', 'pop_black', 'pct_black', 'pop_am_native', 'pct_am_native', 'pop_asian', 'pct_asian']]\n\n\n\n\n\n\nHispanic latino layer\n\nR Project RPython Python\n\n\n\nacs_hispanic_latino &lt;- st_read(\"data/ACS_2019_5YR_TRACT_48_TEXAS.gdb\",\n                      layer = \"X03_HISPANIC_OR_LATINO_ORIGIN\")\nacs_hispanic_latino_df &lt;- acs_hispanic_latino %&gt;%\n  select(GEOID) %&gt;%\n  mutate(total_pop_from_hispanic = acs_hispanic_latino$B03002e1,\n         pop_hispanic_latino = acs_hispanic_latino$B03002e12,\n         pct_hispanic_latino = (pop_hispanic_latino / total_pop_from_hispanic) * 100)\n\n\n\n\nacs_hispanic_latino_py = gpd.read_file('data/ACS_2019_5YR_TRACT_48_TEXAS.gdb',\n                                    layer = \"X03_HISPANIC_OR_LATINO_ORIGIN\")\nacs_hispanic_latino_py['total_pop_from_hispanic'] = acs_hispanic_latino_py['B03002e1']\nacs_hispanic_latino_py['pop_hispanic_latino'] = acs_hispanic_latino_py['B03002e12']\nacs_hispanic_latino_py['pct_hispanic_latino'] = (acs_hispanic_latino_py['pop_hispanic_latino'] / acs_hispanic_latino_py['total_pop_from_hispanic']) * 100\nacs_hispanic_latino_py = acs_hispanic_latino_py[['GEOID', 'total_pop_from_hispanic', 'pop_hispanic_latino', 'pct_hispanic_latino']]\n\n\n\n\n\n\nChildren and household relationship\n\nR Project RPython Python\n\n\n\nacs_children_household &lt;- st_read(\"data/ACS_2019_5YR_TRACT_48_TEXAS.gdb\",\n                      layer = \"X09_CHILDREN_HOUSEHOLD_RELATIONSHIP\")\nacs_children_household_df &lt;- acs_children_household %&gt;%\n  select(GEOID) %&gt;%\n  mutate(pop_children_under_18 = acs_children_household$B09002e1,\n         pct_children_under_18 = (pop_children_under_18 / acs_hispanic_latino$B03002e1) * 100)\n# the children_household_relationship layer did not contain a total population field\n\n\n\n\nacs_children_household_py = gpd.read_file('data/ACS_2019_5YR_TRACT_48_TEXAS.gdb',\n                                       layer = \"X09_CHILDREN_HOUSEHOLD_RELATIONSHIP\")\nacs_children_household_py['pop_children_under_18'] = acs_children_household_py['B09002e1']\nacs_children_household_py['pct_children_under_18'] = (acs_children_household_py['pop_children_under_18'] / acs_hispanic_latino_py['total_pop_from_hispanic']) * 100\nacs_children_household_py = acs_children_household_py[['GEOID', 'pop_children_under_18', 'pct_children_under_18']]\n# the children_household_relationship layer did not contain a total populaiton field\n\n\n\n\n\n\nPoverty layer\n\nR Project RPython Python\n\n\n\nacs_poverty &lt;- st_read(\"data/ACS_2019_5YR_TRACT_48_TEXAS.gdb\",\n                       layer = \"X17_POVERTY\")\nacs_poverty_df &lt;- acs_poverty %&gt;%\n  select(GEOID) %&gt;%\n  mutate(total_households = acs_poverty$B17017e1,\n         num_households_below_poverty = acs_poverty$B17017e2,\n         pct_households_below_poverty = (num_households_below_poverty / total_households) * 100)\n\n\n\n\nacs_poverty_py = gpd.read_file('data/ACS_2019_5YR_TRACT_48_TEXAS.gdb',\n                            layer = \"X17_POVERTY\")\nacs_poverty_py['total_households'] = acs_poverty_py['B17017e1']\nacs_poverty_py['num_households_below_poverty'] = acs_poverty_py['B17017e2']\nacs_poverty_py['pct_households_below_poverty'] = (acs_poverty_py['num_households_below_poverty'] / acs_poverty_py['total_households']) * 100\nacs_poverty_py = acs_poverty_py[['GEOID', 'total_households', 'num_households_below_poverty', 'pct_households_below_poverty']]\n\n\n\n\n\n\nIncome layer\n\nR Project RPython Python\n\n\n\nacs_income &lt;- st_read(\"data/ACS_2019_5YR_TRACT_48_TEXAS.gdb\",\n                      layer = \"X19_INCOME\")\n\nacs_income_df &lt;- acs_income %&gt;%\n  select(GEOID) %&gt;%\n  mutate(median_income = acs_income$B19013e1)\n\n\n\n\nacs_income_py = gpd.read_file('data/ACS_2019_5YR_TRACT_48_TEXAS.gdb',\n                           layer = \"X19_INCOME\")\nacs_income_py['median_income'] = acs_income_py['B19013e1']\nacs_income_py = acs_income_py[['GEOID', 'median_income']]\n\n\n\n\n\n\nJoin census track data\nNext, the selected race, age, and income layers are joined. The resulting dataset contains the geometry of each census tract and corresponding socioeconomic data.\n\nR Project RPython Python\n\n\n\ncensus_tract_data &lt;- acs_geoms %&gt;%\n  left_join(acs_age_sex_df, by = c(\"GEOID_Data\" = \"GEOID\")) %&gt;%\n  left_join(acs_race_df, by = c(\"GEOID_Data\" = \"GEOID\")) %&gt;%\n  left_join(acs_hispanic_latino_df, by = c(\"GEOID_Data\" = \"GEOID\")) %&gt;%\n  left_join(acs_children_household_df, by = c(\"GEOID_Data\" = \"GEOID\")) %&gt;%\n  left_join(acs_poverty_df, by = c(\"GEOID_Data\" = \"GEOID\")) %&gt;%\n  left_join(acs_income_df, by = c(\"GEOID_Data\" = \"GEOID\")) %&gt;%\n  rename(total_population = total_pop_from_age_sex) %&gt;%\n  select(-c(total_pop_from_race, total_pop_from_hispanic))\n\ncensus_tract_data_3083 &lt;- st_transform(census_tract_data, 3083)\ncensus_tract_data_4326 &lt;- st_transform(census_tract_data_3083, crs = 4326)\n\ncensus_tract_data_roi_4326 &lt;- st_crop(census_tract_data_4326, roi_sfc)\ncensus_tract_data_roi_3083 &lt;- st_transform(census_tract_data_roi_4326, crs = 3083)\n\n\n\n\n# join data layers to census tract geometry\ncensus_tract_data_py = acs_geoms_py_3083.copy()\ncensus_tract_data_py = census_tract_data_py.merge(acs_age_sex_py, how='left', left_on='GEOID_Data', right_on='GEOID')\ncensus_tract_data_py = census_tract_data_py.merge(acs_race_py, how='left', left_on='GEOID_Data', right_on='GEOID')\ncensus_tract_data_py = census_tract_data_py.drop(columns=['GEOID_x', 'GEOID_y'])\ncensus_tract_data_py = census_tract_data_py.merge(acs_hispanic_latino_py, how='left', left_on='GEOID_Data', right_on='GEOID')\ncensus_tract_data_py = census_tract_data_py.merge(acs_children_household_py, how='left', left_on='GEOID_Data', right_on='GEOID')\ncensus_tract_data_py = census_tract_data_py.drop(columns=['GEOID_x', 'GEOID_y'])\ncensus_tract_data_py = census_tract_data_py.merge(acs_poverty_py, how='left', left_on='GEOID_Data', right_on='GEOID')\ncensus_tract_data_py = census_tract_data_py.merge(acs_income_py, how='left', left_on='GEOID_Data', right_on='GEOID')\ncensus_tract_data_py = census_tract_data_py.drop(columns=['GEOID_x', 'GEOID_y'])\n\n\n\n\n\ndel(acs_geoms_py, acs_geoms_py_3083, acs_age_sex_py, acs_race_py, acs_hispanic_latino_py, acs_children_household_py, acs_poverty_py, acs_income_py)"
  },
  {
    "objectID": "code_samples/blackout-analysis-with-python-and-r/index.html#create-blackout-mask",
    "href": "code_samples/blackout-analysis-with-python-and-r/index.html#create-blackout-mask",
    "title": "Blackout Analysis with Python and R",
    "section": "Create blackout mask",
    "text": "Create blackout mask\nAn array was created to represent the difference in night light intensity by subtracting the post-storm imagery from the pre-storm imagery. Next, a blackout threshold is created to identify areas where night light intensity decreased by more than 200 nW cm-2 sr-1. For this analysis, differences in night light intensity are assumed to be caused by the power outages. Areas where the change in light intensity met the threshold were converted to vector format and the highway buffer area was removed.\n\nR Project RPython Python\n\n\n\ndifference &lt;- feb7_roi - feb16_roi\nthreshold &lt;- 200\n\nblackout_threshold &lt;- difference\nblackout_threshold[blackout_threshold &lt;= threshold] = NA # not a blackout area\nblackout_threshold[blackout_threshold &gt; threshold] = TRUE # a blackout area\n\n# vectorize the blackout mask\nblackout_threshold_vector &lt;- st_as_sf(blackout_threshold)\nblackout_threshold_vector &lt;- st_make_valid(blackout_threshold_vector) %&gt;% \n  st_transform(crs = 3083)\n\n# remove the highway buffer from the vectorized blackout mask using st_difference()\nblackout_mask_3083 &lt;- st_difference(blackout_threshold_vector, highways_buffer_3083)\n\n\n\n\ndifference_py = feb7_roi_py - feb16_roi_py\nthreshold_py = 200\nblackout_threshold_py = np.where(difference_py &lt;= threshold_py, 0, 1)\nblackout_threshold_dataset = rasterio.io.MemoryFile().open(**profile)\nblackout_threshold_dataset.write(blackout_threshold_py)\nblackout_threshold_data_py = blackout_threshold_dataset.read(1).astype('int16')\n\n# get shapes as GeoJSON-like objects\nshapes_gen = shapes(blackout_threshold_data_py, transform=blackout_threshold_dataset.transform, mask = blackout_threshold_data_py != 0)\n# convert shapes to Shapely geometries\ngeometries = [shape(shape_json) for shape_json, _ in shapes_gen]\n\n# create a GeoDataFrame from Shapely geometries\nblackout_threshold_vector_py = gpd.GeoDataFrame(geometry=geometries, crs=blackout_threshold_dataset.crs)\nblackout_threshold_vector_3083_py = blackout_threshold_vector_py.to_crs(3083)\nblackout_mask_3083_py = gpd.overlay(blackout_threshold_vector_3083_py, highway_buffer_3083_py, how='difference')"
  },
  {
    "objectID": "code_samples/blackout-analysis-with-python-and-r/index.html#visualize",
    "href": "code_samples/blackout-analysis-with-python-and-r/index.html#visualize",
    "title": "Blackout Analysis with Python and R",
    "section": "Visualize",
    "text": "Visualize\nThe map below shows the areas identified as still experiencing power outages as of February 16, 2021.\n\n\nCode for leaflet map\n# transform to WG84 to be compatible with the leaflet package\nblackout_mask_4326 &lt;- st_transform(blackout_mask_3083, crs = 4326)\n\npal_blackout &lt;- colorNumeric(c(\"red\"), 1, na.color = \"transparent\")\n\nleaflet(blackout_mask_4326) %&gt;%\n  setView(lat = 29.75, lng = -95.5, zoom = 9) %&gt;%\n  addProviderTiles(providers$OpenStreetMap) %&gt;%\n  addPolygons(fillColor = ~pal_blackout(DNB_At_Sensor_Radiance_500m), fillOpacity = 0.75, weight = 0)\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nWhile the spatial analysis was completed using the NAD83 / Texas Centric Albers Equal Area (EPSG:3083) projection, the processed data was transposed to WGS 84 / World Geodetic System (EPSF:4326) to be compatible with maps created with the leaflet package."
  },
  {
    "objectID": "code_samples/blackout-analysis-with-python-and-r/index.html#identify-houses-without-power",
    "href": "code_samples/blackout-analysis-with-python-and-r/index.html#identify-houses-without-power",
    "title": "Blackout Analysis with Python and R",
    "section": "Identify houses without power",
    "text": "Identify houses without power\nHouses that lost electricity were identified based on a spatial intersection of the building data and the blackout mask. In this analysis, houses that did not overlap the blackout mask were identified as not losing power (or had power restored by February 16th).\n\nR Project RPython Python\n\n\n\n# use spatial subsetting to find all the residential buildings in blackout areas\nblackout_buildings_3083 &lt;- buildings_3083[blackout_mask_3083, op = st_intersects]\n\nnumber_of_buildings_without_power &lt;- nrow(blackout_buildings_3083)\n\n\n\n\nblackout_buildings_3083_py = gpd.sjoin(buildings_3083_py, blackout_mask_3083_py, how='inner', op='intersects')\n\nsys:1: FutureWarning: The `op` parameter is deprecated and will be removed in a future release. Please use the `predicate` parameter instead.\n\nblackout_buildings_3083_py = blackout_buildings_3083_py.drop_duplicates(subset=['osm_id'])\nblackout_buildings_3083_py = blackout_buildings_3083_py.drop(columns=['index_right'])\n\n\n\n\nBased on this analysis, an estimated 144,317 houses in the Houston metropolitan area were without power on February 16, 2021."
  },
  {
    "objectID": "code_samples/blackout-analysis-with-python-and-r/index.html#aggregate-residential-building-outages-to-census-tract-level",
    "href": "code_samples/blackout-analysis-with-python-and-r/index.html#aggregate-residential-building-outages-to-census-tract-level",
    "title": "Blackout Analysis with Python and R",
    "section": "Aggregate residential building outages to census tract level",
    "text": "Aggregate residential building outages to census tract level\n\nR Project RPython Python\n\n\n\n# use a spatial join to attach the census tract data to the building data\nblackout_buildings_census_3083 &lt;- st_join(blackout_buildings_3083, census_tract_data_3083) %&gt;%\n  mutate(blackout_status = \"lost power\")\n\n# residential houses that did not lose power\nno_blackout_buildings_3083 &lt;- setdiff(buildings_3083, blackout_buildings_3083)\nno_blackout_buildings_census_3083 &lt;- st_join(no_blackout_buildings_3083, census_tract_data_3083) %&gt;%\n  mutate(blackout_status = \"did not lose power\")\n\nall_building_data_3083 &lt;- rbind(blackout_buildings_census_3083, no_blackout_buildings_census_3083)\n\ncounts_of_lost_power_by_census_tract &lt;- all_building_data_3083 %&gt;%\n  st_drop_geometry() %&gt;%\n  group_by(GEOID_Data, blackout_status) %&gt;%\n  summarise(count = n()) %&gt;%\n  pivot_wider(names_from = blackout_status, values_from = count, values_fill = 0) %&gt;%\n  rename(num_houses_did_not_lose_power = \"did not lose power\",\n         num_houses_lost_power = \"lost power\") %&gt;%\n  mutate(pct_houses_that_lost_power = (num_houses_lost_power / (num_houses_did_not_lose_power + num_houses_lost_power)) * 100)\n\ncensus_tract_blackout_data_with_geometry &lt;- left_join(census_tract_data_roi_4326, counts_of_lost_power_by_census_tract, by = \"GEOID_Data\")\n\ncensus_tract_blackout_data &lt;- census_tract_blackout_data_with_geometry %&gt;% \n  st_drop_geometry()\n\n\n\n\nblackout_buildings_census_3083_py = gpd.sjoin(blackout_buildings_3083_py, census_tract_data_py)\nblackout_buildings_census_3083_py['blackout_status'] = 'lost power'\n\nno_blackout_buildings_3083_py = buildings_3083_py[~buildings_3083_py['osm_id'].isin(blackout_buildings_3083_py['osm_id'])]\n\nno_blackout_buildings_census_3083_py = gpd.sjoin(no_blackout_buildings_3083_py, census_tract_data_py)\nno_blackout_buildings_census_3083_py['blackout_status'] = 'did not lost power'\n\nall_building_data_3083_py = pd.concat([blackout_buildings_census_3083_py, no_blackout_buildings_census_3083_py], axis=0)\n\ncounts_of_lost_power_by_census_tract_py = all_building_data_3083_py.copy()\ncounts_of_lost_power_by_census_tract_py = counts_of_lost_power_by_census_tract_py[['GEOID_Data', 'blackout_status']]\ncounts_of_lost_power_by_census_tract_py = pd.pivot_table(counts_of_lost_power_by_census_tract_py, index='GEOID_Data', columns='blackout_status', aggfunc=len, fill_value=0)\ncounts_of_lost_power_by_census_tract_py.columns = ['num_houses_did_not_lose_power', 'num_houses_lost_power']\ncounts_of_lost_power_by_census_tract_py.reset_index(inplace=True)\n\ncensus_tract_blackout_data_py = pd.merge(census_tract_data_py, counts_of_lost_power_by_census_tract_py, how='inner', on='GEOID_Data')\ncensus_tract_blackout_data_py['pct_houses_that_lost_power'] = (census_tract_blackout_data_py['num_houses_lost_power'] / (census_tract_blackout_data_py['num_houses_did_not_lose_power'] + census_tract_blackout_data_py['num_houses_lost_power'])) * 100\n\n\n\n\nThe map below shows blackout areas layered over census tract populations, with labels identifying the number of houses in each census tract that lost power. The buildings layer was not visualized due to the size of the dataset and slow load times.\n\n\nCode for leaflet map\npal_pop &lt;- colorQuantile(\"Blues\", census_tract_blackout_data_with_geometry$total_population, n = 5)\npal_blackout &lt;- colorNumeric(c(\"red\"), 1, na.color = \"transparent\")\n\npop_colors &lt;- unique(pal_pop(sort(census_tract_blackout_data_with_geometry$total_population)))\npop_labs &lt;- format(quantile(census_tract_blackout_data_with_geometry$total_population, seq(0, 1, .2)), big.mark = \",\", digits = 0)\npop_labs &lt;- paste(lag(pop_labs), pop_labs, sep = \" - \")[-1]\n\nleaflet(census_tract_blackout_data_with_geometry) %&gt;%\n  setView(lat = 29.75, lng = -95.5, zoom = 9) %&gt;%\n  addProviderTiles(providers$OpenStreetMap) %&gt;% \n  addPolygons(fillColor = ~pal_pop(total_population), fillOpacity = 0.5,\n              color = \"gray\", weight = 0.75,\n              label = ~paste(\"Number of houses without power:\", format(num_houses_lost_power, big.mark = \",\")),\n              highlight = highlightOptions(weight = 2, color = \"black\", bringToFront = TRUE)) %&gt;%\n  addLegend(colors = pop_colors, labels = pop_labs,\n            title = \"Total Population\",\n            \"bottomright\") %&gt;%\n  addPolygons(data = blackout_mask_4326, fillColor = ~pal_blackout(DNB_At_Sensor_Radiance_500m), fillOpacity = 0.75, weight = 0)\n\n\n\n\n\n\n\ndel(blackout_threshold_dataset, blackout_threshold_py, blackout_threshold_vector_3083_py, blackout_threshold_vector_py, buildings_3083_py, census_tract_data_py, difference_py, ds, feb16_merged_dataset)\n\n\ndel(census_tract_blackout_data_py, counts_of_lost_power_by_census_tract_py, feb16_merged_metadata, feb16_merged_py, feb16_roi_py, feb7_merged_dataset, feb7_merged_metadata, feb7_merged_py, feb7_roi_py, fill_value_py, geometries, highway_buffer_3083_py, max_x, max_y, meta, min_x, min_y, no_blackout_buildings_3083_py, no_blackout_buildings_census_3083_py, profile, roi_coordinates_py, shapes_gen, threshold_py, transform, window)"
  },
  {
    "objectID": "code_samples/blackout-analysis-with-python-and-r/index.html#race",
    "href": "code_samples/blackout-analysis-with-python-and-r/index.html#race",
    "title": "Blackout Analysis with Python and R",
    "section": "Race",
    "text": "Race\n\nPercent white\n\nR Project RPython Python\n\n\n\n# linear regression model\nmodel_pct_white &lt;- lm(data = census_tract_blackout_data, pct_houses_that_lost_power ~ pct_white)\n\n#plot\nplot_model_pct_white &lt;- ggplot(data = census_tract_blackout_data, aes(x = pct_white, y = pct_houses_that_lost_power)) +\n  geom_point(size = 0.5) +\n  geom_smooth(method = lm, formula = y~x, se = FALSE) +\n  theme_classic() +\n  labs(x = \"% white\", y = \"% of houses that lost power\")\nplot_model_pct_white\n\n\n\n\nPercent of houses that lost power vs. percent population white in census tract\n\n\n\n\n\n\nCode for model summary result values\n# model summary\nsummary_model_pct_white &lt;- summary(model_pct_white)\n\n# extract model coefficients\nslope_coef_white &lt;- round(summary_model_pct_white$coefficients[\"pct_white\", \"Estimate\"], digits = 3)\n\nstd_err_white &lt;- round(summary_model_pct_white$coefficients[\"pct_white\", \"Std. Error\"], digits = 3)\n\np_value_white &lt;- format(summary_model_pct_white$coefficients[\"pct_white\", \"Pr(&gt;|t|)\"], scientific = FALSE, digits = 2)\n\n# confidence intervals\nci_95_lower_white &lt;- round(confint(model_pct_white, level = 0.95)[2, 1], digits = 3)\nci_95_upper_white &lt;- round(confint(model_pct_white, level = 0.95)[2, 2], digits = 3)\n\n# r-squared value\nr_sqrd_white &lt;- round(summary_model_pct_white$r.squared, digits = 3)\n\n\n\n\n\nmodel_pct_white_py = ols('pct_houses_that_lost_power ~ pct_white', \n                         data = census_tract_blackout_data_py).fit()\n\nsns.regplot(x = 'pct_white', y = 'pct_houses_that_lost_power',\n            data = census_tract_blackout_data_py, ci = None,\n           scatter_kws = {'s':1, 'color':'black'})\nplt.xlabel('% white')\nplt.ylabel('% of houses that lost power')\nplt.suptitle('Percent of houses that lost power vs. percent population white in census tract', y=0, ha='center')\n\n\n\n\nPercent of houses that lost power vs. percent population white in census tract\n\n\n\n\n\n\nCode for model summary result values\n# extract model coefficients\nslope_coef_white_py = round(model_pct_white_py.params[1], 3)\n\nstd_err_white_py = round(model_pct_white_py.bse[1], 3)\n\np_value_white_py = model_pct_white_py.pvalues[1]\np_value_white_py = f\"{p_value_white_py:.6f}\"\n\nci_95_lower_white_py = round(model_pct_white_py.conf_int(alpha=0.05)[0][1], 3)\n\nci_95_upper_white_py = round(model_pct_white_py.conf_int(alpha=0.05)[1][1], 3)\n\nr_sqrd_white_py = round(model_pct_white_py.rsquared, 3)\n\n\n\n\n\n\n\nPercent black\n\nR Project RPython Python\n\n\n\n# linear regression model\nmodel_pct_black &lt;- lm(data = census_tract_blackout_data, pct_houses_that_lost_power ~ pct_black)\n\n# plot\nplot_model_pct_black &lt;- ggplot(data = census_tract_blackout_data, aes(x = pct_black, y = pct_houses_that_lost_power)) +\n  geom_point(size = 0.5) +\n  geom_smooth(method = lm, formula = y~x, se = FALSE) +\n  theme_classic() +\n  labs(x = \"% black\", y = \"% of houses that lost power\")\nplot_model_pct_black\n\n\n\n\nPercent of houses that lost power vs. percent population black in census tract\n\n\n\n\n\n\nCode for model summary result values\n# model summary\nsummary_model_pct_black &lt;- summary(model_pct_black)\n\n# extract model coefficients\nslope_coef_black &lt;- round(summary_model_pct_black$coefficients[\"pct_black\", \"Estimate\"], digits = 3)\n\nstd_err_black &lt;- round(summary_model_pct_black$coefficients[\"pct_black\", \"Std. Error\"], digits = 3)\n\np_value_black &lt;- format(summary_model_pct_black$coefficients[\"pct_black\", \"Pr(&gt;|t|)\"], scientific = FALSE, digits = 2)\n\n# confidence intervals\nci_95_lower_black &lt;- round(confint(model_pct_black, level = 0.95)[2, 1], digits = 3)\nci_95_upper_black &lt;- round(confint(model_pct_black, level = 0.95)[2, 2], digits = 3)\n\n# r-squared value\nr_sqrd_black &lt;- round(summary_model_pct_black$r.squared, digits = 3)\n\n\n\n\n\nmodel_pct_black_py = ols('pct_houses_that_lost_power ~ pct_black', \n                         data = census_tract_blackout_data_py).fit()\n\nsns.regplot(x = 'pct_black', y = 'pct_houses_that_lost_power',\n            data = census_tract_blackout_data_py, ci = None,\n           scatter_kws = {'s':1, 'color':'black'})\nplt.xlabel('% black')\nplt.ylabel('% of houses that lost power')\n\n\n\n\nPercent of houses that lost power vs. percent population black in census tract\n\n\n\n\n\n\nCode for model summary result values\n# extract model coefficients\nslope_coef_black_py = round(model_pct_black_py.params[1], 3)\nstd_err_black_py = round(model_pct_black_py.bse[1], 3)\np_value_black_py = model_pct_black_py.pvalues[1]\nci_95_lower_black_py = round(model_pct_black_py.conf_int(alpha=0.05)[0][1], 3)\nci_95_upper_black_py = round(model_pct_black_py.conf_int(alpha=0.05)[1][1], 3)\nr_sqrd_black_py = round(model_pct_black_py.rsquared, 3)\n\n\n\n\n\n\n\nPercent native american\n\n\n\n\nPercent of houses that lost power vs. percent population native american in census tract\n\n\n\n\n\n\nPercent of houses that lost power vs. percent population native american in census tract\n\n\n\nR Project RPython Python\n\n\n\n# linear regression model\nmodel_pct_am_native &lt;- lm(data = census_tract_blackout_data, pct_houses_that_lost_power ~ pct_am_native)\n\n# plot\nplot_model_pct_am_native &lt;- ggplot(data = census_tract_blackout_data, aes(x = pct_am_native, y = pct_houses_that_lost_power)) +\n  geom_point(size = 0.5) +\n  geom_smooth(method = lm, formula = y~x, se = FALSE) +\n  theme_classic() +\n  labs(x = \"% native american\", y = \"% of houses that lost power\")\nplot_model_pct_am_native\n\n\n\n\nCode for model summary result values\n# model summary\nsummary_model_pct_am_native &lt;- summary(model_pct_am_native)\n\n# extract model coefficients\nslope_coef_am_native &lt;- round(summary_model_pct_am_native$coefficients[\"pct_am_native\", \"Estimate\"], digits = 3)\n\nstd_err_am_native &lt;- round(summary_model_pct_am_native$coefficients[\"pct_am_native\", \"Std. Error\"], digits = 3)\n\np_value_am_native &lt;- format(summary_model_pct_am_native$coefficients[\"pct_am_native\", \"Pr(&gt;|t|)\"], scientific = FALSE, digits = 2)\n\n# confidence intervals\nci_95_lower_am_native &lt;- round(confint(model_pct_am_native, level = 0.95)[2, 1], digits = 3)\nci_95_upper_am_native &lt;- round(confint(model_pct_am_native, level = 0.95)[2, 2], digits = 3)\n\n# r-squared value\nr_sqrd_am_native &lt;- round(summary_model_pct_am_native$r.squared, digits = 3)\n\n\n\n\n\nmodel_pct_am_native_py = ols('pct_houses_that_lost_power ~ pct_am_native', \n                         data = census_tract_blackout_data_py).fit()\n\nsns.regplot(x = 'pct_am_native', y = 'pct_houses_that_lost_power',\n            data = census_tract_blackout_data_py, ci = None,\n           scatter_kws = {'s':1, 'color':'black'})\nplt.xlabel('% am_native')\nplt.ylabel('% of houses that lost power')\n\n\n\n\nCode for model summary result values\n# extract model coefficients\nslope_coef_am_native_py = round(model_pct_am_native_py.params[1], 3)\nstd_err_am_native_py = round(model_pct_am_native_py.bse[1], 3)\np_value_am_native_py = model_pct_am_native_py.pvalues[1]\nci_95_lower_am_native_py = round(model_pct_am_native_py.conf_int(alpha=0.05)[0][1], 3)\nci_95_upper_am_native_py = round(model_pct_am_native_py.conf_int(alpha=0.05)[1][1], 3)\nr_sqrd_am_native_py = round(model_pct_am_native_py.rsquared, 3)\n\n\n\n\n\n\n\nPercent asian\n\n\n\n\nPercent of houses that lost power vs. percent population asian in census tract\n\n\n\n\n\n\nPercent of houses that lost power vs. percent population asian in census tract\n\n\n\nR Project RPython Python\n\n\n\n# linear regression model\nmodel_pct_asian &lt;- lm(data = census_tract_blackout_data, pct_houses_that_lost_power ~ pct_asian)\n\n# plot\nplot_model_pct_asian &lt;- ggplot(data = census_tract_blackout_data, aes(x = pct_asian, y = pct_houses_that_lost_power)) +\n  geom_point(size = 0.5) +\n  geom_smooth(method = lm, formula = y~x, se = FALSE) +\n  theme_classic() +\n  labs(x = \"% asian\", y = \"% of houses that lost power\")\nplot_model_pct_asian\n\n\n\n\nCode for model summary result values\n# model summary\nsummary_model_pct_asian &lt;- summary(model_pct_asian)\n\n# extract model coefficients\nslope_coef_asian &lt;- round(summary_model_pct_asian$coefficients[\"pct_asian\", \"Estimate\"], digits = 3)\n\nstd_err_asian &lt;- round(summary_model_pct_asian$coefficients[\"pct_asian\", \"Std. Error\"], digits = 3)\n\np_value_asian &lt;- format(summary_model_pct_asian$coefficients[\"pct_asian\", \"Pr(&gt;|t|)\"], scientific = FALSE, digits = 2)\n\n# confidence intervals\nci_95_lower_asian &lt;- round(confint(model_pct_asian, level = 0.95)[2, 1], digits = 3)\nci_95_upper_asian &lt;- round(confint(model_pct_asian, level = 0.95)[2, 2], digits = 3)\n\n# r-squared value\nr_sqrd_asian &lt;- round(summary_model_pct_asian$r.squared, digits = 3)\n\n\n\n\n\nmodel_pct_asian_py = ols('pct_houses_that_lost_power ~ pct_asian', \n                         data = census_tract_blackout_data_py).fit()\n\nsns.regplot(x = 'pct_asian', y = 'pct_houses_that_lost_power',\n            data = census_tract_blackout_data_py, ci = None,\n           scatter_kws = {'s':1, 'color':'black'})\nplt.xlabel('% asian')\nplt.ylabel('% of houses that lost power')\n\n\n\n\nCode for model summary result values\n# extract model coefficients\nslope_coef_asian_py = round(model_pct_asian_py.params[1], 3)\nstd_err_asian_py = round(model_pct_asian_py.bse[1], 3)\np_value_asian_py = model_pct_asian_py.pvalues[1]\nci_95_lower_asian_py = round(model_pct_asian_py.conf_int(alpha=0.05)[0][1], 3)\nci_95_upper_asian_py = round(model_pct_asian_py.conf_int(alpha=0.05)[1][1], 3)\nr_sqrd_asian_py = round(model_pct_asian_py.rsquared, 3)\n\n\n\n\n\n\n\nPercent hispanic / latino\n\n\n\n\nPercent of houses that lost power vs. percent population hispanic / latino in census tract\n\n\n\n\n\n\nPercent of houses that lost power vs. percent population hispanic / latino in census tract\n\n\n\nR Project RPython Python\n\n\n\n# linear regression model\nmodel_pct_hispanic_latino &lt;- lm(data = census_tract_blackout_data, pct_houses_that_lost_power ~ pct_hispanic_latino)\n\n# plot\nplot_model_pct_hispanic_latino &lt;- ggplot(data = census_tract_blackout_data, aes(x = pct_hispanic_latino, y = pct_houses_that_lost_power)) +\n  geom_point(size = 0.5) +\n  geom_smooth(method = lm, formula = y~x, se = FALSE) +\n  theme_classic() +\n  labs(x = \"% hispanic / latino\", y = \"% of houses that lost power\")\nplot_model_pct_hispanic_latino\n\n\n\n\nCode for model summary result values\n# model summary\nsummary_model_pct_hispanic_latino &lt;- summary(model_pct_hispanic_latino)\n\n# extract model coefficients\nslope_coef_hispanic_latino &lt;- round(summary_model_pct_hispanic_latino$coefficients[\"pct_hispanic_latino\", \"Estimate\"], digits = 3)\n\nstd_err_hispanic_latino &lt;- round(summary_model_pct_hispanic_latino$coefficients[\"pct_hispanic_latino\", \"Std. Error\"], digits = 3)\n\np_value_hispanic_latino &lt;- format(summary_model_pct_hispanic_latino$coefficients[\"pct_hispanic_latino\", \"Pr(&gt;|t|)\"], scientific = FALSE, digits = 2)\n\n# confidence intervals\nci_95_lower_hispanic_latino &lt;- round(confint(model_pct_hispanic_latino, level = 0.95)[2, 1], digits = 3)\nci_95_upper_hispanic_latino &lt;- round(confint(model_pct_hispanic_latino, level = 0.95)[2, 2], digits = 3)\n\n# r-squared value\nr_sqrd_hispanic_latino &lt;- round(summary_model_pct_hispanic_latino$r.squared, digits = 3)\n\n\n\n\n\nmodel_pct_hispanic_latino_py = ols('pct_houses_that_lost_power ~ pct_hispanic_latino', \n                         data = census_tract_blackout_data_py).fit()\n\nsns.regplot(x = 'pct_hispanic_latino', y = 'pct_houses_that_lost_power',\n            data = census_tract_blackout_data_py, ci = None,\n           scatter_kws = {'s':1, 'color':'black'})\nplt.xlabel('% hispanic_latino')\nplt.ylabel('% of houses that lost power')\n\n\n\n\nCode for model summary result values\n# extract model coefficients\nslope_coef_hispanic_latino_py = round(model_pct_hispanic_latino_py.params[1], 3)\nstd_err_hispanic_latino_py = round(model_pct_hispanic_latino_py.bse[1], 3)\np_value_hispanic_latino_py = model_pct_hispanic_latino_py.pvalues[1]\nci_95_lower_hispanic_latino_py = round(model_pct_hispanic_latino_py.conf_int(alpha=0.05)[0][1], 3)\nci_95_upper_hispanic_latino_py = round(model_pct_hispanic_latino_py.conf_int(alpha=0.05)[1][1], 3)\nr_sqrd_hispanic_latino_py = round(model_pct_hispanic_latino_py.rsquared, 3)\n\n\n\n\n\n\n\nRace summary plots\n\nmodel_plots_race &lt;- (plot_model_pct_white | plot_model_pct_black | plot_model_pct_am_native) / \n  (plot_model_pct_asian | plot_model_pct_hispanic_latino | plot_spacer()) +\n  plot_annotation(title = \"Lineral Regression Models of Power Outages vs. Race\")\n\nmodel_plots_race"
  },
  {
    "objectID": "code_samples/blackout-analysis-with-python-and-r/index.html#age",
    "href": "code_samples/blackout-analysis-with-python-and-r/index.html#age",
    "title": "Blackout Analysis with Python and R",
    "section": "Age",
    "text": "Age\n\nPercent 65 and older\n\n\n\n\nPercent of houses that lost power vs. percent population age 65 and older in census tract\n\n\n\n\n\n\nPercent of houses that lost power vs. percent population age 65 and older in census tract\n\n\n\nR Project RPython Python\n\n\n\n# linear regression model\nmodel_pct_65_and_over &lt;- lm(data = census_tract_blackout_data, pct_houses_that_lost_power ~ pct_65_and_over)\n\n# plot\nplot_model_pct_65_and_over &lt;- ggplot(data = census_tract_blackout_data, aes(x = pct_65_and_over, y = pct_houses_that_lost_power)) +\n  geom_point(size = 0.5) +\n  geom_smooth(method = lm, formula = y~x, se = FALSE) +\n  theme_classic() +\n  labs(x = \"% 65 and older\", y = \"% of houses that lost power\")\nplot_model_pct_65_and_over\n\n\n\n\nCode for model summary result values\n# model summary\nsummary_model_pct_65_and_over &lt;- summary(model_pct_65_and_over)\n\n# extract model coefficients\nslope_coef_pct_65_and_over &lt;- round(summary_model_pct_65_and_over$coefficients[\"pct_65_and_over\", \"Estimate\"], digits = 3)\n\nstd_err_pct_65_and_over &lt;- round(summary_model_pct_65_and_over$coefficients[\"pct_65_and_over\", \"Std. Error\"], digits = 3)\n\np_value_pct_65_and_over &lt;- format(summary_model_pct_65_and_over$coefficients[\"pct_65_and_over\", \"Pr(&gt;|t|)\"], scientific = FALSE, digits = 2)\n\n# confidence intervals\nci_95_lower_pct_65_and_over &lt;- round(confint(model_pct_65_and_over, level = 0.95)[2, 1], digits = 3)\nci_95_upper_pct_65_and_over &lt;- round(confint(model_pct_65_and_over, level = 0.95)[2, 2], digits = 3)\n\n# r-squared value\nr_sqrd_65_and_over &lt;- round(summary_model_pct_65_and_over$r.squared, digits = 3)\n\n\n\n\n\nmodel_pct_65_and_over_py = ols('pct_houses_that_lost_power ~ pct_65_and_over', \n                         data = census_tract_blackout_data_py).fit()\n\nsns.regplot(x = 'pct_65_and_over', y = 'pct_houses_that_lost_power',\n            data = census_tract_blackout_data_py, ci = None,\n           scatter_kws = {'s':1, 'color':'black'})\nplt.xlabel('% 65 and over')\nplt.ylabel('% of houses that lost power')   \n\n\n\n\nCode for model summary result values\n# extract model coefficients\nslope_coef_pct_65_and_over_py = round(model_pct_65_and_over_py.params[1], 3)\nstd_err_pct_65_and_over_py = round(model_pct_65_and_over_py.bse[1], 3)\np_value_pct_65_and_over_py = model_pct_65_and_over_py.pvalues[1]\nci_95_lower_pct_65_and_over_py = round(model_pct_65_and_over_py.conf_int(alpha=0.05)[0][1], 3)\nci_95_upper_pct_65_and_over_py = round(model_pct_65_and_over_py.conf_int(alpha=0.05)[1][1], 3)\nr_sqrd_pct_65_and_over_py = round(model_pct_65_and_over_py.rsquared, 3)\n\n\n\n\n\n\n\nPercent children under 18\n\n\n\n\nPercent of houses that lost power vs. percent population children under 18 in census tract\n\n\n\n\n\n\nPercent of houses that lost power vs. percent population children under 18 in census tract\n\n\n\nR Project RPython Python\n\n\n\n# linear regression model\nmodel_pct_children_under_18 &lt;- lm(data = census_tract_blackout_data, pct_houses_that_lost_power ~ pct_children_under_18)\n\n# plot\nplot_model_pct_children_under_18 &lt;- ggplot(data = census_tract_blackout_data, aes(x = pct_children_under_18, y = pct_houses_that_lost_power)) +\n  geom_point() +\n  geom_smooth(method = lm, formula = y~x, se = FALSE) +\n  theme_classic() +\n  labs(x = \"% children under 18\", y = \"% of houses that lost power\")\nplot_model_pct_children_under_18\n\n\n\n\nCode for model summary result values\n# model summary\nsummary_model_pct_children_under_18 &lt;- summary(model_pct_children_under_18)\n\n# extract model coefficients\nslope_coef_children_under_18 &lt;- round(summary_model_pct_children_under_18$coefficients[\"pct_children_under_18\", \"Estimate\"], digits = 3)\n\nstd_err_children_under_18 &lt;- round(summary_model_pct_children_under_18$coefficients[\"pct_children_under_18\", \"Std. Error\"], digits = 3)\n\np_value_children_under_18 &lt;- format(summary_model_pct_children_under_18$coefficients[\"pct_children_under_18\", \"Pr(&gt;|t|)\"], scientific = FALSE, digits = 2)\n\n# confidence intervals\nci_95_lower_children_under_18 &lt;- round(confint(model_pct_children_under_18, level = 0.95)[2, 1], digits = 3)\nci_95_upper_children_under_18 &lt;- round(confint(model_pct_children_under_18, level = 0.95)[2, 2], digits = 3)\n\n# r-squared value\nr_sqrd_children_under_18 &lt;- round(summary_model_pct_children_under_18$r.squared, digits = 3)\n\n\n\n\n\nmodel_pct_children_under_18_py = ols('pct_houses_that_lost_power ~ pct_children_under_18', \n                         data = census_tract_blackout_data_py).fit()\n\nsns.regplot(x = 'pct_children_under_18', y = 'pct_houses_that_lost_power',\n            data = census_tract_blackout_data_py, ci = None,\n           scatter_kws = {'s':1, 'color':'black'})\nplt.xlabel('% children under 18')\nplt.ylabel('% of houses that lost power')   \n\n\n\n\nCode for model summary result values\n# extract model coefficients\nslope_coef_children_under_18_py = round(model_pct_children_under_18_py.params[1], 3)\nstd_err_children_under_18_py = round(model_pct_children_under_18_py.bse[1], 3)\np_value_children_under_18_py = model_pct_children_under_18_py.pvalues[1]\nci_95_lower_children_under_18_py = round(model_pct_children_under_18_py.conf_int(alpha=0.05)[0][1], 3)\nci_95_upper_children_under_18_py = round(model_pct_children_under_18_py.conf_int(alpha=0.05)[1][1], 3)\nr_sqrd_children_under_18_py = round(model_pct_children_under_18_py.rsquared, 3)\n\n\n\n\n\n\n\nAge summary plots\n\nmodel_plots_age &lt;- (plot_model_pct_65_and_over | plot_model_pct_children_under_18) +\n  plot_annotation(title = \"Lineral Regression Models of Power Outages vs. Age\")\n\nmodel_plots_age"
  },
  {
    "objectID": "code_samples/blackout-analysis-with-python-and-r/index.html#income",
    "href": "code_samples/blackout-analysis-with-python-and-r/index.html#income",
    "title": "Blackout Analysis with Python and R",
    "section": "Income",
    "text": "Income\n\nPercent households below poverty\n\n\n\n\nPercent of houses that lost power vs. percent population of household below the poverty level in census tract\n\n\n\n\n\n\nPercent of houses that lost power vs. percent population of household below the poverty level in census tract\n\n\n\nR Project RPython Python\n\n\n\n# linear regression model\nmodel_pct_households_below_poverty &lt;- lm(data = census_tract_blackout_data, pct_houses_that_lost_power ~ pct_households_below_poverty)\n\n# plot\nplot_model_pct_households_below_poverty &lt;- ggplot(data = census_tract_blackout_data, aes(x = pct_households_below_poverty, y = pct_houses_that_lost_power)) +\n  geom_point(size = 0.5) +\n  geom_smooth(method = lm, formula = y~x, se = FALSE) +\n  theme_classic() +\n  labs(x = \"% households below poverty\", y = \"% of houses that lost power\")\nplot_model_pct_households_below_poverty\n\n\n\n\nCode for model summary result values\n# model summary\nsummary_model_pct_households_below_poverty &lt;- summary(model_pct_households_below_poverty)\n\n# extract model coefficients\nslope_coef_households_below_poverty &lt;- round(summary_model_pct_households_below_poverty$coefficients[\"pct_households_below_poverty\", \"Estimate\"], digits = 3)\n\nstd_err_households_below_poverty &lt;- round(summary_model_pct_households_below_poverty$coefficients[\"pct_households_below_poverty\", \"Std. Error\"], digits = 3)\n\np_value_households_below_poverty &lt;- format(summary_model_pct_households_below_poverty$coefficients[\"pct_households_below_poverty\", \"Pr(&gt;|t|)\"], scientific = FALSE, digits = 2)\n\n# confidence intervals\nci_95_lower_households_below_poverty &lt;- round(confint(model_pct_households_below_poverty, level = 0.95)[2, 1], digits = 3)\nci_95_upper_households_below_poverty &lt;- round(confint(model_pct_households_below_poverty, level = 0.95)[2, 2], digits = 3)\n\n# r-squared value\nr_sqrd_households_below_poverty &lt;- round(summary_model_pct_households_below_poverty$r.squared, digits = 3)\n\n\n\n\n\nmodel_pct_households_below_poverty_py = ols('pct_houses_that_lost_power ~ pct_households_below_poverty', \n                         data = census_tract_blackout_data_py).fit()\n\nsns.regplot(x = 'pct_households_below_poverty', y = 'pct_houses_that_lost_power',\n            data = census_tract_blackout_data_py, ci = None,\n           scatter_kws = {'s':1, 'color':'black'})\nplt.xlabel('% households below poverty')\nplt.ylabel('% of houses that lost power')  \n\n\n\n\nCode for model summary result values\n# extract model coefficients\nslope_coef_households_below_poverty_py = round(model_pct_households_below_poverty_py.params[1], 3)\nstd_err_households_below_poverty_py = round(model_pct_households_below_poverty_py.bse[1], 3)\np_value_households_below_poverty_py = model_pct_households_below_poverty_py.pvalues[1]\nci_95_lower_households_below_poverty_py = round(model_pct_households_below_poverty_py.conf_int(alpha=0.05)[0][1], 3)\nci_95_upper_households_below_poverty_py = round(model_pct_households_below_poverty_py.conf_int(alpha=0.05)[1][1], 3)\nr_sqrd_households_below_poverty_py = round(model_pct_households_below_poverty_py.rsquared, 3)\n\n\n\n\n\n\n\nMedian income\n\n\n\n\nPercent of houses that lost power vs. median income in census tract\n\n\n\n\n\n\nPercent of houses that lost power vs. median income in census tract\n\n\n\nR Project RPython Python\n\n\n\n# linear regression model\nmodel_median_income &lt;- lm(data = census_tract_blackout_data, pct_houses_that_lost_power ~ median_income)\n\n# plot\nplot_model_median_income &lt;- ggplot(data = census_tract_blackout_data, aes(x = median_income, y = pct_houses_that_lost_power)) +\n  geom_point(size = 0.5) +\n  geom_smooth(method = lm, formula = y~x, se = FALSE) +\n  theme_classic() +\n  labs(x = \"median income\", y = \"% of houses that lost power\")\nplot_model_median_income\n\n\n\n\nCode for model summary result values\n# model summary\nsummary_model_median_income &lt;- summary(model_median_income)\n\n# extract model coefficients\nslope_coef_median_income &lt;- round(summary_model_median_income$coefficients[\"median_income\", \"Estimate\"], digits = 3)\n\nstd_err_median_income &lt;- round(summary_model_median_income$coefficients[\"median_income\", \"Std. Error\"], digits = 3)\n\np_value_median_income &lt;- format(summary_model_median_income$coefficients[\"median_income\", \"Pr(&gt;|t|)\"], scientific = FALSE, digits = 2)\n\n# confidence intervals\nci_95_lower_median_income &lt;- round(confint(model_median_income, level = 0.95)[2, 1], digits = 3)\nci_95_upper_median_income &lt;- round(confint(model_median_income, level = 0.95)[2, 2], digits = 3)\n\n# r-squared value\nr_sqrd_median_income &lt;- round(summary_model_median_income$r.squared, digits = 3)\n\n\n\n\n\nmodel_pct_median_income_py = ols('pct_houses_that_lost_power ~ median_income', \n                         data = census_tract_blackout_data_py).fit()\n\nsns.regplot(x = 'median_income', y = 'pct_houses_that_lost_power',\n            data = census_tract_blackout_data_py, ci = None,\n           scatter_kws = {'s':1, 'color':'black'})\nplt.xlabel('median income')\nplt.ylabel('% of houses that lost power')\n\n\n\n\nCode for model summary result values\n# extract model coefficients\nslope_coef_median_income_py = round(model_pct_median_income_py.params[1], 3)\nstd_err_median_income_py = round(model_pct_median_income_py.bse[1], 3)\np_value_median_income_py = model_pct_median_income_py.pvalues[1]\nci_95_lower_median_income_py = round(model_pct_median_income_py.conf_int(alpha=0.05)[0][1], 3)\nci_95_upper_median_income_py = round(model_pct_median_income_py.conf_int(alpha=0.05)[1][1], 3)\nr_sqrd_median_income_py = round(model_pct_median_income_py.rsquared, 3)\n\n\n\n\n\n\n\nIncome summary plots\n\nmodel_plots_income &lt;- (plot_model_pct_households_below_poverty | plot_model_median_income) +\n  plot_annotation(title = \"Lineral Regression Models of Power Outages vs. Income\")\n\nmodel_plots_income"
  },
  {
    "objectID": "code_samples.html",
    "href": "code_samples.html",
    "title": "Code Samples",
    "section": "",
    "text": "Wind Resource Temporal Variability\n\n\n\nPython\n\n\nrenewable energy\n\n\n\nDiurnal and monthly variability of NREL Wind Toolkit Data\n\n\n\nMarie Rivers\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBlackout Analysis with Python and R\n\n\n\nPython\n\n\nR\n\n\ngeospatial analysis\n\n\n\nA geospatial analysis of power outages based in nighttime imagery using Python and R\n\n\n\nMarie Rivers\n\n\n\n\n\n\n\n\n\n\n\n\nBuoy Data\n\n\n\nPython\n\n\nR\n\n\nxx\n\n\n\nxxx\n\n\n\nMarie Rivers\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProtecting Whales from Ships\n\n\n\nPython\n\n\ngeopandas\n\n\ngeospatial analysis\n\n\n\nA spatial analysis using Python and GeoPandas\n\n\n\nMarie Rivers\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Wrangling with Python and R\n\n\n\nPython\n\n\nR\n\n\nData Wrangling\n\n\n\nExamples of common data wrangling commands with Python and R\n\n\n\nMarie Rivers\n\n\nSep 23, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nShiny Apps\n\n\n\nR\n\n\nShiny App\n\n\n\na sample of projects with interactive web applications\n\n\n\nMarie Rivers\n\n\nJun 10, 2022\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "code_samples/wind-resource-temporal-variability/index.html",
    "href": "code_samples/wind-resource-temporal-variability/index.html",
    "title": "Wind Resource Temporal Variability",
    "section": "",
    "text": "This code sample uses statistical analyses and visualizations to explore the diurnal and monthly variability of wind resources at Mount Washington in New Hampshire using data from the National Renewable Energy Laboratory (NREL) Wind Integration National Dataset (WIND) Toolkit. Datasets within this tool include meteorological conditions such as temperature, pressure, relative humidity, wind direction, and wind speed. Hourly data is available for the continental United States from 2007 to 2013. This analysis used the dataset of wind speed at 100 meters for the year 2012."
  },
  {
    "objectID": "code_samples/wind-resource-temporal-variability/index.html#determine-nearest-timeseries-for-given-latlon",
    "href": "code_samples/wind-resource-temporal-variability/index.html#determine-nearest-timeseries-for-given-latlon",
    "title": "Wind Resource Temporal Variability",
    "section": "Determine nearest timeseries for given Lat/Lon",
    "text": "Determine nearest timeseries for given Lat/Lon\nThe file structure organizes the data into 2 kilometer x 2 kilometer grids. The code below takes the latitude/longitude coordinates of an individual site (in this case Mount Washington) and finds the indices and coordinates of the nearest site within the dataset. Latitude and longitude coordinates are in a modified Lambert Conic projection.\n\n\nsite specific info\nsite_name = \"Mount Washington\"\nsite_coords = (44.27, -71.3)\n\n\n\n\nfunction to find nearest point\n# This function finds the nearest x/y indices for a given lat/lon.\n# Rather than fetching the entire coordinates database, which is 500+ MB, this\n# uses the Proj4 library to find a nearby point and then converts to x/y indices\n\ndef indicesForCoord(f, lat_index, lon_index):\n    dset_coords = f['coordinates']\n    projstring = \"\"\"+proj=lcc +lat_1=30 +lat_2=60 \n                    +lat_0=38.47240422490422 +lon_0=-96.0 \n                    +x_0=0 +y_0=0 +ellps=sphere \n                    +units=m +no_defs \"\"\"\n    projectLcc = Proj(projstring)\n    origin_ll = reversed(dset_coords[0][0])  # Grab origin directly from database\n    origin = projectLcc(*origin_ll)\n    \n    coords = (lon_index,lat_index)\n    coords = projectLcc(*coords)\n    delta = np.subtract(coords, origin)\n    ij = [int(round(x/2000)) for x in delta]\n    return tuple(reversed(ij))\n\nnearest_site = indicesForCoord(f, site_coords[0], site_coords[1] )\n\nprint(\"y,x indices for\", site_name, \": \\t\\t {}\".format(nearest_site))\n\n\ny,x indices for Mount Washington :       (1258, 2423)\n\n\nfunction to find nearest point\nprint(\"Coordinates of\", site_name, \": \\t {}\".format(site_coords))\n\n\nCoordinates of Mount Washington :    (44.27, -71.3)\n\n\nfunction to find nearest point\nprint(\"Coordinates of nearest point: \\t {}\".format(f[\"coordinates\"][nearest_site[0]][nearest_site[1]]))\n\n\nCoordinates of nearest point:    (44.265121, -71.280396)"
  },
  {
    "objectID": "code_samples/wind-resource-temporal-variability/index.html#map",
    "href": "code_samples/wind-resource-temporal-variability/index.html#map",
    "title": "Wind Resource Temporal Variability",
    "section": "Map",
    "text": "Map\nThis map shows the location of Mount Washington and the nearest point from the WIND Toolkit.\n\n\ncreate map showing location of site and nearest point\nnearest_site_coords = f[\"coordinates\"][nearest_site[0]][nearest_site[1]]\n\nsite_map = folium.Map(location = site_coords, zoom_start = 10)\nfolium.Marker(site_coords, popup = site_name).add_to(site_map)\n\n\n&lt;folium.map.Marker object at 0x173c93460&gt;\n\n\ncreate map showing location of site and nearest point\nfolium.Marker(nearest_site_coords, popup = 'Nearest Site').add_to(site_map)\n\n\n&lt;folium.map.Marker object at 0x173c93d00&gt;\n\n\ncreate map showing location of site and nearest point\nsite_map\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "code_samples/wind-resource-temporal-variability/index.html#aggregate-data",
    "href": "code_samples/wind-resource-temporal-variability/index.html#aggregate-data",
    "title": "Wind Resource Temporal Variability",
    "section": "Aggregate data",
    "text": "Aggregate data\nThe pandas dataframe was then aggregated to group the data by month and hour and calculate values for mean and standard deviation. Functions to calculate the first, second, and third quartiles were also used. These dataframes were used for the statistical analysis and visualizations.\n\n\nfunctions to calculate quartiles\ndef quantile25(column):\n    return column.quantile(0.25)\n\ndef quantile50(column):\n    return column.quantile(0.50)\n\ndef quantile75(column):\n    return column.quantile(0.75)\n\n\n\n\n\ncreate dataframe of average wind speed for each hour\nhourly_avg = windspeed_100m_df.groupby(\"hour\")[\"windspeed_100m\"].agg([\"mean\", \"std\", quantile25, quantile50, quantile75])\nhourly_avg = hourly_avg.reset_index()\nhourly_avg.head()\n\n\n   hour       mean       std  quantile25  quantile50  quantile75\n0     0  12.387863  6.712486    7.355722   12.079224   16.777550\n1     1  12.496613  6.747195    7.489243   12.189095   16.737114\n2     2  12.483580  6.864682    7.070374   12.457657   16.932434\n3     3  12.368402  6.868386    6.917786   12.387463   16.807304\n4     4  12.149086  6.715096    6.789604   12.137211   16.203804\n\n\n\n\ncreate dataframe of average wind speed for each month\nmonthly_avg = windspeed_100m_df.groupby([\"month\", \"month_name\"])[\"windspeed_100m\"].agg([\"mean\", \"std\", quantile25, quantile50, quantile75]).reset_index()\nmonthly_avg.head()\n\n\n   month month_name       mean       std  quantile25  quantile50  quantile75\n0      1    January  15.694266  6.280080   11.762596   15.463730   19.280067\n1      2   February  15.809338  7.093691   11.239204   16.272469   20.300148\n2      3      March  12.230855  6.782397    6.586655   11.641289   17.367319\n3      4      April  13.519631  6.016217    9.871977   14.194160   17.768639\n4      5        May   9.106350  5.020830    5.429245    8.146919   12.250893\n\n\n\n\ncreate dataframe of average wind speed for each hour grouped by month\nhourly_avg_by_month = windspeed_100m_df.groupby([\"hour\", \"month\"]).mean()\n\n\n&lt;string&gt;:1: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n\n\ncreate dataframe of average wind speed for each hour grouped by month\nhourly_avg_by_month = hourly_avg_by_month.reset_index().pivot(index = \"hour\", columns = str(\"month\"), values = \"windspeed_100m\")\nhourly_avg_by_month.columns = hourly_avg_by_month.columns.astype(str)\nhourly_avg_by_month.head()\n\n\nmonth          1          2          3  ...         10         11         12\nhour                                    ...                                 \n0      16.933538  17.768404  12.863748  ...  12.589771  11.489303  12.568310\n1      16.990442  17.863747  12.890527  ...  13.027860  11.083610  12.639684\n2      17.024899  17.666428  12.944868  ...  13.207229  11.067130  12.651399\n3      17.247192  17.689474  12.414536  ...  13.030813  10.995311  12.727105\n4      17.087215  17.182659  12.194508  ...  13.004233  10.831832  12.852821\n\n[5 rows x 12 columns]\n\n\n\n\ncreate dataframe of wind speed standard deviation for each hour by month\nhourly_std_by_month = windspeed_100m_df.groupby([\"hour\", \"month\"]).std()\n\n\n&lt;string&gt;:1: FutureWarning: The default value of numeric_only in DataFrameGroupBy.std is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n\n\ncreate dataframe of wind speed standard deviation for each hour by month\nhourly_std_by_month = hourly_std_by_month.reset_index().pivot(index = \"hour\", columns = str(\"month\"), values = \"windspeed_100m\")\nhourly_std_by_month.columns = hourly_std_by_month.columns.astype(str)\nhourly_std_by_month.head()\n\n\nmonth         1         2         3  ...        10        11        12\nhour                                 ...                              \n0      5.391501  6.570820  6.416065  ...  7.869388  8.129505  7.792957\n1      5.445199  6.508991  6.569621  ...  7.946729  7.871586  7.952874\n2      5.927134  6.394429  7.066956  ...  8.314149  7.716703  7.624423\n3      5.916150  6.125285  6.914925  ...  8.346868  7.587217  7.677165\n4      5.975209  5.834017  6.484714  ...  8.281481  7.604963  7.446602\n\n[5 rows x 12 columns]\n\n\n\n\n\ncalculate moving averages\n# 24 hour moving average\nwindow_size_24hr = 24\nwindows_24hr = windspeed_100m_df.rolling(window_size_24hr)\nmoving_averages_24hr = windows_24hr.mean()\n\n# 10 day moving average\n\n\ncalculate moving averages\nwindow_size_10day = 240\nwindows_10day = windspeed_100m_df.rolling(window_size_10day)\nmoving_averages_10day = windows_10day.mean()\n\n# 30 day moving average\nwindow_size_30day = 720\nwindows_30day = windspeed_100m_df.rolling(window_size_30day)\nmoving_averages_30day = windows_30day.mean()"
  },
  {
    "objectID": "code_samples/wind-resource-temporal-variability/index.html#footnotes",
    "href": "code_samples/wind-resource-temporal-variability/index.html#footnotes",
    "title": "Wind Resource Temporal Variability",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe U.S. Energy Information Administration recommends an annual average wind speed of at least 9 mph (4 m/s) for small wind turbines and 13 mph (5.8 m/s) for utility-scale turbines. https://www.eia.gov/energyexplained/wind/where-wind-power-is-harnessed.php#:~:text=Good%20places%20for%20wind%20turbines,)%20for%20utility%2Dscale%20turbines.↩︎\nThe Office of Energy Efficiency & Renewable Energy notes a typical cut-in speed of 6 to 9 mpg and cut-out speed of 55 mph. https://www.energy.gov/eere/articles/how-do-wind-turbines-survive-severe-storms↩︎"
  },
  {
    "objectID": "code_samples/data-wrangling-with-python-and-R/index.html",
    "href": "code_samples/data-wrangling-with-python-and-R/index.html",
    "title": "Data Wrangling with Python and R",
    "section": "",
    "text": "The purpose of this document is to illustrate common data wrangling commands with R and Python. These examples use data from the lterdatasampler package."
  },
  {
    "objectID": "code_samples/data-wrangling-with-python-and-R/index.html#head-and-tail",
    "href": "code_samples/data-wrangling-with-python-and-R/index.html#head-and-tail",
    "title": "Data Wrangling with Python and R",
    "section": "Head and Tail",
    "text": "Head and Tail\nHead returns the first few rows of the data frame and tail returns the last rows. The integer in the examples below is optional and used to specify the number of rows returned.\n\nR Project RPython Python\n\n\n\nhead(trout_salamander_R, 5) # include an integrer is you want to specify the number of rows returned\n\n# A tibble: 5 × 16\n   year sitecode section reach  pass unitnum unittype vert_index pitnumber\n  &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;   &lt;chr&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;         &lt;dbl&gt;     &lt;dbl&gt;\n1  1987 MACKCC-L CC      L         1       1 R                 1        NA\n2  1987 MACKCC-L CC      L         1       1 R                 2        NA\n3  1987 MACKCC-L CC      L         1       1 R                 3        NA\n4  1987 MACKCC-L CC      L         1       1 R                 4        NA\n5  1987 MACKCC-L CC      L         1       1 R                 5        NA\n# ℹ 7 more variables: species &lt;chr&gt;, length_1_mm &lt;dbl&gt;, length_2_mm &lt;dbl&gt;,\n#   weight_g &lt;dbl&gt;, clip &lt;chr&gt;, sampledate &lt;date&gt;, notes &lt;chr&gt;\n\ntail(trout_salamander_R)\n\n# A tibble: 6 × 16\n   year sitecode section reach  pass unitnum unittype vert_index pitnumber\n  &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;   &lt;chr&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;         &lt;dbl&gt;     &lt;dbl&gt;\n1  2019 MACKOG-U OG      U         2      16 C                21        NA\n2  2019 MACKOG-U OG      U         2      16 C                22        NA\n3  2019 MACKOG-U OG      U         2      16 C                23   1043503\n4  2019 MACKOG-U OG      U         2      16 C                24   1043547\n5  2019 MACKOG-U OG      U         2      16 C                25   1043583\n6  2019 MACKOG-U OG      U         2      16 C                26   1043500\n# ℹ 7 more variables: species &lt;chr&gt;, length_1_mm &lt;dbl&gt;, length_2_mm &lt;dbl&gt;,\n#   weight_g &lt;dbl&gt;, clip &lt;chr&gt;, sampledate &lt;date&gt;, notes &lt;chr&gt;\n\n\n\n\n\ntrout_salamander_py.head(5) # include an integrer is you want to specify the number of rows returned\n\n   year  sitecode section reach  ...  weight_g  clip  sampledate  notes\n0  1987  MACKCC-L      CC     L  ...      1.75  NONE  1987-10-07    NaN\n1  1987  MACKCC-L      CC     L  ...      1.95  NONE  1987-10-07    NaN\n2  1987  MACKCC-L      CC     L  ...      5.60  NONE  1987-10-07    NaN\n3  1987  MACKCC-L      CC     L  ...      2.15  NONE  1987-10-07    NaN\n4  1987  MACKCC-L      CC     L  ...      6.90  NONE  1987-10-07    NaN\n\n[5 rows x 16 columns]\n\ntrout_salamander_py.tail()\n\n       year  sitecode section reach  ...  weight_g  clip  sampledate        notes\n32204  2019  MACKOG-U      OG     U  ...       7.9  NONE  2019-09-05          NaN\n32205  2019  MACKOG-U      OG     U  ...       8.7  NONE  2019-09-05          NaN\n32206  2019  MACKOG-U      OG     U  ...       9.6  NONE  2019-09-05          NaN\n32207  2019  MACKOG-U      OG     U  ...      14.3  NONE  2019-09-05          NaN\n32208  2019  MACKOG-U      OG     U  ...      11.6  NONE  2019-09-05  Terrestrial\n\n[5 rows x 16 columns]"
  },
  {
    "objectID": "code_samples/data-wrangling-with-python-and-R/index.html#class-type",
    "href": "code_samples/data-wrangling-with-python-and-R/index.html#class-type",
    "title": "Data Wrangling with Python and R",
    "section": "Class / Type",
    "text": "Class / Type\n\nR Project RPython Python\n\n\n\nclass(trout_salamander_R)\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\n\n\n\nprint(type(trout_salamander_py))\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;"
  },
  {
    "objectID": "code_samples/data-wrangling-with-python-and-R/index.html#shape",
    "href": "code_samples/data-wrangling-with-python-and-R/index.html#shape",
    "title": "Data Wrangling with Python and R",
    "section": "Shape",
    "text": "Shape\n\nR Project RPython Python\n\n\nHere R and Python both tell us that the dataframe has 32,209 rows and 16 columns.\n\n\n\n\n\n\nNote\n\n\n\nHow to format inline code to include a comma for the thousands separator.\nr format(round(trout_salamander_nrow), big.mark=‘,’)\n\n\n\ndim(trout_salamander_R) # returns the number of rows and columns in a data frame\n\n[1] 32209    16\n\nnrow(trout_salamander_R)\n\n[1] 32209\n\nncol(trout_salamander_R)\n\n[1] 16\n\n\n\n\n\ntrout_salamander_py.shape\n\n(32209, 16)\n\ntrout_salamander_py.shape[0] # number of rows\n\n32209\n\ntrout_salamander_py.shape[1] # number of columns\n\n16"
  },
  {
    "objectID": "code_samples/data-wrangling-with-python-and-R/index.html#summary-describe",
    "href": "code_samples/data-wrangling-with-python-and-R/index.html#summary-describe",
    "title": "Data Wrangling with Python and R",
    "section": "Summary / Describe",
    "text": "Summary / Describe\n\nR Project RPython Python\n\n\n\nsummary(trout_salamander_R)\n\n      year        sitecode           section             reach          \n Min.   :1987   Length:32209       Length:32209       Length:32209      \n 1st Qu.:1998   Class :character   Class :character   Class :character  \n Median :2006   Mode  :character   Mode  :character   Mode  :character  \n Mean   :2005                                                           \n 3rd Qu.:2012                                                           \n Max.   :2019                                                           \n                                                                        \n      pass          unitnum         unittype           vert_index    \n Min.   :1.000   Min.   : 1.000   Length:32209       Min.   :  1.00  \n 1st Qu.:1.000   1st Qu.: 3.000   Class :character   1st Qu.:  5.00  \n Median :1.000   Median : 7.000   Mode  :character   Median : 13.00  \n Mean   :1.224   Mean   : 7.696                      Mean   : 20.17  \n 3rd Qu.:1.000   3rd Qu.:11.000                      3rd Qu.: 27.00  \n Max.   :2.000   Max.   :20.000                      Max.   :147.00  \n                                                                     \n   pitnumber          species           length_1_mm      length_2_mm   \n Min.   :   62048   Length:32209       Min.   : 19.00   Min.   : 28.0  \n 1st Qu.:13713632   Class :character   1st Qu.: 47.00   1st Qu.: 77.0  \n Median :18570447   Mode  :character   Median : 63.00   Median : 98.0  \n Mean   :16286432                      Mean   : 73.83   Mean   :100.5  \n 3rd Qu.:19132429                      3rd Qu.: 97.00   3rd Qu.:119.0  \n Max.   :28180046                      Max.   :253.00   Max.   :284.0  \n NA's   :26574                         NA's   :17       NA's   :19649  \n    weight_g           clip             sampledate            notes          \n Min.   :  0.090   Length:32209       Min.   :1987-10-06   Length:32209      \n 1st Qu.:  1.510   Class :character   1st Qu.:1998-09-04   Class :character  \n Median :  6.050   Mode  :character   Median :2006-09-06   Mode  :character  \n Mean   :  8.903                      Mean   :2005-08-05                     \n 3rd Qu.: 11.660                      3rd Qu.:2012-09-05                     \n Max.   :134.590                      Max.   :2019-09-05                     \n NA's   :13268                                                               \n\n\n\n\n\ntrout_salamander_py.describe()\n\n               year          pass  ...   length_2_mm      weight_g\ncount  32209.000000  32209.000000  ...  12560.000000  18941.000000\nmean    2004.917601      1.223664  ...    100.485191      8.902859\nstd        8.572474      0.416706  ...     34.736955     10.676276\nmin     1987.000000      1.000000  ...     28.000000      0.090000\n25%     1998.000000      1.000000  ...     77.000000      1.510000\n50%     2006.000000      1.000000  ...     98.000000      6.050000\n75%     2012.000000      1.000000  ...    119.000000     11.660000\nmax     2019.000000      2.000000  ...    284.000000    134.590000\n\n[8 rows x 8 columns]\n\n\n\ntrout_salamander_py.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 32209 entries, 0 to 32208\nData columns (total 16 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   year         32209 non-null  int64  \n 1   sitecode     32209 non-null  object \n 2   section      32209 non-null  object \n 3   reach        32209 non-null  object \n 4   pass         32209 non-null  int64  \n 5   unitnum      32209 non-null  float64\n 6   unittype     31599 non-null  object \n 7   vert_index   32209 non-null  int64  \n 8   pitnumber    5635 non-null   float64\n 9   species      32206 non-null  object \n 10  length_1_mm  32192 non-null  float64\n 11  length_2_mm  12560 non-null  float64\n 12  weight_g     18941 non-null  float64\n 13  clip         32209 non-null  object \n 14  sampledate   32209 non-null  object \n 15  notes        3174 non-null   object \ndtypes: float64(5), int64(3), object(8)\nmemory usage: 3.9+ MB"
  },
  {
    "objectID": "code_samples/data-wrangling-with-python-and-R/index.html#variable-names",
    "href": "code_samples/data-wrangling-with-python-and-R/index.html#variable-names",
    "title": "Data Wrangling with Python and R",
    "section": "Variable Names",
    "text": "Variable Names\n\nR Project RPython Python\n\n\n\nnames(trout_salamander_R) # returns column names of a data frame\n\n [1] \"year\"        \"sitecode\"    \"section\"     \"reach\"       \"pass\"       \n [6] \"unitnum\"     \"unittype\"    \"vert_index\"  \"pitnumber\"   \"species\"    \n[11] \"length_1_mm\" \"length_2_mm\" \"weight_g\"    \"clip\"        \"sampledate\" \n[16] \"notes\"      \n\n\n\n\n\ntrout_salamander_py.columns\n\nIndex(['year', 'sitecode', 'section', 'reach', 'pass', 'unitnum', 'unittype',\n       'vert_index', 'pitnumber', 'species', 'length_1_mm', 'length_2_mm',\n       'weight_g', 'clip', 'sampledate', 'notes'],\n      dtype='object')"
  },
  {
    "objectID": "code_samples/data-wrangling-with-python-and-R/index.html#unique",
    "href": "code_samples/data-wrangling-with-python-and-R/index.html#unique",
    "title": "Data Wrangling with Python and R",
    "section": "Unique",
    "text": "Unique\n\nR Project RPython Python\n\n\nGet the unique values from a specified column in a dataframe\n\nunique(trout_salamander_R$species)\n\n[1] \"Cutthroat trout\"            NA                          \n[3] \"Coastal giant salamander\"   \"Cascade torrent salamander\"\n\n\nGet number of unique values\n\nlength(unique(trout_salamander_R$species))\n\n[1] 4\n\n\n\n\nGet unique values of a variable\n\ntrout_salamander_py.species.unique()\n\narray(['Cutthroat trout', nan, 'Coastal giant salamander',\n       'Cascade torrent salamander'], dtype=object)\n\n\nGet number of unique values\n\ntrout_salamander_py.nunique()\n\nyear             33\nsitecode          6\nsection           2\nreach             3\npass              2\nunitnum          25\nunittype          7\nvert_index      147\npitnumber      4530\nspecies           3\nlength_1_mm     181\nlength_2_mm     225\nweight_g       2954\nclip              4\nsampledate       99\nnotes           249\ndtype: int64\n\n\n\ntrout_salamander_py['species'].nunique()\n\n3\n\n\n\nlen(pd.unique(trout_salamander_py['species']))\n\n4\n\n\nGet number of observations for each unique value\n\ntrout_salamander_py.groupby(['species']).count()\n\n                             year  sitecode  section  ...   clip  sampledate  notes\nspecies                                               ...                          \nCascade torrent salamander     15        15       15  ...     15          15      1\nCoastal giant salamander    11758     11758    11758  ...  11758       11758   1199\nCutthroat trout             20433     20433    20433  ...  20433       20433   1971\n\n[3 rows x 15 columns]"
  },
  {
    "objectID": "code_samples/data-wrangling-with-python-and-R/index.html#rename-columns",
    "href": "code_samples/data-wrangling-with-python-and-R/index.html#rename-columns",
    "title": "Data Wrangling with Python and R",
    "section": "Rename columns",
    "text": "Rename columns\n\nR Project RPython Python\n\n\n\ntrout_salamander_R_rename &lt;- trout_salamander_R %&gt;% \n  rename(species_name = species,\n         weight_grams = weight_g)\n\n\n\n\n# {'old_name':'new_name'}\ntrout_salamander_py_rename = trout_salamander_py.rename(columns={'species': 'species_name', 'weight_g': 'weight_grams'})"
  },
  {
    "objectID": "code_samples/data-wrangling-with-python-and-R/index.html#scatter-plot",
    "href": "code_samples/data-wrangling-with-python-and-R/index.html#scatter-plot",
    "title": "Data Wrangling with Python and R",
    "section": "Scatter Plot",
    "text": "Scatter Plot\n\nR Project RPython Python\n\n\n\ntrout_salamander_R &lt;- trout_salamander_R %&gt;% \n  filter(species %in% c('Cutthroat trout', 'Coastal giant salamander'))\n\nggplot(data = trout_salamander_R, aes(x = length_1_mm, y = weight_g)) +\n  geom_point(aes(color = species), show.legend = TRUE) +\n  labs(x = \"Length (mm)\",\n       y = \"Weight (g)\",\n       title = \"Length vs. Weight of Cutthroat Trout and Coastal Giant Salamander Length\",\n       color = \"Species\")\n\n\n\n\n\n\n\nFigure 1: scatter plot length vs. weight visualized with ggplot\n\n\n\n\n\n\n\n\ntrout_salamander_py = trout_salamander_py[trout_salamander_py['species'].isin(['Cutthroat trout','Coastal giant salamander'])]\n\nimport matplotlib.pyplot as plt\n\ncolors = {'Cutthroat trout':'blue', 'Coastal giant salamander':'orange'}\nplt.scatter(x=trout_salamander_py.length_1_mm, y=trout_salamander_py.weight_g, \nc= trout_salamander_py.species.apply(lambda x: colors[x]))\nplt.xlabel('length (mm)')\nplt.ylabel('weight (g)')\nplt.show()\n\n\n\n\n\n\n\nFigure 2: scatter plot of length vs. weight visualized with matplotlib"
  },
  {
    "objectID": "code_samples/data-wrangling-with-python-and-R/index.html#histograms",
    "href": "code_samples/data-wrangling-with-python-and-R/index.html#histograms",
    "title": "Data Wrangling with Python and R",
    "section": "Histograms",
    "text": "Histograms\nThese histograms show the distribution of coastal giant salamander lenths.\n\nR Project RPython Python\n\n\n\nsalamander_R &lt;- trout_salamander_R %&gt;% \n  filter(species == 'Coastal giant salamander')\n\nggplot(data = salamander_R, aes(x = length_1_mm)) +\n  geom_histogram(fill = 'blue', bins = 25) +\n  labs(x = \"lenth (mm)\",\n       title = 'Distribution of Coastal Giant Salamander Length')\n\n\n\n\n\n\n\n\n\n\n\nsalamander_py = trout_salamander_py[ (trout_salamander_py['species'] == 'Coastal giant salamander') ]\n\n\nsalamander_py['length_1_mm'].hist(bins=25, color='green')\nplt.title('Distribution of Coastal Giant Salamander Length')\nplt.xlabel('length (mm)')\nplt.show()"
  },
  {
    "objectID": "code_samples/data-wrangling-with-python-and-R/index.html#bar-plots",
    "href": "code_samples/data-wrangling-with-python-and-R/index.html#bar-plots",
    "title": "Data Wrangling with Python and R",
    "section": "Bar Plots",
    "text": "Bar Plots\nThese bar plots show the averge salamander weight based on site code.\n\nR Project RPython Python\n\n\n\nsalamander_avg_weight_by_sitecode_R &lt;- salamander_R %&gt;% \n  group_by(sitecode) %&gt;% \n  summarise(mean_weight = mean(weight_g, na.rm = TRUE))\n\nggplot(data = salamander_avg_weight_by_sitecode_R, aes(x = sitecode, y = mean_weight)) +\n  geom_col(fill = 'darkgreen') +\n  labs(y = 'weight (g)',\n       title = 'Average Coastal Giant Salamander Weight by Site')\n\n\n\n\n\n\n\n\n\n\n\nsalamander_avg_weight_by_sitecode_py = salamander_py.groupby('sitecode')['weight_g'].mean()\n\nsalamander_avg_weight_by_sitecode_py.plot(kind='bar', rot=0)\nplt.title('Average Coastal Giant Salamander Weight by Site')\nplt.ylabel('weight (g)')\nplt.show()"
  },
  {
    "objectID": "code_samples/data-wrangling-with-python-and-R/index.html#line-plots",
    "href": "code_samples/data-wrangling-with-python-and-R/index.html#line-plots",
    "title": "Data Wrangling with Python and R",
    "section": "Line Plots",
    "text": "Line Plots\nThese line plots show average salamander length over time.\n\nR Project RPython Python\n\n\n\nsalamander_avg_length_by_year_R &lt;- salamander_R %&gt;% \n  group_by(year) %&gt;% \n  summarise(mean_length = mean(length_1_mm, na.rm = TRUE))\n\nggplot(data = salamander_avg_length_by_year_R, aes(x = year, y = mean_length)) +\n  geom_line(color = 'red') +\n  labs(x = 'year',\n       y = 'length (mm)',\n       title = 'Average Coastal Giant Salamander Length by Year')\n\n\n\n\n\n\n\n\n\n\n\nsalamander_avg_lenth_by_year_py = salamander_py.groupby('year')['length_1_mm'].mean()\n\nsalamander_avg_lenth_by_year_py.plot(x='year', y='length_1_mm', kind='line')\nplt.title('Average Coastal Giant Salamander Length by Year')\nplt.ylabel('length (mm)')\nplt.show()"
  },
  {
    "objectID": "code_samples/shiny-apps/index.html",
    "href": "code_samples/shiny-apps/index.html",
    "title": "Shiny Apps",
    "section": "",
    "text": "Snow Today Capstone Project\nVisualize 20 years of snow cover and albedo data from the Western US\n\n\n\nCalifornia Snow Depth Shiny App\nA Shiny App displaying California snow depth data\n\n\n\n\n\n\n\nCitationBibTeX citation:@online{rivers2022,\n  author = {Rivers, Marie},\n  title = {Shiny {Apps}},\n  date = {2022-06-10},\n  url = {https://marierivers.github.io/code_samples/shiny-apps/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nRivers, Marie. 2022. “Shiny Apps.” June 10, 2022. https://marierivers.github.io/code_samples/shiny-apps/."
  },
  {
    "objectID": "posts/2021-11-03-how-i-made-this-visualization/index.html",
    "href": "posts/2021-11-03-how-i-made-this-visualization/index.html",
    "title": "How I made this visualization",
    "section": "",
    "text": "An assignment for my Metadata Standards, Data Modeling and Data Semantics class included using the metajam R package to download Alaskan household languages data and metadata from knb. After reviewing the metadata and reading the data into R, we were tasked with writing code to compute the percentage of Alaskan households that only speak English for the years 2009-2015 and visualizing these results…a straightforward task.\nBut…as I reviewed the numbers I thought about what I actually wanted to capture in the visualization. Did I want to focus on changes over time? There was a slight trend, but nothing significant. The data included State of Alaska Salmon and People Regions (SASAP) so I wanted to show any regional differences.\nHere’s the final visualization\nThe items below outline my data visualization process and helpful resources. Full code for data processing and visualization are included at the end."
  },
  {
    "objectID": "posts/2021-11-03-how-i-made-this-visualization/index.html#download-data-using-metajam",
    "href": "posts/2021-11-03-how-i-made-this-visualization/index.html#download-data-using-metajam",
    "title": "How I made this visualization",
    "section": "Download data using metajam",
    "text": "Download data using metajam\n\n# url to csv file\ndata_url &lt;- \"https://knb.ecoinformatics.org/knb/d1/mn/v2/object/urn%3Auuid%3A7fc6f6db-c5ea-426a-a743-1f2edafb43b8\"\n\n# download the data and metadata to project folder\ndata_path &lt;- metajam::download_d1_data(data_url, \"~/Documents/metajam_example\")\n\n# Read the data and metadata\nhh_list &lt;- metajam::read_d1_files(data_path)\n\n# get the household data frame\nhh_data &lt;- hh_list$data\n\n# get the attribute (columns) metadata\nhh_att_metadata &lt;- hh_list$attribute_metadata"
  },
  {
    "objectID": "posts/2021-11-03-how-i-made-this-visualization/index.html#tidy-data",
    "href": "posts/2021-11-03-how-i-made-this-visualization/index.html#tidy-data",
    "title": "How I made this visualization",
    "section": "Tidy Data",
    "text": "Tidy Data\n\nCode used to process the data\nI adjusted the original data processing code as my plan for the visualization evolved\n\nhousehold_language &lt;- read_csv(here(\"data\", \"doi_10.5063_F1CJ8BPH__household_language__csv\", \"household_language.csv\"))\n\nhh_data_english &lt;- household_language %&gt;%\n  filter(Year &gt;= 2009) %&gt;%\n  filter(total &gt; 0) %&gt;%\n  mutate(percent_only_english = (speak_only_english / total) * 100) %&gt;%\n  relocate(percent_only_english, .before = german) %&gt;%\n  mutate(SASAP.Region = fct_reorder(SASAP.Region, percent_only_english, .fun = mean)) %&gt;%\n  group_by(SASAP.Region, Year) %&gt;%\n  summarise(avg_percent_english = mean(percent_only_english))"
  },
  {
    "objectID": "posts/2021-11-03-how-i-made-this-visualization/index.html#ggplot",
    "href": "posts/2021-11-03-how-i-made-this-visualization/index.html#ggplot",
    "title": "How I made this visualization",
    "section": "ggplot",
    "text": "ggplot\nCode used to create the plot\n\nonly_english_plot &lt;- ggplot(hh_data_english, aes(x = Year, y = SASAP.Region)) +\n  geom_tile(aes(fill = avg_percent_english), show.legend = TRUE) +\n  geom_text(aes(label = paste0(round(avg_percent_english, 0),\"%\")), color = \"white\", size = 3) +\n  scale_fill_gradientn(colors = c(\"antiquewhite3\", \"antiquewhite4\", \"steelblue4\", \"springgreen4\", \"indianred4\")) +\n  theme_minimal() +\n  theme(panel.grid.major = element_blank()) +\n  labs(x = \"Year\", y = NULL,\n       fill = \"Percent\",\n       title = \"Percent of Alaska Households that Only Speak English\",\n       subtitle = \"based on State of Alaska Salmon and People Region\",\n       caption = \"source: Jeanette Clark, Sharis Ochs, Derek Strong, and National Historic Geographic Information System. 2018.\\nLanguages used in Alaskan households, 1990-2015. urn:node:KNB. doi:10.5063/F1N58JPP\") +\n  theme(plot.caption = element_text(size = 8, hjust = 0),\n        plot.caption.position = \"plot\") +\n  theme(plot.title.position = \"plot\") +\n  scale_x_discrete(name = \"Year\", limits = c(2009, 2010, 2011, 2012, 2013, 2014, 2015))\n\nData citation: Jeanette Clark, Sharis Ochs, Derek Strong, and National Historic Geographic Information System. 2018. Languages used in Alaskan households, 1990-2015. urn:node:KNB. doi:10.5063/F1N58JPP.\nhttps://knb.ecoinformatics.org/knb/d1/mn/v2/object/urn%3Auuid%3A7fc6f6db-c5ea-426a-a743-1f2edafb43b8”"
  },
  {
    "objectID": "posts/2022-06-07-the-importance-of-meaningfully-open-snow-data/index.html",
    "href": "posts/2022-06-07-the-importance-of-meaningfully-open-snow-data/index.html",
    "title": "The Importance of Meaningfully Open Snow Data",
    "section": "",
    "text": "For the past 6 months I’ve worked with the Snow Today capstone group. Our initial goal was to develop web-based visualizations of snow cover and albedo data. We worked with output files from a model called SPIReS (Snow Property Inversion from Remote Sensing) that was developed by researchers with the UCSB Earth Research Institute (Bair 2021). SPIReS uses MODIS satellite imagery to estimate snow cover and albedo. MODIS (Moderate Resolution Imaging Spectroradiometer) is a NASA satellite instrument that has collected daily data on Earth’s land, oceans, and atmosphere since the year 2000. MODIS also collects data on the cryosphere, the frozen parts of earth covered by snow and ice. The Snow Today group quickly began brainstorming visualization and website platform strategies, but soon realized we had to take a step back. We had no idea how to even open the datasets.\nThe datasets used in the Snow Today project could be improved by incorporating FAIR data principles (Wilkinson 2016). FAIR stands for Findable, Accessible, Interoperable, and Reusable. The metadata associated with the snow cover and albedo datasets was hard to find and once it was found, it was hard to interpret. The metadata told us the map projection of the data, but this information wasn’t attached in a standard format that could be recognized by common mapping software and spatial packages. This meant that we couldn’t plot the data on a map in the correct location. This also meant that researchers or water managers faced a significant barrier if trying to use the dataset to learn about their local water supply.\nAs we worked through the challenges of the snow cover and albedo datasets, our goals shifted towards creating an open-source workflow to make the data more meaningfully open. While the data used for our project is available online for anyone to download (if you know where to find it), insights can be hard or near impossible to gather without specialized training. After many conversations with the dataset creators, we were able to develop a workflow around the metadata challenges. To see the final product of the Snow Today capstone group, including tutorials to guide others through the steps of repeating our workflow, visit our interactive web app\nQuantifying snow cover area is important because much of the world’s population, from the Western US to High Mountain Asia, relies on winter snowpacks for year-round drinking water, but…\n\nWhy do we care about albedo?\nAlbedo is a measure of how much solar energy is reflected from a surface. Albedo has important climate implications because it determines how much radiation the planet absorbs. Dark surfaces like soil and vegetation have low albedo values while lighter surfaces such as snow have higher albedo values. Dirty snow absorbs more solar radiation and therefore melts faster than clean snow. Since spring snowmelt contributes to drinking water reservoirs in drier months, earlier snowmelt can leave less water in the summer when it’s needed most. A layer of fresh snow increases albedo for that area, which can result in local cooling. When snow melts, it reveals darker surfaces with lower albedo which increases local temperatures and encourages more melting in a feedback loop where the surface absorbs more solar radiation. \n\n\n\n\nCitationBibTeX citation:@online{rivers2022,\n  author = {Rivers, Marie},\n  title = {The {Importance} of {Meaningfully} {Open} {Snow} {Data}},\n  date = {2022-06-07},\n  url = {https://marierivers.github.io/posts/2022-06-07-the-importance-of-meaningfully-open-snow-data/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nRivers, Marie. 2022. “The Importance of Meaningfully Open Snow\nData.” June 7, 2022. https://marierivers.github.io/posts/2022-06-07-the-importance-of-meaningfully-open-snow-data/."
  },
  {
    "objectID": "posts/2022-02-08-how-to-make-a-dumbbell-schedule-with-r/index.html",
    "href": "posts/2022-02-08-how-to-make-a-dumbbell-schedule-with-r/index.html",
    "title": "How to make a dumbbell schedule with R",
    "section": "",
    "text": "Here is a tutorial to create a custom dot plot / dumbbell plot schedule with R and geom_dumbbell and geom_segment. With this template you can also color items by categories such as deliverable, status or quarter.\nOriginal inspiration: and guidance on using geom_dumbbell.\n\nRead and view the data\nI started from an excel sheet.\n\nschedule &lt;- read_excel(here(\"posts\", \"2022-02-08-how-to-make-a-dumbbell-schedule-with-r\", \"schedule.xlsx\"), sheet = \"Sheet1\")\n\n\nkable(schedule) %&gt;% \n  kable_paper(full_width = TRUE) %&gt;% \n  row_spec(0, bold = T) %&gt;% \n  kable_styling(latex_options = \"HOLD_position\")\n\n\n\n\nmilestone\ndeliverable\nstart_date\ndue_date\nstatus\nquarter\n\n\n\n\nDraft Feasibility Study\nFeasibility Study\n2022-01-12\n2022-02-07\ncomplete\nQ1\n\n\nFinal Feasibility Study\nFeasibility Study\n2022-02-12\n2022-03-11\nin progress\nQ1\n\n\nDraft Design Drawings\nDesign Drawings\n2022-01-24\n2022-03-18\nin progress\nQ1\n\n\nFinal Design Drawings\nDesign Drawings\n2022-04-12\n2022-05-20\nnot started\nQ2\n\n\nDraft Technical Documentation\nTechnical Documentation\n2022-03-01\n2022-03-18\nnot started\nQ1\n\n\nFinal Technical Documentation\nTechnical Documentation\n2022-03-28\n2022-04-22\nnot started\nQ2\n\n\nStakeholder Presentation\nPresentation\nNA\n2022-03-25\nnot started\nQ1\n\n\nDraft Web Application\nWeb Application\n2022-02-18\n2022-03-16\nnot started\nQ1\n\n\nFinal Web Application\nWeb Application\n2022-03-28\n2022-05-27\nnot started\nQ2\n\n\nClient Presentation\nPresentation\nNA\n2022-06-03\nnot started\nQ2\n\n\n\n\n\n\n\n\n\nClean the data\nPay attention to date formats with class(schedule$start_date). Cleaning steps created a field for month abbreviation + day of month to use as labels. The deliverable category was reordered based on due date using fct_reorder.\n\nschedule &lt;- schedule %&gt;% \n  mutate(start_month_num = month(start_date)) %&gt;% \n  mutate(end_month_num = month(due_date)) %&gt;% \n  mutate(start_month_name = case_when(\n    start_month_num == 1 ~ \"Jan\",\n    start_month_num == 2  ~ \"Feb\",\n    start_month_num == 3 ~ \"Mar\",\n    start_month_num == 4 ~ \"Apr\",\n    start_month_num == 5 ~ \"May\",\n    start_month_num == 6 ~ \"Jun\",\n  )) %&gt;% \n  mutate(end_month_name = case_when(\n    end_month_num == 1 ~ \"Jan\",\n    end_month_num == 2 ~ \"Feb\",\n    end_month_num == 3 ~ \"Mar\",\n    end_month_num == 4 ~ \"Apr\",\n    end_month_num == 5 ~ \"May\",\n    end_month_num == 6 ~ \"Jun\",\n  )) %&gt;% \n  mutate(start_label = paste(start_month_name, day(start_date))) %&gt;% \n  mutate(end_label = paste(end_month_name, day(due_date))) %&gt;% \n  mutate(milestone = as_factor(milestone)) %&gt;% \n  mutate(milestone = fct_reorder(milestone, as.numeric(due_date), .desc = TRUE))\n\nThis step was used for the bounds of the rectangle highlighting the second quarter.\n\nQ2 &lt;- schedule %&gt;% \n  filter(quarter == \"Q2\")\nxmin_Q2 &lt;- as.POSIXct(min(Q2$start_date, na.rm = TRUE))\nxmax_Q2 &lt;- as.POSIXct(max(Q2$due_date, na.rm = TRUE))\n\nThis step was used to set the x-axis limits when using scale_x_datetime.\n\nmin &lt;- as.POSIXct(\"2022-1-1\")\nmax &lt;- as.POSIXct(\"2022-6-15\")\n\nI originally used geom_dumbbell for the start and end dots, but using geom_point twice gave more flexibility for colors and made it easy add transparency to the start dots. The geom_dumbbell code is include for reference.\n\ntime_plot &lt;- ggplot(data = schedule, aes(y = milestone)) +\n  geom_rect(aes(xmin = xmin_Q2, ymin = -Inf,\n                xmax = xmax_Q2, ymax = Inf),\n                fill = \"grey80\", alpha = 0.5) +\n  # create a thick line between x and xend instead of using default provided by geom_dumbbell\n  geom_segment(aes(x = start_date, xend = due_date, y = milestone, yend = milestone), \n               color = \"grey50\", \n               size = 1.5) +\n  # geom_dumbbell(color = \"grey80\", size_x = 5, size_xend = 5,\n  #               colour_x = \"blue\", colour_xend = \"red\") +\n  geom_point(data = schedule, aes(x = start_date, y = milestone,\n                              color = deliverable), size = 5, alpha = 0.5,\n             show.legend = FALSE) +\n  # scale_colour_paletteer_d(\"Redmonder::qMSOPap\") +\n  # scale_colour_paletteer_d(\"Redmonder::qMSOPap\") +\n  # scale_colour_paletteer_d(\"Redmonder::qMSOMed\") +\n  # scale_colour_paletteer_d(\"palettetown::pelipper\") +\n  scale_colour_manual(values = c(\"#007a76\", \"#b7245c\", \"#ca7f0e\", \"#0d4fbd\", \"#785ceb\")) + # manually specify colors or use an existing palette\n  geom_point(data = schedule, aes(x = due_date, y = milestone,\n                              color = deliverable), size = 5,\n             show.legend = FALSE) +\n  labs(x = NULL, y = NULL,\n       title = \"Milestone Timeline\") +\n  geom_text(color = \"black\", size = 3, hjust = 1.5,\n            aes(x = start_date, label = start_label)) +\n  geom_text(color = \"black\", size = 3, hjust = -0.5,\n            aes(x = due_date, label = end_label)) +\n  theme_minimal() +\n  theme(panel.grid = element_blank()) +\n  theme(plot.title.position = \"plot\") +\n  scale_x_datetime(limits = c(min, max)) +\n  annotate(\"text\", x = as.POSIXct(\"2022-5-1\"), y = schedule$milestone[1], label = \"Q2\", color = \"black\", size = 5)\ntime_plot\n\nWarning: Removed 2 rows containing missing values (geom_segment).\n\n\nWarning: Removed 2 rows containing missing values (geom_point).\n\n\nWarning: Removed 2 rows containing missing values (geom_text).\n\n\n\n\n\n\n\n\n\n\n\n\n\nCitationBibTeX citation:@online{rivers2022,\n  author = {Rivers, Marie},\n  title = {How to Make a Dumbbell Schedule with {R}},\n  date = {2022-02-08},\n  url = {https://marierivers.github.io/posts/2022-02-08-how-to-make-a-dumbbell-schedule-with-r/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nRivers, Marie. 2022. “How to Make a Dumbbell Schedule with\nR.” February 8, 2022. https://marierivers.github.io/posts/2022-02-08-how-to-make-a-dumbbell-schedule-with-r/."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Marie T. Rivers",
    "section": "",
    "text": "LinkedIn\n  \n  \n      Twitter\n  \n  \n      GitHub\n  \n\n  \n  \n\nEducation\n\n\nMaster of Environmental Data Science\n\nBren School of Environmental Science & Management University of California, Santa Barbara (2022)\n\nMS in Environmental Engineering\n\nDepartment of Civil and Environmental Engineering University of Massachusetts, Amherst (2011)\n\nBS in Environmental Engineering\n\nDepartment of Civil and Environmental Engineering University of Delaware (2009)\n\n\n\n\nMarie is a graduate of UC Santa Barbara’s Bren School of Environmental Science and Management inaugural environmental data science cohort. She is also a registered professional civil water resources engineer with 10 years of experience modeling and designing municipal drinking water distribution systems. During this time, she observed the growing need for reproducible data analysis and visualization skills to solve current and emerging environmental problems. Marie is interested in using geospatial analyses to understand interactions between human and natural systems so that these insights can advance renewable energy technologies that support climate change goals."
  },
  {
    "objectID": "presentations/wind_resource_presentation.html#data-workflow",
    "href": "presentations/wind_resource_presentation.html#data-workflow",
    "title": "Marie T. Rivers",
    "section": "Data Workflow",
    "text": "Data Workflow\n\n\n\nExplored diurnal and monthly variability of wind resources at Mount Washington in NH\nUsed the NREL Wind Integration National Dataset (WIND) Toolkit\nAccessed data with the h5pyd Python package and NREL Highly Scalable Data Service (HSDS)\nSubset the for the windspeed_100m, dataset and year 2012\nConverted to pandas dataframe\nAggregated data by hourly and monthly groupings to calculate mean, standard deviation, and quartiles"
  },
  {
    "objectID": "presentations/wind_resource_presentation.html#visualization-of-full-time-series",
    "href": "presentations/wind_resource_presentation.html#visualization-of-full-time-series",
    "title": "Marie T. Rivers",
    "section": "Visualization of full time series",
    "text": "Visualization of full time series\n\ngraphcode\n\n\n\n\n                        \n                                            \n\n\n\n\n\nfig = go.Figure([\n    go.Scatter(x = windspeed_100m_df.index, y = windspeed_100m_df['windspeed_100m'], \n              mode = 'lines', legendrank = 1, \n              name = 'hourly', line=dict(color='blue', width=0.75)),\n    go.Scatter(x = moving_averages_24hr.index, y = moving_averages_24hr['windspeed_100m'], \n              mode = 'lines', legendrank = 1,\n              name = '24 hour avg', line=dict(color='green', width=1), visible='legendonly'),\n    go.Scatter(x = moving_averages_10day.index, y = moving_averages_10day['windspeed_100m'], \n              mode = 'lines', legendrank = 1, \n              name = '10 day avg', line=dict(color='red', width=1), visible='legendonly'),\n    go.Scatter(x = moving_averages_30day.index, y = moving_averages_30day['windspeed_100m'], \n              mode = 'lines', legendrank = 1, \n              name = '30 day avg', line=dict(color='yellow', width=3), visible='legendonly')\n])\n\nfig.update_layout(\n    margin=dict(l=20, r=20, t=30, b=20),\n    paper_bgcolor=\"#FFFFFF\",\n    plot_bgcolor='#f5f5f5',\n    yaxis=dict(\n        title_text=\"windspeed (m/s)\",\n        titlefont=dict(size=16)),\n    title={\n        'text': \"Hourly Wind Speed\",\n        'y':0.99,\n        'x':0.5,\n        'xanchor': 'center',\n        'yanchor': 'top'}\n)"
  },
  {
    "objectID": "presentations/wind_resource_presentation.html#diurnal-and-monthly-variability",
    "href": "presentations/wind_resource_presentation.html#diurnal-and-monthly-variability",
    "title": "Marie T. Rivers",
    "section": "Diurnal and Monthly Variability",
    "text": "Diurnal and Monthly Variability\n\nBoth wind speed and electricity demands fluctuate throughout the day and seasonally\nWind and electricity patterns may not match\nUnlike water resources, electricity can be challenging to store during low demands\nWhen selecting sites for utility scale wind power it is important to have adequate wind speeds at the same time as peak electricity demands\nStatistics such as the interquartile range and standard deviation can help quantify the spread of data"
  },
  {
    "objectID": "presentations/wind_resource_presentation.html#diurnal-and-monthly-variability-1",
    "href": "presentations/wind_resource_presentation.html#diurnal-and-monthly-variability-1",
    "title": "Marie T. Rivers",
    "section": "Diurnal and Monthly Variability",
    "text": "Diurnal and Monthly Variability\n\n\n\nHourly average\n\ngraphcode\n\n\n\n\n                        \n                                            \n\n\n\n\n\nfig = go.Figure([\n    go.Scatter(name = 'mean', y = hourly_avg['mean'], x = hourly_avg['hour'], mode = 'lines',\n              line = dict(color = \"blue\", width = 4),\n              error_y = dict(type = 'data', array = hourly_avg['std'], visible = True)),\n    go.Scatter(\n        name = 'IQR 75', y = hourly_avg['quantile75'], x = hourly_avg['hour'],\n        mode='lines',\n        marker=dict(color=\"#444\"),\n        line=dict(width=0),\n        #legendgroup = 'IQR',\n        showlegend = False\n    ),\n    # Create IQR 25 fill color\n    go.Scatter(\n        name='IQR', y = hourly_avg['quantile25'], x = hourly_avg['hour'],\n        marker=dict(color=\"#444\"),\n        line=dict(width=0),\n        mode='lines',\n        fillcolor='rgba(68, 68, 68, 0.3)',\n        fill='tonexty', # fill to next y\n        legendgroup = 'IQR',\n        showlegend = True\n    )\n])\nfig.update_layout(\n    xaxis=dict(\n        title_text=\"hour (UTC)\",\n        titlefont=dict(size=16),\n        dtick = 2),\n    yaxis=dict(\n        title_text=\"windspeed (m/s)\",\n        titlefont=dict(size=16)),\n    title={\n        'text': \"Average Hourly Wind Speed for the Year 2012\",\n        'y':0.99,\n        'x':0.5,\n        'xanchor': 'center',\n        'yanchor': 'top'},\n    margin=dict(l=20, r=20, t=30, b=20),\n    paper_bgcolor=\"#FFFFFF\",\n    plot_bgcolor='#f5f5f5'\n)\n\n\n\n\n\nMonthly average\n\ngraphcode\n\n\n\n\n                        \n                                            \n\n\n\n\n\nfig = go.Figure([\n    go.Scatter(name = 'mean', y = monthly_avg['mean'], x = monthly_avg['month'], \n              mode = 'lines', line = dict(color = \"blue\", width = 4),\n              error_y = dict(type = 'data', array = monthly_avg['std'], visible = True)),\n    go.Scatter(\n        name = 'IQR 75', y = monthly_avg['quantile75'], x = monthly_avg['month'],\n        mode='lines', marker=dict(color=\"#444\"), line=dict(width=0),\n        showlegend = False\n    ),\n\n    # Create IQR 25 fill color\n    go.Scatter(\n        name='IQR', y = monthly_avg['quantile25'], x = monthly_avg['month'],\n        marker=dict(color=\"#444\"), line=dict(width=0), mode='lines',\n        fillcolor='rgba(68, 68, 68, 0.3)',\n        fill='tonexty', # fill to next y\n        legendgroup = 'IQR',\n        showlegend = True)\n])\nfig.update_layout(\n    xaxis=dict(\n        title_text=\"month\",\n        titlefont=dict(size=16),\n        dtick = 1),\n    yaxis=dict(\n        title_text=\"windspeed (m/s)\",\n        titlefont=dict(size=16)),\n    title={\n        'text': \"Average Monthly Wind Speed for the Year 2012\",\n        'y':0.99,\n        'x':0.5,\n        'xanchor': 'center',\n        'yanchor': 'top'},\n    margin=dict(l=20, r=20, t=30, b=20),\n    paper_bgcolor=\"#FFFFFF\",\n    plot_bgcolor='#f5f5f5'\n)"
  },
  {
    "objectID": "presentations/wind_resource_presentation.html#diurnal-and-monthly-variability-2",
    "href": "presentations/wind_resource_presentation.html#diurnal-and-monthly-variability-2",
    "title": "Marie T. Rivers",
    "section": "Diurnal and Monthly Variability",
    "text": "Diurnal and Monthly Variability\n\ngraphcode\n\n\n\n\n                        \n                                            \n\n\n\n\n\nfig = go.Figure([\n    go.Scatter(y = hourly_avg_by_month['1'], x = hourly_avg_by_month.index, \n              mode = 'lines', legendrank = 1, \n              name = 'January', line=dict(color='#DC050C', width=2)),\n    go.Scatter(y = hourly_avg_by_month['2'], x = hourly_avg_by_month.index,\n              mode = 'lines+markers', legendrank = 2, \n              name = 'February', line=dict(color='#E8601c', width=2)),\n    go.Scatter(y = hourly_avg_by_month['3'], x = hourly_avg_by_month.index, \n              mode = 'lines', legendrank = 3, \n              name = 'March', line=dict(color='#f4a736', width=2)),\n    go.Scatter(y = hourly_avg_by_month['4'], x = hourly_avg_by_month.index, \n              mode = 'lines+markers', legendrank = 4, \n              name = 'April', line=dict(color='#f7f056', width=2)),\n    go.Scatter(y = hourly_avg_by_month['5'], x = hourly_avg_by_month.index, \n              mode = 'lines', legendrank = 5, \n              name = 'May', line=dict(color='#cae0ab', width=2)),\n    go.Scatter(y = hourly_avg_by_month['6'], x = hourly_avg_by_month.index, \n              mode = 'lines+markers', legendrank = 6, \n              name = 'June', line=dict(color='#4eb265', width=2)),\n    go.Scatter(y = hourly_avg_by_month['7'], x = hourly_avg_by_month.index, \n              mode = 'lines', legendrank = 7, \n              name = 'July', line=dict(color='#7bafde', width=2)),\n    go.Scatter(y = hourly_avg_by_month['8'], x = hourly_avg_by_month.index, \n              mode = 'lines+markers', legendrank = 8, \n              name = 'August', line=dict(color='#5289c7', width=2)),\n    go.Scatter(y = hourly_avg_by_month['9'], x = hourly_avg_by_month.index, \n              mode = 'lines', legendrank = 9, \n              name = 'September', line=dict(color='#1965b0', width=2)),\n    go.Scatter(y = hourly_avg_by_month['10'], x = hourly_avg_by_month.index, \n              mode = 'lines+markers', legendrank = 10, \n              name = 'October', line=dict(color='#882e72', width=2)),\n    go.Scatter(y = hourly_avg_by_month['11'], x = hourly_avg_by_month.index, \n              mode = 'lines', legendrank = 11, \n              name = 'November', line=dict(color='#ae76a3', width=2)),\n    go.Scatter(y = hourly_avg_by_month['12'], x = hourly_avg_by_month.index, \n              mode = 'lines+markers', legendrank = 12, \n              name = 'December', line=dict(color='#d1bbd7', width=2)),\n    go.Scatter(name = 'annual mean', y = hourly_avg['mean'], x = hourly_avg['hour'], mode = 'lines',\n              line = dict(color = \"black\", width = 5))\n\n])\n\nvariables_to_hide = ['February', 'March', 'April', 'May', 'June', 'July',\n                    'August', 'September', 'October', 'November', 'December']\nfig.for_each_trace(lambda trace: trace.update(visible=\"legendonly\") \n                   if trace.name in variables_to_hide else ())\n                   \nfig.update_layout(\n    xaxis=dict(\n        title_text=\"hour (UTC)\",\n        titlefont=dict(size=16),\n        dtick = 4),\n    yaxis=dict(\n        title_text=\"windspeed (m/s)\",\n        titlefont=dict(size=16)),\n    title={\n        'text': \"Average Hourly Wind Speed by Month\",\n        'y':0.99,\n        'x':0.5,\n        'xanchor': 'center',\n        'yanchor': 'top'},\n    margin=dict(l=20, r=20, t=30, b=20),\n    paper_bgcolor=\"#FFFFFF\",\n    plot_bgcolor='#f5f5f5'\n)"
  },
  {
    "objectID": "presentations/wind_resource_presentation.html#diurnal-and-monthly-variability-3",
    "href": "presentations/wind_resource_presentation.html#diurnal-and-monthly-variability-3",
    "title": "Marie T. Rivers",
    "section": "Diurnal and Monthly Variability",
    "text": "Diurnal and Monthly Variability\n\ngraphcode\n\n\n\n\n                        \n                                            \n\n\n\n\n\nheatmap_month = hourly_avg_by_month.columns.tolist()\nheatmap_hour = hourly_avg_by_month.index.tolist()\nheatmap_windspeed = hourly_avg_by_month.values.tolist()\n\ntrace = go.Heatmap(\n   x = heatmap_month,\n   y = heatmap_hour,\n   z = heatmap_windspeed,\n   type = 'heatmap',\n   #colorscale = [(0,\"blue\"), (1,\"red\")],\n   colorscale = 'mint',\n   colorbar=dict(title='Wind Speed (m/s)')\n)\ndata = [trace]\nfig = go.Figure(data = data)\n\nfig.update_layout(\n    #width=1000,\n    height=650,\n    xaxis=dict(\n        title_text=\"month\",\n        titlefont=dict(size=16),\n        #dtick = 1,\n        tickmode = 'array',\n        # Set tick intervals to correspond with months\n        tickvals = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12],\n        ticktext = ['January', 'February', 'March', 'April', \n                    'May', 'June', 'July', 'August', \n                    'September', 'October', 'November', 'December'],\n        tickfont = dict(size=16)),\n    yaxis=dict(\n        title_text=\"hour (UTC)\",\n        titlefont=dict(size=16),\n        dtick = 1,\n        tickfont = dict(size=16)),\n    title={\n        'text': \"Average Wind Speed by Month and Hour\",\n        'y':0.99,\n        'x':0.5,\n        'xanchor': 'center',\n        'yanchor': 'top'},\n    margin=dict(l=20, r=20, t=30, b=20),\n)"
  },
  {
    "objectID": "presentations/wind_resource_presentation.html#standard-deviation",
    "href": "presentations/wind_resource_presentation.html#standard-deviation",
    "title": "Marie T. Rivers",
    "section": "Standard Deviation",
    "text": "Standard Deviation\n\ngraphcode\n\n\n\n\n                        \n                                            \n\n\n\n\n\nstd_heatmap_month = hourly_std_by_month.columns.tolist()\nstd_heatmap_hour = hourly_std_by_month.index.tolist()\nstd_heatmap_windspeed = hourly_std_by_month.values.tolist()\n\ntrace = go.Heatmap(\n   x = std_heatmap_month,\n   y = std_heatmap_hour,\n   z = std_heatmap_windspeed,\n   type = 'heatmap',\n   colorscale = 'Blues',\n   colorbar=dict(title='Standard Deviation (m/s)')\n)\ndata = [trace]\nfig = go.Figure(data = data)\n\nfig.update_layout(\n    #width=1000,\n    height=650,\n    xaxis=dict(\n        titlefont=dict(size=16),\n        #dtick = 1,\n        tickmode = 'array',\n        # Set tick intervals to correspond with months\n        tickvals = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12],\n        ticktext = ['January', 'February', 'March', 'April', \n                    'May', 'June', 'July', 'August', \n                    'September', 'October', 'November', 'December'],\n        tickfont = dict(size=16)),\n    yaxis=dict(\n        title_text=\"hour (UTC)\",\n        titlefont=dict(size=16),\n        dtick = 1,\n        tickfont = dict(size=16)),\n    title={\n        'text': \"Wind Speed Standard Deviation by Month and Hour\",\n        'y':0.99,\n        'x':0.5,\n        'xanchor': 'center',\n        'yanchor': 'top'},\n    margin=dict(l=20, r=20, t=30, b=20),\n)"
  },
  {
    "objectID": "presentations/wind_resource_presentation.html#statistical-summary",
    "href": "presentations/wind_resource_presentation.html#statistical-summary",
    "title": "Marie T. Rivers",
    "section": "Statistical Summary",
    "text": "Statistical Summary\n\n\n\n\nmin wind speed (m/s)\nmax wind speed (m/s)\n\n\n\n\nhourly\n0.1\n36.7\n\n\nhourly average\n10.7\n12.5\n\n\nmonthly average\n8.8\n15.8\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\nsmallest variability\ngreatest variability\nsmallest average wind speed\ngreatest average wind speed\n\n\n\n\nmonthly\nJuly\nNovember\nAugust\nFebruary\n\n\nhourly\n19\n13\n16\n1"
  },
  {
    "objectID": "presentations/wind_resource_presentation.html#conclusions",
    "href": "presentations/wind_resource_presentation.html#conclusions",
    "title": "Marie T. Rivers",
    "section": "Conclusions",
    "text": "Conclusions\n\nGreatest monthly variability in November\nSmallest monthly variability in July\nHighest wind speeds in February\nLowest wind speeds in August\nSlower wind speeds mid-day than at night\nBased on the seasonal variability, this site would be better at meeting high winter demands than summer demands\nThis site may not be ideal for meeting all daytime demands."
  },
  {
    "objectID": "presentations/wind_resource_presentation.html#expanded-geographic-scale",
    "href": "presentations/wind_resource_presentation.html#expanded-geographic-scale",
    "title": "Marie T. Rivers",
    "section": "Expanded Geographic Scale",
    "text": "Expanded Geographic Scale\nCreated a parameterized report using Quarto as a tool to allow users to generate summary reports based on specified inputs from CLI or with render function for multiple sites.\n\n\nModify by specifying parameters for:\n\nsite name\nsite latitude\nsite longitude\nstart date\nend date\nturbine cut-in speed\nturbine cut-out speed\nrequired annual average wind speed\n\n\nDefault parameters are:\n\nstart date: ‘2012-01-01’\nend date: ‘2013-01-01’\ncut-in speed: 3.6 m/s\ncut-out speed: 24.6 m/s\nrequired annual avg speed: 5.8 m/s"
  },
  {
    "objectID": "presentations/wind_resource_presentation.html#expanded-geographic-scale-1",
    "href": "presentations/wind_resource_presentation.html#expanded-geographic-scale-1",
    "title": "Marie T. Rivers",
    "section": "Expanded Geographic Scale",
    "text": "Expanded Geographic Scale\nIn the CLI the example code below renders a report for NYC in 2010 using default values for cut-in speed, cut-out speed, and required annual average wind speed.\n\nquarto render report.qmd -P site_name:“New York City” -P site_lat:40.7128 -P site_lon:-74.0059 -P start_date:2010-01-01 -P end_date:2011-01-01 --output new_york_city_report.pdf (&gt;)\n\nThis function generates multiples reports from a dataframe of parameters for different sites.\n\nrender_fun &lt;- function(param_df){\n  quarto::quarto_render(\n    input = \"report.qmd\",\n    execute_params = list(site_name = param_df$site_name,\n                          site_lat = param_df$site_lat,\n                          site_lon = param_df$site_lon,\n                          start_date = param_df$start_date,\n                          end_date = param_df$end_date),\n    output_file = glue::glue(\"{param_df$site_name}-report.pdf\"))}\n\nparam_list &lt;- split(report_parameters, seq(nrow(report_parameters))) %&gt;% \n  purrr::walk(render_fun)"
  },
  {
    "objectID": "presentations/wind_resource_presentation.html#report",
    "href": "presentations/wind_resource_presentation.html#report",
    "title": "Marie T. Rivers",
    "section": "Report",
    "text": "Report\nFor the specified site, the report answers the following questions:\n\nIs the annual average wind speed at least 13 mph (5.8 m/s)? 1\nHow often the wind is below the cut-in speed of 8 mph (3.6 m/s)? 2\nHow often the wind exceed the cut-out speed of 55 mph (24.6 m/s)?\n\nUniversity of Delaware\nUMass Amherst\nUC Santa Barbara\nNREL - Golden, CO\nThe U.S. Energy Information Administration recommends an annual average wind speed of at least 9 mph (4 m/s) for small wind turbines and 13 mph (5.8 m/s) for utility-scale turbines. https://www.eia.gov/energyexplained/wind/where-wind-power-is-harnessed.php#:~:text=Good%20places%20for%20wind%20turbines,)%20for%20utility%2Dscale%20turbines.The Office of Energy Efficiency & Renewable Energy notes a typical cut-in speed of 6 to 9 mpg and cut-out speed of 55 mph. https://www.energy.gov/eere/articles/how-do-wind-turbines-survive-severe-storms"
  },
  {
    "objectID": "presentations/wind_resource_presentation.html#citations",
    "href": "presentations/wind_resource_presentation.html#citations",
    "title": "Marie T. Rivers",
    "section": "Citations",
    "text": "Citations\nDraxl, C., B.M. Hodge, A. Clifton, and J. McCaa. 2015. Overview and Meteorological Validation of the Wind Integration National Dataset Toolkit (Technical Report, NREL/TP-5000-61740). Golden, CO: National Renewable Energy Laboratory.\nDraxl, C., B.M. Hodge, A. Clifton, and J. McCaa. 2015. “The Wind Integration National Dataset (WIND) Toolkit.” Applied Energy 151: 355366.\nKing, J., A. Clifton, and B.M. Hodge. 2014. Validation of Power Output for the WIND Toolkit (Technical Report, NREL/TP-5D00-61714). Golden, CO: National Renewable Energy Laboratory.\nhttps://www.eia.gov/electricity/gridmonitor/dashboard/electric_overview/US48/US48\nhttps://www.eia.gov/energyexplained/wind/where-wind-power-is-harnessed.php#:~:text=Good%20places%20for%20wind%20turbines,)%20for%20utility%2Dscale%20turbines.\n\n\n\nhttps://marierivers.github.io/wind_resource_temporal_variability/"
  },
  {
    "objectID": "presentations/snow_today_presentation.html#section",
    "href": "presentations/snow_today_presentation.html#section",
    "title": "Snow Today",
    "section": "",
    "text": "Knowing the spatial extent of snow cover is critical for water management and winter recreation. Climate change will affect the variability of frozen water resources."
  },
  {
    "objectID": "presentations/snow_today_presentation.html#section-1",
    "href": "presentations/snow_today_presentation.html#section-1",
    "title": "Snow Today",
    "section": "",
    "text": "Snow Science: UCSB CUES Field Station Site Visit"
  },
  {
    "objectID": "presentations/snow_today_presentation.html#albedo-importance",
    "href": "presentations/snow_today_presentation.html#albedo-importance",
    "title": "Snow Today",
    "section": "Albedo Importance",
    "text": "Albedo Importance\n\n\n\nRegulates the Earth’s temperature by reflecting solar radiation  \nInfluences rate of snow melt  \nParticularly important in the Western US  \nAccurate estimates critical for climate models and predicting water storage"
  },
  {
    "objectID": "presentations/snow_today_presentation.html#snow-today",
    "href": "presentations/snow_today_presentation.html#snow-today",
    "title": "Snow Today",
    "section": "Snow Today",
    "text": "Snow Today\n\n\n\nScientific analysis website that provides data on snow conditions from satellite and surface measurements  \nUsed by scientists, water managers, and outdoor enthusiasts for snow observations  \nSpatial products offered include measures of snow cover extent and albedo"
  },
  {
    "objectID": "presentations/snow_today_presentation.html#snow-today-usability",
    "href": "presentations/snow_today_presentation.html#snow-today-usability",
    "title": "Snow Today",
    "section": "Snow Today: Usability",
    "text": "Snow Today: Usability\n\n\n\n\nSnow Today can be hard to navigate for new users unfamiliar with the website’s layout\nVisualizations are of current snow conditions, and have limited customization options\nSnow cover and albedo files are hard to find\nData format may be challenging for new users\nSnow metadata is stored in a non-standardized format which is difficult for some software to interpret the data\nUsers may have trouble processing and analyzing snow data without the help"
  },
  {
    "objectID": "presentations/snow_today_presentation.html#snow-today-visualizations",
    "href": "presentations/snow_today_presentation.html#snow-today-visualizations",
    "title": "Snow Today",
    "section": "Snow Today: Visualizations",
    "text": "Snow Today: Visualizations"
  },
  {
    "objectID": "presentations/snow_today_presentation.html#objectives",
    "href": "presentations/snow_today_presentation.html#objectives",
    "title": "Snow Today",
    "section": "Objectives",
    "text": "Objectives\n\n\n\n\nCreate an open source workflow for processing and visualizing snow data\n\n\n\nProvide recommendations for the Snow Today website\n\n\n\n\n\nCreate interactive visualizations\n\n\n\n\nImprove data usability through tutorials in Python"
  },
  {
    "objectID": "presentations/snow_today_presentation.html#website-recommendations",
    "href": "presentations/snow_today_presentation.html#website-recommendations",
    "title": "Snow Today",
    "section": "Website Recommendations",
    "text": "Website Recommendations\nwebsite architecture"
  },
  {
    "objectID": "presentations/snow_today_presentation.html#website-recommendations-1",
    "href": "presentations/snow_today_presentation.html#website-recommendations-1",
    "title": "Snow Today",
    "section": "Website Recommendations",
    "text": "Website Recommendations"
  },
  {
    "objectID": "presentations/snow_today_presentation.html#objectives-1",
    "href": "presentations/snow_today_presentation.html#objectives-1",
    "title": "Snow Today",
    "section": "Objectives",
    "text": "Objectives\n\n\n\n\nCreate an open source workflow for processing and visualizing snow data\n\nProvide recommendations for the Snow Today website\n\n2. Create interactive visualizations\n\nImprove data usability through tutorials in Python"
  },
  {
    "objectID": "presentations/snow_today_presentation.html#interactive-visualizations",
    "href": "presentations/snow_today_presentation.html#interactive-visualizations",
    "title": "Snow Today",
    "section": "Interactive Visualizations",
    "text": "Interactive Visualizations\n\n\nPrototype Web Application\n  Daily maps of snow cover and albedo for any selected date"
  },
  {
    "objectID": "presentations/snow_today_presentation.html#interactive-visualizations-1",
    "href": "presentations/snow_today_presentation.html#interactive-visualizations-1",
    "title": "Snow Today",
    "section": "Interactive Visualizations",
    "text": "Interactive Visualizations\n\n\nMonthly Average and Anomaly\n  Users can select a specific month, water year, and variable to view averages on anomalies."
  },
  {
    "objectID": "presentations/snow_today_presentation.html#interactive-visualizations-2",
    "href": "presentations/snow_today_presentation.html#interactive-visualizations-2",
    "title": "Snow Today",
    "section": "Interactive Visualizations",
    "text": "Interactive Visualizations\nAnnual Comparisons"
  },
  {
    "objectID": "presentations/snow_today_presentation.html#interactive-visualizations-3",
    "href": "presentations/snow_today_presentation.html#interactive-visualizations-3",
    "title": "Snow Today",
    "section": "Interactive Visualizations",
    "text": "Interactive Visualizations\n\ngraphcode\n\n\n\n\n                        \n                                            \n\n\n\n\n\nsnow_cover_df = pd.read_csv('data/snow_cover_df.csv')\nsnow_cover_df = snow_cover_df.fillna(0)\n\n# Create empty list to input with for loop\nIQR_25 = []\nIQR_75 = []\nIQR_50 = []\ndays = []\nfor i in range(len(snow_cover_df)): \n    #Takes the IQR of each day (25, 50, 75)\n    Q1 = np.percentile(snow_cover_df.iloc[i], 25)\n    Q2 = np.percentile(snow_cover_df.iloc[i], 50)\n    Q3 = np.percentile(snow_cover_df.iloc[i], 75)\n    #appends list with IQR outputs\n    IQR_25.append(Q1)\n    IQR_50.append(Q2)\n    IQR_75.append(Q3)\n    #Creates day list to append dataset with\n    days.append(i + 1)\n    \n# Next, need to create a single column of mean values. \nsnow_cover_df['Average Snow Cover'] = snow_cover_df.mean(axis = 1)\n\n#Appends list for loop lists\nsnow_cover_df['IQR_25'] = IQR_25\nsnow_cover_df['IQR_75'] = IQR_75\nsnow_cover_df['IQR_50'] = IQR_50\nsnow_cover_df['days'] = days\n\nmonth_day = [31, 30, 31, 31, 28, 31, 30, 31, 30, 31, 31, 30]\nnew_list = []\n\nj = 0 \nfor i in range(0,len(month_day)):\n    j+=month_day[i]\n    new_list.append(j)\n    \n# Create a list of years to graph. legend rank allows lets you order where the lines are located on the chart. \nfor i in range(len(snow_cover_df)):\n    print(\"\"\"go.Scatter(\"\"\"\n        \"\"\"name = '\"\"\" + str(i + 2001) + \"\"\"', \"\"\"\n        \"\"\"y = snow_cover_df['\"\"\"+ str(i + 2001) + \"\"\"'], x = snow_cover_df['days'], \"\"\"\n        \"\"\"mode = 'lines', legendrank = \"\"\" + str(19-i) + \"\"\"),\"\"\"\n    )\n#Plot the figure. \nfig = go.Figure([\n\n#create median line\ngo.Scatter(\n    #Name that appears on legend\n    name = 'Median',\n    # y-dim\n    y = snow_cover_df['IQR_50'],\n    # x-dim\n    x = snow_cover_df['days'],\n    # type of plot\n    mode = 'lines',\n    # Include to select/deselect multiple variables at once\n    legendgroup = 'IQR',\n    # Name of legend group on legend\n    legendgrouptitle_text=\"&lt;b&gt;Interquartile Range&lt;/b&gt;\",\n    # Legend position\n    legendrank = 20,\n    # Line color\n    line=dict(color='rgb(31, 119, 180)'),\n),\n#Create IQR 75 line\ngo.Scatter(\n        name = 'IQR 75', y = snow_cover_df['IQR_75'], x = snow_cover_df['days'],\n        mode='lines', marker=dict(color=\"#444\"), line=dict(width=0),\n        legendgroup = 'IQR', showlegend = False\n        # Here we 'hide' the name from appearing on the legend since it's lumped in with the legendgroup 'IQR'\n    ),\n    #Create IQR 25 fill color\n    go.Scatter(\n        name='IQR 25', y = snow_cover_df['IQR_25'], x = snow_cover_df['days'],\n        marker=dict(color=\"#444\"), line=dict(width=0),  mode='lines',\n        fillcolor='rgba(68, 68, 68, 0.3)', fill='tonexty',\n        legendgroup = 'IQR', showlegend = False\n    ),\n    #Create mean line\n    go.Scatter(\n        name = 'Average Snow Cover',  y = snow_cover_df['Average Snow Cover'], x = snow_cover_df['days'],\n        mode = 'lines', legendgroup = 'Average',\n        legendgrouptitle_text = '&lt;b&gt;Average&lt;/b&gt;', legendrank = 21\n    ),\n#Create lines for each respective year\ngo.Scatter(name = '2001', y = snow_cover_df['2001'], x = snow_cover_df['days'], mode = 'lines', legendrank = 19),\ngo.Scatter(name = '2002', y = snow_cover_df['2002'], x = snow_cover_df['days'], mode = 'lines', legendrank = 18),\ngo.Scatter(name = '2003', y = snow_cover_df['2003'], x = snow_cover_df['days'], mode = 'lines', legendrank = 17),\ngo.Scatter(name = '2004', y = snow_cover_df['2004'], x = snow_cover_df['days'], mode = 'lines', legendrank = 16),\ngo.Scatter(name = '2005', y = snow_cover_df['2005'], x = snow_cover_df['days'], mode = 'lines', legendrank = 15),\ngo.Scatter(name = '2006', y = snow_cover_df['2006'], x = snow_cover_df['days'], mode = 'lines', legendrank = 14),\ngo.Scatter(name = '2007', y = snow_cover_df['2007'], x = snow_cover_df['days'], mode = 'lines', legendrank = 13),\ngo.Scatter(name = '2008', y = snow_cover_df['2008'], x = snow_cover_df['days'], mode = 'lines', legendrank = 12),\ngo.Scatter(name = '2009', y = snow_cover_df['2009'], x = snow_cover_df['days'], mode = 'lines', legendrank = 11),\ngo.Scatter(name = '2010', y = snow_cover_df['2010'], x = snow_cover_df['days'], mode = 'lines', legendrank = 10),\ngo.Scatter(name = '2011', y = snow_cover_df['2011'], x = snow_cover_df['days'], mode = 'lines', legendrank = 9),\ngo.Scatter(name = '2012', y = snow_cover_df['2012'], x = snow_cover_df['days'], mode = 'lines', legendrank = 8),\ngo.Scatter(name = '2013', y = snow_cover_df['2013'], x = snow_cover_df['days'], mode = 'lines', legendrank = 7),\ngo.Scatter(name = '2014', y = snow_cover_df['2014'], x = snow_cover_df['days'], mode = 'lines', legendrank = 6),\ngo.Scatter(name = '2015', y = snow_cover_df['2015'], x = snow_cover_df['days'], mode = 'lines', legendrank = 5),\ngo.Scatter(name = '2016', y = snow_cover_df['2016'], x = snow_cover_df['days'], mode = 'lines', legendrank = 4),\ngo.Scatter(name = '2017', y = snow_cover_df['2017'], x = snow_cover_df['days'], mode = 'lines', legendrank = 3),\ngo.Scatter(name = '2018', y = snow_cover_df['2018'], x = snow_cover_df['days'], mode = 'lines', legendrank = 2),\ngo.Scatter(name = '2019', y = snow_cover_df['2019'], x = snow_cover_df['days'], mode = 'lines', legendrank = 1)\n\n])\n# Can change default \"off\" variables. Right now, the only variable visible is year_2019 and IQR\nvariables_to_hide = ['2001', '2002', '2003', '2004', '2005', '2006', '2007', \n'2008', '2009', '2010', '2011', '2012', '2013', '2014', '2015', '2016', '2017', '2018',\n'Average Snow Cover']\nfig.for_each_trace(lambda trace: trace.update(visible=\"legendonly\") \n                   if trace.name in variables_to_hide else ())\nfig.update_layout(\n    title = \"&lt;b&gt; Annual Snow Cover Area: Sierra Nevada Region &lt;/b&gt; &lt;br&gt; &lt;sup&gt;2001-2019&lt;/sup&gt;&lt;/br&gt;\",\n    legend_title=\"&lt;b&gt;Year&lt;/b&gt;\",\n    autosize=False,\n    width=1200,\n    height=700,\n    template = 'none',\n    font=dict(\n        size=16),\nxaxis = dict(\n        tickmode = 'array',\n        tickvals = [1, 31, 61, 92, 123, 151, 182, 212, 243, 273, 304, 335, 365],\n        ticktext = ['&lt;b&gt;October&lt;/b&gt;', '&lt;b&gt;November&lt;/b&gt;', '&lt;b&gt;December&lt;/b&gt;', '&lt;b&gt;January&lt;/b&gt;', '&lt;b&gt;February&lt;/b&gt;', '&lt;b&gt;March&lt;/b&gt;', '&lt;b&gt;April&lt;/b&gt;', '&lt;b&gt;May&lt;/b&gt;', \n        '&lt;b&gt;June&lt;/b&gt;', '&lt;b&gt;July', '&lt;b&gt;August&lt;/b&gt;', \"&lt;b&gt;September&lt;/b&gt;\", \"&lt;b&gt;October&lt;/b&gt;\"],\n        tickfont = dict(size=12))\n)\n\nfig.update_xaxes(title_text = \"\", gridcolor = 'lightgrey', gridwidth = 0.1)\nfig.update_yaxes(title_text = \"&lt;b&gt; Area (Thousands of Square Kilometers) &lt;/b&gt;\", \n    title_font = {\"size\": 15}, gridcolor = 'lightgrey', gridwidth = 0.1)"
  },
  {
    "objectID": "presentations/snow_today_presentation.html#objectives-2",
    "href": "presentations/snow_today_presentation.html#objectives-2",
    "title": "Snow Today",
    "section": "Objectives",
    "text": "Objectives\n\n\n\n\nCreate an open source workflow for processing and visualizing snow data\n\nProvide recommendations for the Snow Today website\nCreate interactive visualizations\n\n3. Improve data usability through tutorials in Python"
  },
  {
    "objectID": "presentations/snow_today_presentation.html#tutorials",
    "href": "presentations/snow_today_presentation.html#tutorials",
    "title": "Snow Today",
    "section": "Tutorials",
    "text": "Tutorials\n\n\n 1. Download and Explore Datasets     2. Process and Format Data     3. Analyze and Visualize Snow Data"
  },
  {
    "objectID": "presentations/snow_today_presentation.html#tutorials-1",
    "href": "presentations/snow_today_presentation.html#tutorials-1",
    "title": "Snow Today",
    "section": "Tutorials",
    "text": "Tutorials\n\n\n 1. Download and Explore Datasets     2. Process and Format Data     3. Analyze and Visualize Snow Data\n\n\nDownload snow cover and albedo datasets\nOpen datasets and view metadata\nCreate basic visualizations of each dataset"
  },
  {
    "objectID": "presentations/snow_today_presentation.html#tutorials-2",
    "href": "presentations/snow_today_presentation.html#tutorials-2",
    "title": "Snow Today",
    "section": "Tutorials",
    "text": "Tutorials\n\n\n 1. Download and Explore Datasets     2. Process and Format Data     3. Analyze and Visualize Snow Data\n\n\nProcess and subset datasets\nCalculate monthly and yearly snow cover and albedo averages and anomalies\nCreate interactive maps of processed data\nConvert processed data to GeoTiff and NetCDF formats"
  },
  {
    "objectID": "presentations/snow_today_presentation.html#tutorials-3",
    "href": "presentations/snow_today_presentation.html#tutorials-3",
    "title": "Snow Today",
    "section": "Tutorials",
    "text": "Tutorials\n\n\n 1. Download and Explore Datasets     2. Process and Format Data     3. Analyze and Visualize Snow Data\n\n\nCalculate total snow cover area and average albedo for entire spatial domain\nPerform basic statistical analysis of datasets\nDevelop interactive charts to compare snow cover area and albedo percentages for each water year"
  },
  {
    "objectID": "presentations/snow_today_presentation.html#contributions",
    "href": "presentations/snow_today_presentation.html#contributions",
    "title": "Snow Today",
    "section": "Contributions",
    "text": "Contributions"
  },
  {
    "objectID": "presentations/snow_today_presentation.html#section-3",
    "href": "presentations/snow_today_presentation.html#section-3",
    "title": "Snow Today",
    "section": "",
    "text": "As water resources become harder to manage due to climate change, implementing these tools will open a valuable dataset to a wider audience"
  },
  {
    "objectID": "presentations/snow_today_presentation.html#acknowledgements",
    "href": "presentations/snow_today_presentation.html#acknowledgements",
    "title": "Snow Today",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nFaculty Advisors\nSam Stevenson, UCSB Bren School\nAllison Horst, UCSB Bren School\nClients\nTimbo Stillinger, UCSB Earth Research Institute\nNed Bair, UCSB Earth Research Institute\nKarl Rittger, CU Boulder Institute of Arctic & Alpine Research\nExternal Advisors\nJames Frew, UCSB Bren School\nNiklas Griessbaum, UCSB Bren School\nKat Le, UCSB Bren School\nMichael Colee, UCSB Geography & Earth Research Institute\nBren School Faculty and Staff and the MEDS 2022 cohort\n\n\n\nhttps://shiny.snow.ucsb.edu/snow_today_shiny_app/&gt;"
  }
]