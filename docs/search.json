[
  {
    "objectID": "code_samples.html",
    "href": "code_samples.html",
    "title": "Code Samples",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\nPython\n\n\ngeopandas\n\n\nstatial analysis\n\n\n\nA spatial analysis using Python and GeoPandas\n\n\n\nMarie Rivers\n\n\nApr 3, 2022\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "code_samples/2022-04-03-protecting-whales-from-ships/index.html",
    "href": "code_samples/2022-04-03-protecting-whales-from-ships/index.html",
    "title": "Protecting Whales from Ships",
    "section": "",
    "text": "This analysis identifies a speed reduction zone off the island of Dominica for the purpose of reducing the occurrence of ships striking whales and quantifies the impact of reduced travel speeds on marine traffic."
  },
  {
    "objectID": "code_samples/2022-04-03-protecting-whales-from-ships/index.html#load-data",
    "href": "code_samples/2022-04-03-protecting-whales-from-ships/index.html#load-data",
    "title": "Protecting Whales from Ships",
    "section": "Load data",
    "text": "Load data\n\n# Join folder path and filename \nfp3 = os.path.join(input_folder2, \"station1249.csv\")\n\n# Print out the full file path\nprint(fp3)\n\ndata/station1249.csv\n\n\n\n# Read file using gpd.read_file()\nvessels = gpd.read_file(fp3)\n\n\ntype(vessels)\n\n<class 'geopandas.geodataframe.GeoDataFrame'>\n\n\n\nvessels.head\n\n<bound method NDFrame.head of        field_1       MMSI        LON       LAT            TIMESTAMP geometry\n0            0  233092000  -61.84788  15.23238  2015-05-22 13:53:26     None\n1            1  255803280  -61.74397  15.96114  2015-05-22 13:52:57     None\n2            2  329002300  -61.38968  15.29744  2015-05-22 13:52:32     None\n3            3  257674000  -61.54395   16.2334  2015-05-22 13:52:24     None\n4            4  636092006  -61.52401  15.81954  2015-05-22 13:51:23     None\n...        ...        ...        ...       ...                  ...      ...\n617257  238722  256525000  -61.40679  15.36907  2015-05-21 21:34:59     None\n617258  238723  311077100  -61.37539  15.27406  2015-05-21 21:34:55     None\n617259  238724  377907247  -61.39461  15.30672  2015-05-21 21:34:46     None\n617260  238725  253365000  -61.49001  16.14007  2015-05-21 21:34:46     None\n617261  238726  329002300  -61.48073  15.44751  2015-05-21 21:34:45     None\n\n[617262 rows x 6 columns]>\n\n\n\n# bootstrap the geometries\nvessel_points = gpd.points_from_xy(vessels['LON'], vessels['LAT'])\nvessel_gdf = gpd.GeoDataFrame(vessels, geometry=vessel_points)\n\n\n# project the dataset into an appropriate CRS\nvessel_gdf = vessel_gdf.set_crs(epsg=4326)\nvessel_gdf = vessel_gdf.to_crs(epsg=proj_area_crs)\n\n\nvessel_gdf.crs\n\n<Derived Projected CRS: EPSG:2002>\nName: Dominica 1945 / British West Indies Grid\nAxis Info [cartesian]:\n- E[east]: Easting (metre)\n- N[north]: Northing (metre)\nArea of Use:\n- name: Dominica - onshore.\n- bounds: (-61.55, 15.14, -61.2, 15.69)\nCoordinate Operation:\n- name: British West Indies Grid\n- method: Transverse Mercator\nDatum: Dominica 1945\n- Ellipsoid: Clarke 1880 (RGS)\n- Prime Meridian: Greenwich\n\n\n\nvessel_gdf['TIMESTAMP'] = pd.to_datetime(vessel_gdf['TIMESTAMP'])\n\n\nvessel_gdf.head()\n\n  field_1       MMSI  ...           TIMESTAMP                        geometry\n0       0  233092000  ... 2015-05-22 13:53:26  POINT (415373.315 1683307.035)\n1       1  255803280  ... 2015-05-22 13:52:57  POINT (426434.345 1763918.193)\n2       2  329002300  ... 2015-05-22 13:52:32  POINT (464555.392 1690588.725)\n3       3  257674000  ... 2015-05-22 13:52:24  POINT (447770.634 1794068.620)\n4       4  636092006  ... 2015-05-22 13:51:23  POINT (450006.361 1748297.844)\n\n[5 rows x 6 columns]\n\n\n\n# plot of all vessel points\n\nvessel_gdf.plot(figsize=(5,10))\n\n\n\n\n\n# plot of all vessel points\nbase = dominica.plot(facecolor='none', edgecolor='black', linewidth=3, figsize=(15, 15))\nvessel_gdf.plot(ax=base, markersize = 3)\nspeed_reduction_zone.plot(ax=base, edgecolor='red', linewidth=2)\n\n\n\n\n\n# spatially subset AIS data to only include vessels within identified whale habitat\nvessels_in_whale_habitat = vessel_gdf.sjoin(speed_reduction_zone, how=\"inner\")\nvessels_in_whale_habitat\n\n       field_1       MMSI  ...                        geometry index_right\n2            2  329002300  ...  POINT (464555.392 1690588.725)           0\n7            7  338143127  ...  POINT (463892.452 1694650.397)           0\n13          13  329002300  ...  POINT (464555.389 1690589.831)           0\n15          15  338143015  ...  POINT (463910.683 1694655.978)           0\n16          16  338143127  ...  POINT (463697.964 1694341.275)           0\n...        ...        ...  ...                             ...         ...\n617252  238717  329002300  ...  POINT (453901.647 1709712.916)           0\n617253  238718  338143015  ...  POINT (463915.972 1694683.643)           0\n617255  238720  338143127  ...  POINT (463905.177 1694705.734)           0\n617259  238724  377907247  ...  POINT (464023.288 1691613.624)           0\n617261  238726  329002300  ...  POINT (454741.236 1707161.130)           0\n\n[167411 rows x 7 columns]"
  },
  {
    "objectID": "code_samples/2022-04-03-protecting-whales-from-ships/index.html#calculate-distance-and-speed",
    "href": "code_samples/2022-04-03-protecting-whales-from-ships/index.html#calculate-distance-and-speed",
    "title": "Protecting Whales from Ships",
    "section": "Calculate distance and speed",
    "text": "Calculate distance and speed\n\n# plot of only vessel points within speed reduction zone\nbase = dominica.plot(facecolor='none', linewidth=3, figsize=(15, 15))\nspeed_reduction_zone.plot(ax=base, facecolor='none', edgecolor='red', linewidth=3)\nvessels_in_whale_habitat.plot(ax=base, markersize = 0.5, facecolor='black')\nctx.add_basemap(ax=base, crs=dominica.crs.to_string())\n\n\n# sort vessel dataframe by MMSI and time\nvessels_in_whale_habitat = vessels_in_whale_habitat.sort_values(by=['MMSI', 'TIMESTAMP'])\nvessels_in_whale_habitat\n\n       field_1       MMSI  ...                        geometry index_right\n235025  235025  203106200  ...  POINT (462476.396 1680935.224)           0\n235018  235018  203106200  ...  POINT (462283.995 1681393.698)           0\n235000  235000  203106200  ...  POINT (461936.769 1682722.187)           0\n234989  234989  203106200  ...  POINT (461798.818 1683708.377)           0\n234984  234984  203106200  ...  POINT (461654.150 1683997.765)           0\n...        ...        ...  ...                             ...         ...\n259103  259103  983191049  ...  POINT (465250.372 1690066.434)           0\n259094  259094  983191049  ...  POINT (465243.965 1690054.249)           0\n258954  258954  983191049  ...  POINT (465226.597 1690121.667)           0\n258930  258930  983191049  ...  POINT (465242.895 1690053.140)           0\n258206  258206  983191049  ...  POINT (465272.964 1690049.908)           0\n\n[167411 rows x 7 columns]\n\n\n\n# create a copy of the vessel dataframe and shift each observation down one row using `shift()`\nvessels_shift = vessels_in_whale_habitat.copy(deep=True).shift(periods=1)\n\n\n# rename shifted column names\nvessels_shift = vessels_shift.rename(columns={\"field_1\": \"field_1_shift\", \"MMSI\": \"MMSI_shift\", \"LON\": \"LON_shift\", \"LAT\": \"LAT_shift\", \"TIMESTAMP\": \"TIMESTAMP_shift\", \"geometry\": \"geometry_shift\", \"index_right\": \"index_right_shift\"})\n\n\n# join original dataframe with the shifted copy using `join()`\nvessels_shift_join = vessels_in_whale_habitat.join(vessels_shift).sort_values(by=['MMSI', 'TIMESTAMP'])\n\n\n# drop all rows in the joined dataframe in which the MMSI of the left is not the same as the one on the right\nvessels_keep = vessels_shift_join.drop(vessels_shift_join[vessels_shift_join['MMSI'] != vessels_shift_join['MMSI_shift']].index)\n\n\n# set the geometry column\nvessels_keep = vessels_keep.set_geometry(\"geometry\")\nvessels_keep2 = vessels_keep.set_geometry(\"geometry_shift\")\n\n\n# calculate distance between each observation\nvessels_keep['distance_m'] = vessels_keep.distance(vessels_keep2)\n\n\n# calculate time difference between each observation to the next\nvessels_keep['time'] = vessels_keep['TIMESTAMP'] - vessels_keep['TIMESTAMP_shift']\n\n\n# calculate speed\nmeters_per_nm = 1852\n\nvessels_keep['speed_m_per_sec'] = vessels_keep['distance_m'] / vessels_keep['time'].dt.total_seconds()\nvessels_keep['speed_knots'] = vessels_keep['speed_m_per_sec'] * 60 * 60 / meters_per_nm\nvessels_keep['time_10knots_minutes'] = (vessels_keep['distance_m'] * 60 ) / ( meters_per_nm * 10 )\nvessels_keep['time_dif_minutes'] = vessels_keep['time_10knots_minutes'] - (vessels_keep['time'].dt.total_seconds() / 60 )\nvessels_keep\n\n       field_1       MMSI  ... time_10knots_minutes time_dif_minutes\n235018  235018  203106200  ...             1.610828        -0.889172\n235000  235000  203106200  ...             4.448540        -3.034793\n234989  234989  203106200  ...             3.226109        -1.773891\n234984  234984  203106200  ...             1.048164        -1.468503\n234972  234972  203106200  ...             1.394116        -3.589217\n...        ...        ...  ...                  ...              ...\n259103  259103  983191049  ...             0.043139        -5.940194\n259094  259094  983191049  ...             0.044599        -4.355401\n258954  258954  983191049  ...             0.225548       -55.824452\n258930  258930  983191049  ...             0.228202       -11.471798\n258206  258206  983191049  ...             0.097976      -248.585358\n\n[166255 rows x 20 columns]\n\n\n\nvessels_keep = vessels_keep.sort_values(by=['speed_knots'], ascending=False)\n\n\n# look at the vessels that would be affected by the speed reduction zone\nvessels_going_too_fast = vessels_keep.drop(vessels_keep[vessels_keep['time_dif_minutes'] < 0].index)\nvessels_going_too_fast\n\n       field_1       MMSI  ... time_10knots_minutes time_dif_minutes\n585844  207309  341387000  ...             0.209101         0.209101\n67091    67091  227528210  ...             7.979167         6.245834\n66925    66925  228008600  ...            13.481637        10.498303\n499754  121219  329002300  ...             8.728323         6.711656\n546817  168282  329002300  ...            12.866650         9.849984\n...        ...        ...  ...                  ...              ...\n616429  237894  636091437  ...             0.000000         0.000000\n616427  237892  636091437  ...             0.000000         0.000000\n616422  237887  636091437  ...             0.000000         0.000000\n616419  237884  636091437  ...             0.000000         0.000000\n616408  237873  636091437  ...             0.000000         0.000000\n\n[21410 rows x 20 columns]\n\n\n\nshipping_impact_minutes = vessels_going_too_fast['time_dif_minutes'].sum()\nshipping_impact_days = round(shipping_impact_minutes / ( 60 * 24), 2)\nshipping_impact_days\n\n27.88\n\n\nA 10-knot reduced speed zone in the identified whale habitat will increase travel time by approximately 27.88 days."
  },
  {
    "objectID": "posts/2021-11-03-how-i-made-this-visualization/index.html",
    "href": "posts/2021-11-03-how-i-made-this-visualization/index.html",
    "title": "How I made this visualization",
    "section": "",
    "text": "An assignment for my Metadata Standards, Data Modeling and Data Semantics class included using the metajam R package to download Alaskan household languages data and metadata from knb. After reviewing the metadata and reading the data into R, we were tasked with writing code to compute the percentage of Alaskan households that only speak English for the years 2009-2015 and visualizing these results…a straightforward task.\nBut…as I reviewed the numbers I thought about what I actually wanted to capture in the visualization. Did I want to focus on changes over time? There was a slight trend, but nothing significant. The data included State of Alaska Salmon and People Regions (SASAP) so I wanted to show any regional differences.\nHere’s the final visualization\nThe items below outline my data visualization process and helpful resources. Full code for data processing and visualization are included at the end."
  },
  {
    "objectID": "posts/2021-11-03-how-i-made-this-visualization/index.html#download-data-using-metajam",
    "href": "posts/2021-11-03-how-i-made-this-visualization/index.html#download-data-using-metajam",
    "title": "How I made this visualization",
    "section": "Download data using metajam",
    "text": "Download data using metajam\n\n# url to csv file\ndata_url <- \"https://knb.ecoinformatics.org/knb/d1/mn/v2/object/urn%3Auuid%3A7fc6f6db-c5ea-426a-a743-1f2edafb43b8\"\n\n# download the data and metadata to project folder\ndata_path <- metajam::download_d1_data(data_url, \"~/Documents/metajam_example\")\n\n# Read the data and metadata\nhh_list <- metajam::read_d1_files(data_path)\n\n# get the household data frame\nhh_data <- hh_list$data\n\n# get the attribute (columns) metadata\nhh_att_metadata <- hh_list$attribute_metadata"
  },
  {
    "objectID": "posts/2021-11-03-how-i-made-this-visualization/index.html#tidy-data",
    "href": "posts/2021-11-03-how-i-made-this-visualization/index.html#tidy-data",
    "title": "How I made this visualization",
    "section": "Tidy Data",
    "text": "Tidy Data\n\nCode used to process the data\nI adjusted the original data processing code as my plan for the visualization evolved\n\nhousehold_language <- read_csv(here(\"data\", \"doi_10.5063_F1CJ8BPH__household_language__csv\", \"household_language.csv\"))\n\nhh_data_english <- household_language %>%\n  filter(Year >= 2009) %>%\n  filter(total > 0) %>%\n  mutate(percent_only_english = (speak_only_english / total) * 100) %>%\n  relocate(percent_only_english, .before = german) %>%\n  mutate(SASAP.Region = fct_reorder(SASAP.Region, percent_only_english, .fun = mean)) %>%\n  group_by(SASAP.Region, Year) %>%\n  summarise(avg_percent_english = mean(percent_only_english))"
  },
  {
    "objectID": "posts/2021-11-03-how-i-made-this-visualization/index.html#ggplot",
    "href": "posts/2021-11-03-how-i-made-this-visualization/index.html#ggplot",
    "title": "How I made this visualization",
    "section": "ggplot",
    "text": "ggplot\nCode used to create the plot\n\nonly_english_plot <- ggplot(hh_data_english, aes(x = Year, y = SASAP.Region)) +\n  geom_tile(aes(fill = avg_percent_english), show.legend = TRUE) +\n  geom_text(aes(label = paste0(round(avg_percent_english, 0),\"%\")), color = \"white\", size = 3) +\n  scale_fill_gradientn(colors = c(\"antiquewhite3\", \"antiquewhite4\", \"steelblue4\", \"springgreen4\", \"indianred4\")) +\n  theme_minimal() +\n  theme(panel.grid.major = element_blank()) +\n  labs(x = \"Year\", y = NULL,\n       fill = \"Percent\",\n       title = \"Percent of Alaska Households that Only Speak English\",\n       subtitle = \"based on State of Alaska Salmon and People Region\",\n       caption = \"source: Jeanette Clark, Sharis Ochs, Derek Strong, and National Historic Geographic Information System. 2018.\\nLanguages used in Alaskan households, 1990-2015. urn:node:KNB. doi:10.5063/F1N58JPP\") +\n  theme(plot.caption = element_text(size = 8, hjust = 0),\n        plot.caption.position = \"plot\") +\n  theme(plot.title.position = \"plot\") +\n  scale_x_discrete(name = \"Year\", limits = c(2009, 2010, 2011, 2012, 2013, 2014, 2015))\n\nData citation: Jeanette Clark, Sharis Ochs, Derek Strong, and National Historic Geographic Information System. 2018. Languages used in Alaskan households, 1990-2015. urn:node:KNB. doi:10.5063/F1N58JPP.\nhttps://knb.ecoinformatics.org/knb/d1/mn/v2/object/urn%3Auuid%3A7fc6f6db-c5ea-426a-a743-1f2edafb43b8”"
  },
  {
    "objectID": "posts/2021-10-17-do-you-learn-about-the-chicken-or-the-egg-first/index.html",
    "href": "posts/2021-10-17-do-you-learn-about-the-chicken-or-the-egg-first/index.html",
    "title": "Do you learn about the chicken or the egg first",
    "section": "",
    "text": "A big take away from the class wasn’t any specific coding knowledge, but rather an idea of the general mindset required to successfully learn how to code. I experienced the problem solving component of coding and the potentially frustrating cycle trying, failing, trying again, tweaking, consulting other references, and approaching from a different direction. More important than the lessons, I confirmed that I enjoy this type of work, my mind thinks in the logical way that code often requires, and I have the patience to work through debugging. An aspiring coder can’t be daunted when things don’t work the first, second, or third time. I also completed enough Codecademy R tutorials to confirm that yep, this coding thing was for me. I skimmed a few reference docs and watched a few intro R YouTube videos, but I didn’t have a great understanding of how R was used beyond tutorial world. A lot of this early context didn’t stick.\nOnce I arrived at UCSB, we started using R (and a little Python) straight away. From the beginning, our classes emphasized the importance of reproducible workflows, documenting code with comments, and version control. We spent more time typing code than just looking at code. Our classes include the right amount of repetition to retain essential content and appreciate its use in a variety of contexts.\nThere is no “right” way to being to learn how to code. Starting is the most important part. Don’t try to gain some arbitrary minimum knowledge base before starting. The first time you use a new package or function, you’re not going to remember all the details. The first times you read reference books, package documentations or blog posts they won’t make much sense. The more you learn, the more your capacity to ingest new information will increase. You will never know everything, but you will gain a larger understanding of what is possible (and where to go to learn how to do it). AND…you’ll start to understand R jokes. The best way to learn is by doing. This grad program has a good balance of formal instruction and, more importantly, plenty of opportunities to figure things out for yourself.\n\n\n\nCitationBibTeX citation:@online{rivers2021,\n  author = {Marie Rivers},\n  title = {Do You Learn about the Chicken or the Egg First},\n  date = {2021-10-17},\n  url = {https://marierivers.github.io/posts/2021-10-17-do-you-learn-about-the-chicken-or-the-egg-first/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nMarie Rivers. 2021. “Do You Learn about the Chicken or the Egg\nFirst.” October 17, 2021. https://marierivers.github.io/posts/2021-10-17-do-you-learn-about-the-chicken-or-the-egg-first/."
  },
  {
    "objectID": "posts/2022-06-07-the-importance-of-meaningfully-open-snow-data/index.html",
    "href": "posts/2022-06-07-the-importance-of-meaningfully-open-snow-data/index.html",
    "title": "The Importance of Meaningfully Open Snow Data",
    "section": "",
    "text": "The datasets used in the Snow Today project could be improved by incorporating FAIR data principles (Wilkinson 2016). FAIR stands for Findable, Accessible, Interoperable, and Reusable. The metadata associated with the snow cover and albedo datasets was hard to find and once it was found, it was hard to interpret. The metadata told us the map projection of the data, but this information wasn’t attached in a standard format that could be recognized by common mapping software and spatial packages. This meant that we couldn’t plot the data on a map in the correct location. This also meant that researchers or water managers faced a significant barrier if trying to use the dataset to learn about their local water supply.\nAs we worked through the challenges of the snow cover and albedo datasets, our goals shifted towards creating an open-source workflow to make the data more meaningfully open. While the data used for our project is available online for anyone to download (if you know where to find it), insights can be hard or near impossible to gather without specialized training. After many conversations with the dataset creators, we were able to develop a workflow around the metadata challenges. To see the final product of the Snow Today capstone group, including tutorials to guide others through the steps of repeating our workflow, visit our interactive web app\nQuantifying snow cover area is important because much of the world’s population, from the Western US to High Mountain Asia, relies on winter snowpacks for year-round drinking water, but…\n\nWhy do we care about albedo?\nAlbedo is a measure of how much solar energy is reflected from a surface. Albedo has important climate implications because it determines how much radiation the planet absorbs. Dark surfaces like soil and vegetation have low albedo values while lighter surfaces such as snow have higher albedo values. Dirty snow absorbs more solar radiation and therefore melts faster than clean snow. Since spring snowmelt contributes to drinking water reservoirs in drier months, earlier snowmelt can leave less water in the summer when it’s needed most. A layer of fresh snow increases albedo for that area, which can result in local cooling. When snow melts, it reveals darker surfaces with lower albedo which increases local temperatures and encourages more melting in a feedback loop where the surface absorbs more solar radiation. \n\n\n\n\nCitationBibTeX citation:@online{rivers2022,\n  author = {Marie Rivers},\n  title = {The {Importance} of {Meaningfully} {Open} {Snow} {Data}},\n  date = {2022-06-07},\n  url = {https://marierivers.github.io/posts/2022-06-07-the-importance-of-meaningfully-open-snow-data/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nMarie Rivers. 2022. “The Importance of Meaningfully Open Snow\nData.” June 7, 2022. https://marierivers.github.io/posts/2022-06-07-the-importance-of-meaningfully-open-snow-data/."
  },
  {
    "objectID": "posts/2021-08-18-a-new-perspective-on-data/index.html",
    "href": "posts/2021-08-18-a-new-perspective-on-data/index.html",
    "title": "A New Perspective on Data",
    "section": "",
    "text": "During college I used excel for everything inside and outside of class. My classmates and I would joke about using solver to answer everything from mundane domestic questions to meaning of life type queries. While I haven’t used solver since finding the optimal 2012 US Olympic gymnastics team based on various scoring scenarios, spreadsheets are still prevalent in my life:\n\n– Planning a group vacation…shared Google sheet!\n– Outlining an epic fantasy novel…no need for scrivener\n– Any major life decision…better back that choice up with excel\n\nAnd my works days were full of engineering calcs, budgets, cost estimates, asset management analyses, and graphs. From summary statistics, if statements, and well placed $ signs to conditional formatting and pivot tables, I thought my data storage, analysis, and visualization skills were top of their game…until I discovered the Masters of Environmental Data Science (MEDS) program at UC Santa Barbara’s Bren school of Environmental Science & Management. I started to get a hunch that there were other alternatives when working with data.\nSince starting the MEDS program, I’ve begun to expand my data storage, analysis, and visualization outlook. I was very impressed with an article we read in class Data Organization in Spreadsheet by Karl W. Broman & Kara H. Woo. (Karl W. Broman & Kara H. Woo (2018) Data Organization in Spreadsheets, The American Statistician, 72:1, 2-10, DOI: 10.1080/00031305.2017.1375989). This paper articulates many of the logistical challenges I’ve faced with data and presented useful recommendations. I’ve worked with or created spreadsheets where each of the basic spreadsheet principles were not used.\nI have received and passed along complicated spreadsheets with equations linked to cells in tabs throughout the workbook. While my coworkers and I attempted to used consistent file names (even if only consistent with their own files) these efforts often fall short when unexpected complexities arise or deadlines get close. When you are desperately trying to get a deliverable out the door, intermediate file names are the least of your priorities. Challenges associated with combining or separating dates and addresses and entering zip codes or ID numbers that begin with 0 were expected parts of data manipulation. I’ve received data with no documentation for column headers, acronyms, or units; from these experiences I began to use perhaps overly lengthy variable names, often with a top row merged over several variables.\nThe concept of Tidy data was completely knew to me. At first, this data structure seemed drawn-out and a little redundant, but as we’ve worked with more data in class I now see the advantages. Taking the time to use functions such as pivot_wider and pivot_longer to get data in Tide format ultimately gives your subsequent analysis more flexibility and ease.\nAs I continue with the MEDS program, I’m looking forward to gaining the same intuition for R and Python that I have with excel. I’m sure this journey will be frustrating and time consuming, but know that learning new ways to store, analyze, and visual data will be rewarding!\nP.S. I never fully appreciated CSV files\n\n\n\nCitationBibTeX citation:@online{rivers2012,\n  author = {Marie Rivers},\n  title = {A {New} {Perspective} on {Data}},\n  date = {2012-08-18},\n  url = {https://marierivers.github.io/posts/2021-08-18-a-new-perspective-on-data/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nMarie Rivers. 2012. “A New Perspective on Data.” August 18,\n2012. https://marierivers.github.io/posts/2021-08-18-a-new-perspective-on-data/."
  },
  {
    "objectID": "posts/2022-02-08-how-to-make-a-dumbbell-schedule-with-r/index.html",
    "href": "posts/2022-02-08-how-to-make-a-dumbbell-schedule-with-r/index.html",
    "title": "How to make a dumbbell schedule with R",
    "section": "",
    "text": "Original inspiration: and guidance on using geom_dumbbell.\n\nRead and view the data\nI started from an excel sheet.\n\nschedule <- read_excel(here(\"posts\", \"2022-02-08-how-to-make-a-dumbbell-schedule-with-r\", \"schedule.xlsx\"), sheet = \"Sheet1\")\n\n\nkable(schedule) %>% \n  kable_paper(full_width = TRUE) %>% \n  row_spec(0, bold = T) %>% \n  kable_styling(latex_options = \"HOLD_position\")\n\n\n \n  \n    milestone \n    deliverable \n    start_date \n    due_date \n    status \n    quarter \n  \n \n\n  \n    Draft Feasibility Study \n    Feasibility Study \n    2022-01-12 \n    2022-02-07 \n    complete \n    Q1 \n  \n  \n    Final Feasibility Study \n    Feasibility Study \n    2022-02-12 \n    2022-03-11 \n    in progress \n    Q1 \n  \n  \n    Draft Design Drawings \n    Design Drawings \n    2022-01-24 \n    2022-03-18 \n    in progress \n    Q1 \n  \n  \n    Final Design Drawings \n    Design Drawings \n    2022-04-12 \n    2022-05-20 \n    not started \n    Q2 \n  \n  \n    Draft Technical Documentation \n    Technical Documentation \n    2022-03-01 \n    2022-03-18 \n    not started \n    Q1 \n  \n  \n    Final Technical Documentation \n    Technical Documentation \n    2022-03-28 \n    2022-04-22 \n    not started \n    Q2 \n  \n  \n    Stakeholder Presentation \n    Presentation \n    NA \n    2022-03-25 \n    not started \n    Q1 \n  \n  \n    Draft Web Application \n    Web Application \n    2022-02-18 \n    2022-03-16 \n    not started \n    Q1 \n  \n  \n    Final Web Application \n    Web Application \n    2022-03-28 \n    2022-05-27 \n    not started \n    Q2 \n  \n  \n    Client Presentation \n    Presentation \n    NA \n    2022-06-03 \n    not started \n    Q2 \n  \n\n\n\n\n\n\n\nClean the data\nPay attention to date formats with class(schedule$start_date). Cleaning steps created a field for month abbreviation + day of month to use as labels. The deliverable category was reordered based on due date using fct_reorder.\n\nschedule <- schedule %>% \n  mutate(start_month_num = month(start_date)) %>% \n  mutate(end_month_num = month(due_date)) %>% \n  mutate(start_month_name = case_when(\n    start_month_num == 1 ~ \"Jan\",\n    start_month_num == 2  ~ \"Feb\",\n    start_month_num == 3 ~ \"Mar\",\n    start_month_num == 4 ~ \"Apr\",\n    start_month_num == 5 ~ \"May\",\n    start_month_num == 6 ~ \"Jun\",\n  )) %>% \n  mutate(end_month_name = case_when(\n    end_month_num == 1 ~ \"Jan\",\n    end_month_num == 2 ~ \"Feb\",\n    end_month_num == 3 ~ \"Mar\",\n    end_month_num == 4 ~ \"Apr\",\n    end_month_num == 5 ~ \"May\",\n    end_month_num == 6 ~ \"Jun\",\n  )) %>% \n  mutate(start_label = paste(start_month_name, day(start_date))) %>% \n  mutate(end_label = paste(end_month_name, day(due_date))) %>% \n  mutate(milestone = as_factor(milestone)) %>% \n  mutate(milestone = fct_reorder(milestone, as.numeric(due_date), .desc = TRUE))\n\nThis step was used for the bounds of the rectangle highlighting the second quarter.\n\nQ2 <- schedule %>% \n  filter(quarter == \"Q2\")\nxmin_Q2 <- as.POSIXct(min(Q2$start_date, na.rm = TRUE))\nxmax_Q2 <- as.POSIXct(max(Q2$due_date, na.rm = TRUE))\n\nThis step was used to set the x-axis limits when using scale_x_datetime.\n\nmin <- as.POSIXct(\"2022-1-1\")\nmax <- as.POSIXct(\"2022-6-15\")\n\nI originally used geom_dumbbell for the start and end dots, but using geom_point twice gave more flexibility for colors and made it easy add transparency to the start dots. The geom_dumbbell code is include for reference.\n\ntime_plot <- ggplot(data = schedule, aes(y = milestone)) +\n  geom_rect(aes(xmin = xmin_Q2, ymin = -Inf,\n                xmax = xmax_Q2, ymax = Inf),\n                fill = \"grey80\", alpha = 0.5) +\n  # create a thick line between x and xend instead of using default provided by geom_dumbbell\n  geom_segment(aes(x = start_date, xend = due_date, y = milestone, yend = milestone), \n               color = \"grey50\", \n               size = 1.5) +\n  # geom_dumbbell(color = \"grey80\", size_x = 5, size_xend = 5,\n  #               colour_x = \"blue\", colour_xend = \"red\") +\n  geom_point(data = schedule, aes(x = start_date, y = milestone,\n                              color = deliverable), size = 5, alpha = 0.5,\n             show.legend = FALSE) +\n  # scale_colour_paletteer_d(\"Redmonder::qMSOPap\") +\n  # scale_colour_paletteer_d(\"Redmonder::qMSOPap\") +\n  # scale_colour_paletteer_d(\"Redmonder::qMSOMed\") +\n  # scale_colour_paletteer_d(\"palettetown::pelipper\") +\n  scale_colour_manual(values = c(\"#007a76\", \"#b7245c\", \"#ca7f0e\", \"#0d4fbd\", \"#785ceb\")) + # manually specify colors or use an existing palette\n  geom_point(data = schedule, aes(x = due_date, y = milestone,\n                              color = deliverable), size = 5,\n             show.legend = FALSE) +\n  labs(x = NULL, y = NULL,\n       title = \"Milestone Timeline\") +\n  geom_text(color = \"black\", size = 3, hjust = 1.5,\n            aes(x = start_date, label = start_label)) +\n  geom_text(color = \"black\", size = 3, hjust = -0.5,\n            aes(x = due_date, label = end_label)) +\n  theme_minimal() +\n  theme(panel.grid = element_blank()) +\n  theme(plot.title.position = \"plot\") +\n  scale_x_datetime(limits = c(min, max)) +\n  annotate(\"text\", x = as.POSIXct(\"2022-5-1\"), y = schedule$milestone[1], label = \"Q2\", color = \"black\", size = 5)\ntime_plot\n\nWarning: Removed 2 rows containing missing values (geom_segment).\n\n\nWarning: Removed 2 rows containing missing values (geom_point).\n\n\nWarning: Removed 2 rows containing missing values (geom_text).\n\n\n\n\n\n\n\n\n\nCitationBibTeX citation:@online{rivers2022,\n  author = {Marie Rivers},\n  title = {How to Make a Dumbbell Schedule with {R}},\n  date = {2022-02-08},\n  url = {https://marierivers.github.io/posts/2022-02-08-how-to-make-a-dumbbell-schedule-with-r/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nMarie Rivers. 2022. “How to Make a Dumbbell Schedule with\nR.” February 8, 2022. https://marierivers.github.io/posts/2022-02-08-how-to-make-a-dumbbell-schedule-with-r/."
  },
  {
    "objectID": "posts/2021-12-02-environmental-quality-population-change/index.html",
    "href": "posts/2021-12-02-environmental-quality-population-change/index.html",
    "title": "Does Environmental Quality Influence Where People Live?",
    "section": "",
    "text": "This post discusses a statistical analysis used to answer the question: Does environmental quality influence where people live in the United States?"
  },
  {
    "objectID": "posts/2021-12-02-environmental-quality-population-change/index.html#background",
    "href": "posts/2021-12-02-environmental-quality-population-change/index.html#background",
    "title": "Does Environmental Quality Influence Where People Live?",
    "section": "Background",
    "text": "Background\nThe Environmental Quality Index (EQI), developed by the U.S. Environmental Protection Agency (EPA) provides a county level snapshop of environmental conditions throughout the country. EPA first released EQIs for the period 2000-2005 and updated these indexes for 2006-2010. This statistical evaluation focuses on the 2006-2010 EQI. The purpose of the EQI is to use (1) as an indicator of ambient conditions/exposure in environmental health and modeling and (2) as a covariate to adjust for ambient conditions in environmental models (EPA 2020). Previous studies have used the EQI to evaluate relationships between environmental quality and public health outcomes such as cancer incidence, asthma, obesity, and infant mortality.\nThe EQI is developed from five domains each with identified environmental constructs as shown in Table 1. Each county has an overall environmental index and a domain specific index. Indexes were also stratified by rural-urban continuum codes (RUCCs) for counties classified as metropolitan urbanized, non-metro urbanized, less urbanized, and thinly populated.\n\n\n\n\n\n\nEQI Environmental Domains and Constructs\n \n  \n    Domain \n    Constructs \n  \n \n\n  \n    air \n    criteria air pollutants and hazardous air pollutants \n  \n  \n    water \n    overall water quality, general water contamination, domestic use, atmospheric deposition, drought, chemical contamination, and drinking water quality \n  \n  \n    land \n    agriculture, pesticides, facilities, radon, and mining activity \n  \n  \n    built \n    roads, highway/road safety, commuting behavior, housing environment, walkability, and green space \n  \n  \n    sociodemographic \n    crime, socioeconomic, political character, and creative class representation \n  \n\n\n\n\n\n\nLimitations\nWhile the EQI can identify counties with higher environmental burdens, it may not identify environmental injustices at the local community level. The EQI cannot quantify environmental exposure for individuals and reflects only outside environmental conditions, not indoor conditions. The EQI can be used to identify locations for future research, but is not intended for regulatory purposes or as a diagnostics tool. Due to changes in methodology and datasets, the 2000-2005 and 2006-2010 EQIs should not be directly compared."
  },
  {
    "objectID": "posts/2021-12-02-environmental-quality-population-change/index.html#the-data",
    "href": "posts/2021-12-02-environmental-quality-population-change/index.html#the-data",
    "title": "Does Environmental Quality Influence Where People Live?",
    "section": "The Data",
    "text": "The Data\n\nEnvironmental Quality Index\nTo develop the EQI, variables were identified from available data to represent each environmental domain and assessed for collinearity so redundant variables could be excluded. Variables were standardized based on geographic space or on a per capita rate, as appropriate and transformations such as log-transformations were performed as needed based on the normality of each variable. Data gaps were evaluated to distinguish between missing data and meaningful zeros. Where applicable, spatial kriging was used to interpolate values when data was not available for all counties. Principal component analysis was used to aggregate variables into domain specific indexes. The domain indexes were then aggregated into overall indexes for each county. A result of this method is that each domain does not equally influence the overall EQI value for a given county. The EQI is developed to be normally distributed with mean=0 and standard deviation=1. Higher EQI values correspond with worse environmental quality. Lower (more negative) EQI values correspond with better environmental quality.\n\n\nCensus Population Data\nCounty level population data was obtained from the U.S. Census Bureau’s county intercensal datasets for 2000-2010. Percent population change was calculated for 2006-2010 then winsorized to remove outliers above the 99.9th percentile (ie. counties with population change above 28.5%)."
  },
  {
    "objectID": "posts/2021-12-02-environmental-quality-population-change/index.html#statistical-analysis",
    "href": "posts/2021-12-02-environmental-quality-population-change/index.html#statistical-analysis",
    "title": "Does Environmental Quality Influence Where People Live?",
    "section": "Statistical Analysis",
    "text": "Statistical Analysis\nKalawao County, Hawaii had the lowest EQI value (highest environmental quality) and greatest decrease in population (-17.4%). Falls Church, Virginia had the highest EQI (lowest environmental quality) and a population change of 15.4% which falls is the 99th percentile. The summary statistics in Table 2 suggest that population growth is correlated with worse environmental characteristics.\n\n\n\nEQI summary statistics for counties based on population change between 2006 and 2010\n \n  \n    Population Change \n    Min \n    Max \n    Mean \n    Standard Deviation \n    Variance \n    Number of Counties \n  \n \n\n  \n    negative \n    -5.88 \n    2.59 \n    -0.24 \n    0.93 \n    0.87 \n    1088 \n  \n  \n    positive \n    -5.05 \n    2.85 \n    0.13 \n    1.01 \n    1.02 \n    2052 \n  \n  \n    all counties \n    -5.88 \n    2.85 \n    0.00 \n    1.00 \n    1.00 \n    3140 \n  \n\n\n\n\n\n\nHypothesis Testing\nA different means test was completed to determine if mean environmental quality was statistically different for counties that experienced positive vs. negative population change between 2006-2010. The figure below shows a histogram of EQI values for counties with negative and positive population change.\n\n\n\n\n\nnull hypothesis: There is no difference in mean EQI for counties with positive and negative population change.\n\\[H_{0}: \\mu_{posPopChange} - \\mu_{negPopChange} = 0\\]\nalternative hypothesis: There is a difference in mean EQI for counties with positive and negative population change.\n\\[H_{A}: \\mu_{posPopChange} - \\mu_{negPopChange} \\neq 0\\]\n\n\n\n\n\n\n\\[\\text{point estimate} = \\mu_{posPopChange} - \\mu_{negPopChange} =0.131 - -0.245 = 0.376\\]\nThe standard error for the difference in means is:\n\\[SE = \\sqrt{\\frac{s_1^2}{n_1} + \\frac{s^2_2}{n_2}} = \\sqrt{\\frac{1.009^2}{2052} + \\frac{0.933^2}{1088}} = 0.036\\]\nThe z-score for hypothesis testing is:\n\\[z = \\frac{\\text{point estimate - null}}{SE} = \\frac{0.376 - 0}{0.036} = 10.443\\]\n\n\n\nThe p-value, the probability of getting a point estimate at least as extreme as calculated if the null hypothesis were true, is:\n\\[p\\text {-value }=\\operatorname{Pr}(Z<-|z| \\text { or } Z>|z|)=2 * \\operatorname{Pr}(Z>|z|) = 1.5809578\\times 10^{-25}\\]\n\n\n\nSince the p-value is < 0.001 we reject the null that there is no difference in EQI for counties with positive population change versus negative population change. There is a statistically significant difference (at the 0.1% significance level) in EQI across the two population change groups. The 95% confidence interval ranges from 0.31 to 0.45. This means that there is a 95% chance that this interval includes the true difference in mean EQI between counties with positive and negative percent population change.\n\n\nLinear Regression\nLinear regression was used to model the relationship between population change and environmental quality using the overall EQI value and each domain specific EQI to determine if a particular domain was a stronger predictor of population change.\n\\[\\text{percent population change}_i=\\beta_{0}+\\beta_{1} \\cdot EQI_i + \\varepsilon_i\\]\n\n\n\n\n\nFirst, hypothesis testing was used to test whether the slope coefficient for the percent population change rate is equal to zero or not.\nnull hypothesis: The slope coefficient is equal to zero\n\\[H_{0}: \\beta_{1} = 0\\] alternative hypothesis: The slope coefficient is NOT equal to zero\n\\[H_{A}: \\beta_{1} \\neq 0\\] ::: {.cell}\n:::\nThe point estimate, \\(\\beta_1\\) = 0.991 and the standard error, SE = 0.077.\n\\[z = \\frac{\\text{point estimate - null}}{SE} = \\frac{{0.991 - 0}}{0.077} = 12.817\\]\n\\[p\\text {-value }=\\operatorname{Pr}(Z<-|z| \\text { or } Z>|z|)=2 * \\operatorname{Pr}(Z>|z|) = 1.0849376\\times 10^{-36}\\]\n\n\n\nSince the p-value for the slope coefficient was < 0.001, we reject the null hypothesis that EQI has no influence on population change at the 0.1% level. There is a statistically significant relationship between EQI and percent population change and the coefficient is significantly different from zero. Based on value of \\(\\beta_1\\), for each one unit increase in EQI, the percent population change increases by 0.991. The 95% confidence interval for the slope coefficient ranges from 0.84 to 1.143. This means that there is a 95% chance that this interval includes the true county level rate of change for percent population change for each one unit change in EQI.\n\n\nDomain Specific Linear Models\n\\[\\text{percent population change}_i=\\beta_{0,domain}+\\beta_{1,domain} \\cdot EQI_{i,domain} + \\varepsilon_i\\] ::: {.cell}\n:::\n\n\n\n\n\n\n\n\nWhile a partly manual method was used above, statistical functions in R were used to test for the significance of domain models. Table 3 presents coefficients for each domain specific model. The numbers in [brackets] are the 95% confidence intervals for each estimated coefficient.\n\n\n\nSummary of EQI Domain Slope Coefficients\n \n  \n    coefficient \n    overall EQI \n    air model \n    water model \n    land model \n    built model \n    sociodem model \n  \n \n\n  \n    Intercept \n    1.868 *** \n    1.869 *** \n    1.868 *** \n    1.868 *** \n    1.867 *** \n    1.868 *** \n  \n  \n     \n    [1.716, 2.019] \n    [1.717, 2.020] \n    [1.714, 2.023] \n    [1.713, 2.024] \n    [1.713, 2.021] \n    [1.718, 2.019] \n  \n  \n    EQI \n    0.991 *** \n     \n     \n     \n     \n     \n  \n  \n     \n    [0.840, 1.143] \n     \n     \n     \n     \n     \n  \n  \n    air EQI \n     \n    0.998 *** \n     \n     \n     \n     \n  \n  \n     \n     \n    [0.847, 1.150] \n     \n     \n     \n     \n  \n  \n    water EQI \n     \n     \n    0.485 *** \n     \n     \n     \n  \n  \n     \n     \n     \n    [0.331, 0.640] \n     \n     \n     \n  \n  \n    land EQI \n     \n     \n     \n    -0.314 *** \n     \n     \n  \n  \n     \n     \n     \n     \n    [-0.469, -0.158] \n     \n     \n  \n  \n    built EQI \n     \n     \n     \n     \n    0.635 *** \n     \n  \n  \n     \n     \n     \n     \n     \n    [0.481, 0.790] \n     \n  \n  \n    sociodem EQI \n     \n     \n     \n     \n     \n    1.068 *** \n  \n  \n     \n     \n     \n     \n     \n     \n    [0.917, 1.219] \n  \n  \n    n \n    3140 \n    3140 \n    3140 \n    3140 \n    3140 \n    3140 \n  \n  \n    R2 \n    0.050 \n    0.051 \n    0.012 \n    0.005 \n    0.020 \n    0.058 \n  \n\n\n\n\n\nThe figure below provides a visual comparison of each model result. The bold portion of the line represents the 90% confidence interval and the full line represents the 95% confidence interval for each estimate.\n\n\n\n\n\nThe p-value on the slope coefficient was < 0.001 for all domain specific linear models which indicates a statistically significant relationship at the 0.01% level. Based on the \\(R^2\\) values and slope coefficients, the air and sociodemographic domains account for most of the overall relationship between population change and EQI. All domains except land are positively correlated with population change. Since higher EQI values indicate poorer environmental quality, these models show that population increased more in counties with worse environmental conditions. For a one unit increase in sociodemogrpahic EQI, the percent population change increases by 1.068. For a one unit increase in air EQI, the percent population change increases by 0.998. The \\(R^2\\) terms represent the variance in percent population change that can be explained by EQI. For the overall EQI value, 5% of the variance in percent population change is explained by environmental conditions. The sociodemographic EQI explains 5.8% of the variance in population change while the air EQI explains 5.1%."
  },
  {
    "objectID": "posts/2021-12-02-environmental-quality-population-change/index.html#conclusions",
    "href": "posts/2021-12-02-environmental-quality-population-change/index.html#conclusions",
    "title": "Does Environmental Quality Influence Where People Live?",
    "section": "Conclusions",
    "text": "Conclusions\nThe identified relationships between population change and environmental quality are noteworthy for their public health and environmental justice implications. Positive population trends in areas with worse environmental conditions could result in increased incidences of cancer, asthma, obesity, and infant mortality. While this project did not evaluate economic variables, locations with higher environmental quality could also have higher living costs which drive people to move to more affordable places. If economic factors contribute to population growth in counties with poor environmental quality, then this could negatively affect the health of vulnerable populations and perpetuate social inequalities. Further analysis could evaluate trends in mean household income to determine if there is growing income inequality between counties with better and worse environmental quality. Economic variables and other factors influencing demographic shifts from rural areas to cities may be stronger predictors of population change than environmental quality.\nData availability:\nEPA Datasets and files from the EQI county data from 2006-2010\nU.S. Census Bureau County Intercensal Tables: 2000-2010"
  },
  {
    "objectID": "posts/2021-12-02-environmental-quality-population-change/index.html#references",
    "href": "posts/2021-12-02-environmental-quality-population-change/index.html#references",
    "title": "Does Environmental Quality Influence Where People Live?",
    "section": "References",
    "text": "References\n\nU.S. EPA. Environmental Quality Index - Technical Report (2006-2010) (Final, 2020). U.S. Environmental Protection Agency, Washington, DC, EPA/600/R-20/367, 2020.\nU.S. Census Bureau. County Intercensal Datasets: 2000-2010. https://www.census.gov/data/datasets/time-series/demo/popest/intercensal-2000-2010-counties.html"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Marie T. Rivers",
    "section": "",
    "text": "LinkedIn\n  \n  \n      Twitter\n  \n  \n      GitHub\n  \n\n  \n  \n\n\nBren School of Environmental Science & Management University of California, Santa Barbara (2022)\n\nDepartment of Civil and Environmental Engineering University of Massachusetts, Amherst (2011)\n\nDepartment of Civil and Environmental Engineering University of Delaware (2009)"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "1 + 1\n\n[1] 2"
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Posts",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nopen science\n\n\n\n\nCommunicating climate and water resource data to a wider audience\n\n\n\n\n\n\nJun 7, 2022\n\n\nMarie Rivers\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\ndata visualization\n\n\n\n\nA modern update to the Gantt chart\n\n\n\n\n\n\nFeb 8, 2022\n\n\nMarie Rivers\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nR\n\n\n\n\nPopulation Change vs. the EPA Environmental Quality Index (EQI): a statistical analysis\n\n\n\n\n\n\nDec 2, 2021\n\n\nMarie Rivers\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nR\n\n\ndata visualization\n\n\n\n\nVisualization of Alaska household languages data\n\n\n\n\n\n\nNov 3, 2021\n\n\nMarie Rivers\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPython\n\n\nR\n\n\nMEDS\n\n\n\n\nIf you are new to coding, what first steps should you take?\n\n\n\n\n\n\nOct 17, 2021\n\n\nMarie Rivers\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nMEDS\n\n\n\n\nlife beyond excel spreadsheets\n\n\n\n\n\n\nAug 18, 2012\n\n\nMarie Rivers\n\n\n\n\n\n\nNo matching items"
  }
]