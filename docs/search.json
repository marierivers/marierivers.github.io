[
  {
    "objectID": "code_samples.html",
    "href": "code_samples.html",
    "title": "Code Samples",
    "section": "",
    "text": "Wind Resource Temporal Variability\n\n\n\nPython\n\n\nrenewable energy\n\n\n\nDiurnal and monthly variability of NREL Wind Toolkit Data\n\n\n\nMarie Rivers\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Wrangling with Python and R\n\n\n\nPython\n\n\nR\n\n\nData Wrangling\n\n\n\nExamples of common data wrangling commands with Python and R\n\n\n\nMarie Rivers\n\n\nSep 23, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShiny Apps\n\n\n\nR\n\n\nShiny App\n\n\n\na sample of projects with interactive web applications\n\n\n\nMarie Rivers\n\n\nJun 10, 2022\n\n\n\n\n\n\n\n\n\n\n \n\n\n\nProtecting Whales from Ships\n\n\n\nPython\n\n\ngeopandas\n\n\nstatial analysis\n\n\n\nA spatial analysis using Python and GeoPandas\n\n\n\nMarie Rivers\n\n\nApr 3, 2022\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "code_samples/wind-resource-temporal-variability/index.html",
    "href": "code_samples/wind-resource-temporal-variability/index.html",
    "title": "Wind Resource Temporal Variability",
    "section": "",
    "text": "under construction\nThis code sample uses statistical analyses and visualizations to explore the diurnal and monthly variability of wind resources at Mount Washington in New Hampshire using data from the National Renewable Energy Laboratory (NREL) Wind Integration National Dataset (WIND) Toolkit. Datasets within this tool include meteorological conditions such as temperature, pressure, relative humidity, wind direction, and wind speed. Hourly data is available for the continental United States from 2007 to 2013. This analysis used the dataset of wind speed at 100 meters for the year 2012.\n\n\n\n\n\n\nNote\n\n\n\nClick the drop down to the right of the title to view all code written for this analysis\n\n\n\n\n\n\n\n\n\nTelevision slides\n\n\nGitHub Square source code\n\n\n\n\n\nAccess data\nThe code below was used to access the data with the h5pyd Python package and NREL Highly Scalable Data Service (HSDS). The data are stored in the file wtk_us.h5 which is in HDF5 format. Data are stored in three dimensions dataset[t, y, x] where x is the longitudinal index, y is the latitudinal index, and t is the temporal index. Timestamps are in the UTC time zone.\n\n\n\n\n\n\nCitationBibTeX citation:@online{rivers,\n  author = {Marie Rivers},\n  title = {Wind {Resource} {Temporal} {Variability}},\n  url = {https://marierivers.github.io/code_samples/wind-resource-temporal-variability/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nMarie Rivers. n.d. “Wind Resource Temporal Variability.” https://marierivers.github.io/code_samples/wind-resource-temporal-variability/."
  },
  {
    "objectID": "code_samples/2022-04-03-protecting-whales-from-ships/index.html",
    "href": "code_samples/2022-04-03-protecting-whales-from-ships/index.html",
    "title": "Protecting Whales from Ships",
    "section": "",
    "text": "This analysis identifies a speed reduction zone off the island of Dominica for the purpose of reducing the occurrence of ships striking whales and quantifies the impact of reduced travel speeds on marine traffic."
  },
  {
    "objectID": "code_samples/2022-04-03-protecting-whales-from-ships/index.html#load-data",
    "href": "code_samples/2022-04-03-protecting-whales-from-ships/index.html#load-data",
    "title": "Protecting Whales from Ships",
    "section": "Load data",
    "text": "Load data\n\n# Join folder path and filename \nfp3 = os.path.join(input_folder2, \"station1249.csv\")\n\n# Print out the full file path\nprint(fp3)\n\ndata/station1249.csv\n\n\n\n# Read file using gpd.read_file()\nvessels = gpd.read_file(fp3)\n\n\ntype(vessels)\n\n<class 'geopandas.geodataframe.GeoDataFrame'>\n\n\n\nvessels.head\n\n<bound method NDFrame.head of        field_1       MMSI        LON       LAT            TIMESTAMP geometry\n0            0  233092000  -61.84788  15.23238  2015-05-22 13:53:26     None\n1            1  255803280  -61.74397  15.96114  2015-05-22 13:52:57     None\n2            2  329002300  -61.38968  15.29744  2015-05-22 13:52:32     None\n3            3  257674000  -61.54395   16.2334  2015-05-22 13:52:24     None\n4            4  636092006  -61.52401  15.81954  2015-05-22 13:51:23     None\n...        ...        ...        ...       ...                  ...      ...\n617257  238722  256525000  -61.40679  15.36907  2015-05-21 21:34:59     None\n617258  238723  311077100  -61.37539  15.27406  2015-05-21 21:34:55     None\n617259  238724  377907247  -61.39461  15.30672  2015-05-21 21:34:46     None\n617260  238725  253365000  -61.49001  16.14007  2015-05-21 21:34:46     None\n617261  238726  329002300  -61.48073  15.44751  2015-05-21 21:34:45     None\n\n[617262 rows x 6 columns]>\n\n\n\n# bootstrap the geometries\nvessel_points = gpd.points_from_xy(vessels['LON'], vessels['LAT'])\nvessel_gdf = gpd.GeoDataFrame(vessels, geometry=vessel_points)\n\n\n# project the dataset into an appropriate CRS\nvessel_gdf = vessel_gdf.set_crs(epsg=4326)\nvessel_gdf = vessel_gdf.to_crs(epsg=proj_area_crs)\n\n\nvessel_gdf.crs\n\n<Derived Projected CRS: EPSG:2002>\nName: Dominica 1945 / British West Indies Grid\nAxis Info [cartesian]:\n- E[east]: Easting (metre)\n- N[north]: Northing (metre)\nArea of Use:\n- name: Dominica - onshore.\n- bounds: (-61.55, 15.14, -61.2, 15.69)\nCoordinate Operation:\n- name: British West Indies Grid\n- method: Transverse Mercator\nDatum: Dominica 1945\n- Ellipsoid: Clarke 1880 (RGS)\n- Prime Meridian: Greenwich\n\n\n\nvessel_gdf['TIMESTAMP'] = pd.to_datetime(vessel_gdf['TIMESTAMP'])\n\n\nvessel_gdf.head()\n\n  field_1       MMSI  ...           TIMESTAMP                        geometry\n0       0  233092000  ... 2015-05-22 13:53:26  POINT (415373.315 1683307.035)\n1       1  255803280  ... 2015-05-22 13:52:57  POINT (426434.345 1763918.193)\n2       2  329002300  ... 2015-05-22 13:52:32  POINT (464555.392 1690588.725)\n3       3  257674000  ... 2015-05-22 13:52:24  POINT (447770.634 1794068.620)\n4       4  636092006  ... 2015-05-22 13:51:23  POINT (450006.361 1748297.844)\n\n[5 rows x 6 columns]\n\n\n\n# plot of all vessel points\n\nvessel_gdf.plot(figsize=(5,10))\n\n\n\n\n\n# plot of all vessel points\nbase = dominica.plot(facecolor='none', edgecolor='black', linewidth=3, figsize=(15, 15))\nvessel_gdf.plot(ax=base, markersize = 3)\nspeed_reduction_zone.plot(ax=base, edgecolor='red', linewidth=2)\n\n\n\n\n\n# spatially subset AIS data to only include vessels within identified whale habitat\nvessels_in_whale_habitat = vessel_gdf.sjoin(speed_reduction_zone, how=\"inner\")\nvessels_in_whale_habitat\n\n       field_1       MMSI  ...                        geometry index_right\n2            2  329002300  ...  POINT (464555.392 1690588.725)           0\n7            7  338143127  ...  POINT (463892.452 1694650.397)           0\n13          13  329002300  ...  POINT (464555.389 1690589.831)           0\n15          15  338143015  ...  POINT (463910.683 1694655.978)           0\n16          16  338143127  ...  POINT (463697.964 1694341.275)           0\n...        ...        ...  ...                             ...         ...\n617252  238717  329002300  ...  POINT (453901.647 1709712.916)           0\n617253  238718  338143015  ...  POINT (463915.972 1694683.643)           0\n617255  238720  338143127  ...  POINT (463905.177 1694705.734)           0\n617259  238724  377907247  ...  POINT (464023.288 1691613.624)           0\n617261  238726  329002300  ...  POINT (454741.236 1707161.130)           0\n\n[167411 rows x 7 columns]"
  },
  {
    "objectID": "code_samples/2022-04-03-protecting-whales-from-ships/index.html#calculate-distance-and-speed",
    "href": "code_samples/2022-04-03-protecting-whales-from-ships/index.html#calculate-distance-and-speed",
    "title": "Protecting Whales from Ships",
    "section": "Calculate distance and speed",
    "text": "Calculate distance and speed\n\n# plot of only vessel points within speed reduction zone\nbase = dominica.plot(facecolor='none', linewidth=3, figsize=(15, 15))\nspeed_reduction_zone.plot(ax=base, facecolor='none', edgecolor='red', linewidth=3)\nvessels_in_whale_habitat.plot(ax=base, markersize = 0.5, facecolor='black')\nctx.add_basemap(ax=base, crs=dominica.crs.to_string())\n\n\n# sort vessel dataframe by MMSI and time\nvessels_in_whale_habitat = vessels_in_whale_habitat.sort_values(by=['MMSI', 'TIMESTAMP'])\nvessels_in_whale_habitat\n\n       field_1       MMSI  ...                        geometry index_right\n235025  235025  203106200  ...  POINT (462476.396 1680935.224)           0\n235018  235018  203106200  ...  POINT (462283.995 1681393.698)           0\n235000  235000  203106200  ...  POINT (461936.769 1682722.187)           0\n234989  234989  203106200  ...  POINT (461798.818 1683708.377)           0\n234984  234984  203106200  ...  POINT (461654.150 1683997.765)           0\n...        ...        ...  ...                             ...         ...\n259103  259103  983191049  ...  POINT (465250.372 1690066.434)           0\n259094  259094  983191049  ...  POINT (465243.965 1690054.249)           0\n258954  258954  983191049  ...  POINT (465226.597 1690121.667)           0\n258930  258930  983191049  ...  POINT (465242.895 1690053.140)           0\n258206  258206  983191049  ...  POINT (465272.964 1690049.908)           0\n\n[167411 rows x 7 columns]\n\n\n\n# create a copy of the vessel dataframe and shift each observation down one row using `shift()`\nvessels_shift = vessels_in_whale_habitat.copy(deep=True).shift(periods=1)\n\n\n# rename shifted column names\nvessels_shift = vessels_shift.rename(columns={\"field_1\": \"field_1_shift\", \"MMSI\": \"MMSI_shift\", \"LON\": \"LON_shift\", \"LAT\": \"LAT_shift\", \"TIMESTAMP\": \"TIMESTAMP_shift\", \"geometry\": \"geometry_shift\", \"index_right\": \"index_right_shift\"})\n\n\n# join original dataframe with the shifted copy using `join()`\nvessels_shift_join = vessels_in_whale_habitat.join(vessels_shift).sort_values(by=['MMSI', 'TIMESTAMP'])\n\n\n# drop all rows in the joined dataframe in which the MMSI of the left is not the same as the one on the right\nvessels_keep = vessels_shift_join.drop(vessels_shift_join[vessels_shift_join['MMSI'] != vessels_shift_join['MMSI_shift']].index)\n\n\n# set the geometry column\nvessels_keep = vessels_keep.set_geometry(\"geometry\")\nvessels_keep2 = vessels_keep.set_geometry(\"geometry_shift\")\n\n\n# calculate distance between each observation\nvessels_keep['distance_m'] = vessels_keep.distance(vessels_keep2)\n\n\n# calculate time difference between each observation to the next\nvessels_keep['time'] = vessels_keep['TIMESTAMP'] - vessels_keep['TIMESTAMP_shift']\n\n\n# calculate speed\nmeters_per_nm = 1852\n\nvessels_keep['speed_m_per_sec'] = vessels_keep['distance_m'] / vessels_keep['time'].dt.total_seconds()\nvessels_keep['speed_knots'] = vessels_keep['speed_m_per_sec'] * 60 * 60 / meters_per_nm\nvessels_keep['time_10knots_minutes'] = (vessels_keep['distance_m'] * 60 ) / ( meters_per_nm * 10 )\nvessels_keep['time_dif_minutes'] = vessels_keep['time_10knots_minutes'] - (vessels_keep['time'].dt.total_seconds() / 60 )\nvessels_keep\n\n       field_1       MMSI  ... time_10knots_minutes time_dif_minutes\n235018  235018  203106200  ...             1.610828        -0.889172\n235000  235000  203106200  ...             4.448540        -3.034793\n234989  234989  203106200  ...             3.226109        -1.773891\n234984  234984  203106200  ...             1.048164        -1.468503\n234972  234972  203106200  ...             1.394116        -3.589217\n...        ...        ...  ...                  ...              ...\n259103  259103  983191049  ...             0.043139        -5.940194\n259094  259094  983191049  ...             0.044599        -4.355401\n258954  258954  983191049  ...             0.225548       -55.824452\n258930  258930  983191049  ...             0.228202       -11.471798\n258206  258206  983191049  ...             0.097976      -248.585358\n\n[166255 rows x 20 columns]\n\n\n\nvessels_keep = vessels_keep.sort_values(by=['speed_knots'], ascending=False)\n\n\n# look at the vessels that would be affected by the speed reduction zone\nvessels_going_too_fast = vessels_keep.drop(vessels_keep[vessels_keep['time_dif_minutes'] < 0].index)\nvessels_going_too_fast\n\n       field_1       MMSI  ... time_10knots_minutes time_dif_minutes\n585844  207309  341387000  ...             0.209101         0.209101\n67091    67091  227528210  ...             7.979167         6.245834\n66925    66925  228008600  ...            13.481637        10.498303\n499754  121219  329002300  ...             8.728323         6.711656\n546817  168282  329002300  ...            12.866650         9.849984\n...        ...        ...  ...                  ...              ...\n616429  237894  636091437  ...             0.000000         0.000000\n616427  237892  636091437  ...             0.000000         0.000000\n616422  237887  636091437  ...             0.000000         0.000000\n616419  237884  636091437  ...             0.000000         0.000000\n616408  237873  636091437  ...             0.000000         0.000000\n\n[21410 rows x 20 columns]\n\n\n\nshipping_impact_minutes = vessels_going_too_fast['time_dif_minutes'].sum()\nshipping_impact_days = round(shipping_impact_minutes / ( 60 * 24), 2)\nshipping_impact_days\n\n27.88\n\n\nA 10-knot reduced speed zone in the identified whale habitat will increase travel time by approximately 27.88 days."
  },
  {
    "objectID": "code_samples/data-wrangling-with-python-and-R/index.html",
    "href": "code_samples/data-wrangling-with-python-and-R/index.html",
    "title": "Data Wrangling with Python and R",
    "section": "",
    "text": "The purpose of this document is to illustrate common data wrangling commands with R and Python. These examples use data from the lterdatasampler package."
  },
  {
    "objectID": "code_samples/data-wrangling-with-python-and-R/index.html#head-and-tail",
    "href": "code_samples/data-wrangling-with-python-and-R/index.html#head-and-tail",
    "title": "Data Wrangling with Python and R",
    "section": "Head and Tail",
    "text": "Head and Tail\nHead returns the first few rows of the data frame and tail returns the last rows. The integer in the examples below is optional and used to specify the number of rows returned.\n\nR Project RPython Python\n\n\n\nhead(trout_salamander_R, 5) # include an integrer is you want to specify the number of rows returned\n\n# A tibble: 5 × 16\n   year sitecode section reach  pass unitnum unittype vert_index pitnumber\n  <dbl> <chr>    <chr>   <chr> <dbl>   <dbl> <chr>         <dbl>     <dbl>\n1  1987 MACKCC-L CC      L         1       1 R                 1        NA\n2  1987 MACKCC-L CC      L         1       1 R                 2        NA\n3  1987 MACKCC-L CC      L         1       1 R                 3        NA\n4  1987 MACKCC-L CC      L         1       1 R                 4        NA\n5  1987 MACKCC-L CC      L         1       1 R                 5        NA\n# … with 7 more variables: species <chr>, length_1_mm <dbl>, length_2_mm <dbl>,\n#   weight_g <dbl>, clip <chr>, sampledate <date>, notes <chr>\n\ntail(trout_salamander_R)\n\n# A tibble: 6 × 16\n   year sitecode section reach  pass unitnum unittype vert_index pitnumber\n  <dbl> <chr>    <chr>   <chr> <dbl>   <dbl> <chr>         <dbl>     <dbl>\n1  2019 MACKOG-U OG      U         2      16 C                21        NA\n2  2019 MACKOG-U OG      U         2      16 C                22        NA\n3  2019 MACKOG-U OG      U         2      16 C                23   1043503\n4  2019 MACKOG-U OG      U         2      16 C                24   1043547\n5  2019 MACKOG-U OG      U         2      16 C                25   1043583\n6  2019 MACKOG-U OG      U         2      16 C                26   1043500\n# … with 7 more variables: species <chr>, length_1_mm <dbl>, length_2_mm <dbl>,\n#   weight_g <dbl>, clip <chr>, sampledate <date>, notes <chr>\n\n\n\n\n\ntrout_salamander_py.head(5) # include an integrer is you want to specify the number of rows returned\n\n   year  sitecode section reach  ...  weight_g  clip  sampledate  notes\n0  1987  MACKCC-L      CC     L  ...      1.75  NONE  1987-10-07    NaN\n1  1987  MACKCC-L      CC     L  ...      1.95  NONE  1987-10-07    NaN\n2  1987  MACKCC-L      CC     L  ...      5.60  NONE  1987-10-07    NaN\n3  1987  MACKCC-L      CC     L  ...      2.15  NONE  1987-10-07    NaN\n4  1987  MACKCC-L      CC     L  ...      6.90  NONE  1987-10-07    NaN\n\n[5 rows x 16 columns]\n\ntrout_salamander_py.tail()\n\n       year  sitecode section reach  ...  weight_g  clip  sampledate        notes\n32204  2019  MACKOG-U      OG     U  ...       7.9  NONE  2019-09-05          NaN\n32205  2019  MACKOG-U      OG     U  ...       8.7  NONE  2019-09-05          NaN\n32206  2019  MACKOG-U      OG     U  ...       9.6  NONE  2019-09-05          NaN\n32207  2019  MACKOG-U      OG     U  ...      14.3  NONE  2019-09-05          NaN\n32208  2019  MACKOG-U      OG     U  ...      11.6  NONE  2019-09-05  Terrestrial\n\n[5 rows x 16 columns]"
  },
  {
    "objectID": "code_samples/data-wrangling-with-python-and-R/index.html#class-type",
    "href": "code_samples/data-wrangling-with-python-and-R/index.html#class-type",
    "title": "Data Wrangling with Python and R",
    "section": "Class / Type",
    "text": "Class / Type\n\nR Project RPython Python\n\n\n\nclass(trout_salamander_R)\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\n\n\n\nprint(type(trout_salamander_py))\n\n<class 'pandas.core.frame.DataFrame'>"
  },
  {
    "objectID": "code_samples/data-wrangling-with-python-and-R/index.html#shape",
    "href": "code_samples/data-wrangling-with-python-and-R/index.html#shape",
    "title": "Data Wrangling with Python and R",
    "section": "Shape",
    "text": "Shape\n\nR Project RPython Python\n\n\n\n\n\nHere R and Python both tell us that the dataframe has 32,209 rows and 16 columns.\n\n\n\n\n\n\nNote\n\n\n\nHow to format inline code to include a comma for the thousands separator.\nr format(round(trout_salamander_nrow), big.mark=‘,’)\n\n\n\ndim(trout_salamander_R) # returns the number of rows and columns in a data frame\n\n[1] 32209    16\n\nnrow(trout_salamander_R)\n\n[1] 32209\n\nncol(trout_salamander_R)\n\n[1] 16\n\n\n\n\n\ntrout_salamander_py.shape\n\n(32209, 16)\n\ntrout_salamander_py.shape[0] # number of rows\n\n32209\n\ntrout_salamander_py.shape[1] # number of columns\n\n16"
  },
  {
    "objectID": "code_samples/data-wrangling-with-python-and-R/index.html#summary-describe",
    "href": "code_samples/data-wrangling-with-python-and-R/index.html#summary-describe",
    "title": "Data Wrangling with Python and R",
    "section": "Summary / Describe",
    "text": "Summary / Describe\n\nR Project RPython Python\n\n\n\nsummary(trout_salamander_R)\n\n      year        sitecode           section             reach          \n Min.   :1987   Length:32209       Length:32209       Length:32209      \n 1st Qu.:1998   Class :character   Class :character   Class :character  \n Median :2006   Mode  :character   Mode  :character   Mode  :character  \n Mean   :2005                                                           \n 3rd Qu.:2012                                                           \n Max.   :2019                                                           \n                                                                        \n      pass          unitnum         unittype           vert_index    \n Min.   :1.000   Min.   : 1.000   Length:32209       Min.   :  1.00  \n 1st Qu.:1.000   1st Qu.: 3.000   Class :character   1st Qu.:  5.00  \n Median :1.000   Median : 7.000   Mode  :character   Median : 13.00  \n Mean   :1.224   Mean   : 7.696                      Mean   : 20.17  \n 3rd Qu.:1.000   3rd Qu.:11.000                      3rd Qu.: 27.00  \n Max.   :2.000   Max.   :20.000                      Max.   :147.00  \n                                                                     \n   pitnumber          species           length_1_mm      length_2_mm   \n Min.   :   62048   Length:32209       Min.   : 19.00   Min.   : 28.0  \n 1st Qu.:13713632   Class :character   1st Qu.: 47.00   1st Qu.: 77.0  \n Median :18570447   Mode  :character   Median : 63.00   Median : 98.0  \n Mean   :16286432                      Mean   : 73.83   Mean   :100.5  \n 3rd Qu.:19132429                      3rd Qu.: 97.00   3rd Qu.:119.0  \n Max.   :28180046                      Max.   :253.00   Max.   :284.0  \n NA's   :26574                         NA's   :17       NA's   :19649  \n    weight_g           clip             sampledate            notes          \n Min.   :  0.090   Length:32209       Min.   :1987-10-06   Length:32209      \n 1st Qu.:  1.510   Class :character   1st Qu.:1998-09-04   Class :character  \n Median :  6.050   Mode  :character   Median :2006-09-06   Mode  :character  \n Mean   :  8.903                      Mean   :2005-08-05                     \n 3rd Qu.: 11.660                      3rd Qu.:2012-09-05                     \n Max.   :134.590                      Max.   :2019-09-05                     \n NA's   :13268                                                               \n\n\n\n\n\ntrout_salamander_py.describe()\n\n               year          pass  ...   length_2_mm      weight_g\ncount  32209.000000  32209.000000  ...  12560.000000  18941.000000\nmean    2004.917601      1.223664  ...    100.485191      8.902859\nstd        8.572474      0.416706  ...     34.736955     10.676276\nmin     1987.000000      1.000000  ...     28.000000      0.090000\n25%     1998.000000      1.000000  ...     77.000000      1.510000\n50%     2006.000000      1.000000  ...     98.000000      6.050000\n75%     2012.000000      1.000000  ...    119.000000     11.660000\nmax     2019.000000      2.000000  ...    284.000000    134.590000\n\n[8 rows x 8 columns]\n\n\n\ntrout_salamander_py.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 32209 entries, 0 to 32208\nData columns (total 16 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   year         32209 non-null  int64  \n 1   sitecode     32209 non-null  object \n 2   section      32209 non-null  object \n 3   reach        32209 non-null  object \n 4   pass         32209 non-null  int64  \n 5   unitnum      32209 non-null  float64\n 6   unittype     31599 non-null  object \n 7   vert_index   32209 non-null  int64  \n 8   pitnumber    5635 non-null   float64\n 9   species      32206 non-null  object \n 10  length_1_mm  32192 non-null  float64\n 11  length_2_mm  12560 non-null  float64\n 12  weight_g     18941 non-null  float64\n 13  clip         32209 non-null  object \n 14  sampledate   32209 non-null  object \n 15  notes        3174 non-null   object \ndtypes: float64(5), int64(3), object(8)\nmemory usage: 3.9+ MB"
  },
  {
    "objectID": "code_samples/data-wrangling-with-python-and-R/index.html#variable-names",
    "href": "code_samples/data-wrangling-with-python-and-R/index.html#variable-names",
    "title": "Data Wrangling with Python and R",
    "section": "Variable Names",
    "text": "Variable Names\n\nR Project RPython Python\n\n\n\nnames(trout_salamander_R) # returns column names of a data frame\n\n [1] \"year\"        \"sitecode\"    \"section\"     \"reach\"       \"pass\"       \n [6] \"unitnum\"     \"unittype\"    \"vert_index\"  \"pitnumber\"   \"species\"    \n[11] \"length_1_mm\" \"length_2_mm\" \"weight_g\"    \"clip\"        \"sampledate\" \n[16] \"notes\"      \n\n\n\n\n\ntrout_salamander_py.columns\n\nIndex(['year', 'sitecode', 'section', 'reach', 'pass', 'unitnum', 'unittype',\n       'vert_index', 'pitnumber', 'species', 'length_1_mm', 'length_2_mm',\n       'weight_g', 'clip', 'sampledate', 'notes'],\n      dtype='object')"
  },
  {
    "objectID": "code_samples/data-wrangling-with-python-and-R/index.html#unique",
    "href": "code_samples/data-wrangling-with-python-and-R/index.html#unique",
    "title": "Data Wrangling with Python and R",
    "section": "Unique",
    "text": "Unique\n\nR Project RPython Python\n\n\nGet the unique values from a specified column in a dataframe\n\nunique(trout_salamander_R$species)\n\n[1] \"Cutthroat trout\"            NA                          \n[3] \"Coastal giant salamander\"   \"Cascade torrent salamander\"\n\n\n\n\nGet unique values of a variable\n\ntrout_salamander_py.species.unique()\n\narray(['Cutthroat trout', nan, 'Coastal giant salamander',\n       'Cascade torrent salamander'], dtype=object)\n\n\nGet number of unique values\n\nlen(pd.unique(trout_salamander_py['species']))\n\n4\n\n\nGet number of observations for each unique value\n\ntrout_salamander_py.groupby(['species']).count()\n\n                             year  sitecode  section  ...   clip  sampledate  notes\nspecies                                               ...                          \nCascade torrent salamander     15        15       15  ...     15          15      1\nCoastal giant salamander    11758     11758    11758  ...  11758       11758   1199\nCutthroat trout             20433     20433    20433  ...  20433       20433   1971\n\n[3 rows x 15 columns]"
  },
  {
    "objectID": "code_samples/data-wrangling-with-python-and-R/index.html#scatter-plot",
    "href": "code_samples/data-wrangling-with-python-and-R/index.html#scatter-plot",
    "title": "Data Wrangling with Python and R",
    "section": "Scatter Plot",
    "text": "Scatter Plot\n\nR Project RPython Python\n\n\n\ntrout_salamander_R <- trout_salamander_R %>% \n  filter(species %in% c('Cutthroat trout', 'Coastal giant salamander'))\n\nggplot(data = trout_salamander_R, aes(x = length_1_mm, y = weight_g)) +\n  geom_point(aes(color = species), show.legend = TRUE) +\n  labs(x = \"Length (mm)\",\n       y = \"Weight (g)\",\n       title = \"Length vs. Weight of Cutthroat Trout and Coastal Giant Salamander Length\",\n       color = \"Species\")\n\n\n\n\nFigure 1: scatter plot length vs. weight visualized with ggplot\n\n\n\n\n\n\n\ntrout_salamander_py = trout_salamander_py[trout_salamander_py['species'].isin(['Cutthroat trout','Coastal giant salamander'])]\n\nimport matplotlib.pyplot as plt\n\ncolors = {'Cutthroat trout':'blue', 'Coastal giant salamander':'orange'}\nplt.scatter(x=trout_salamander_py.length_1_mm, y=trout_salamander_py.weight_g, \nc= trout_salamander_py.species.apply(lambda x: colors[x]))\nplt.xlabel('length (mm)')\nplt.ylabel('weight (g)')\nplt.show()\n\n\n\n\nFigure 2: scatter plot of length vs. weight visualized with matplotlib"
  },
  {
    "objectID": "code_samples/data-wrangling-with-python-and-R/index.html#histograms",
    "href": "code_samples/data-wrangling-with-python-and-R/index.html#histograms",
    "title": "Data Wrangling with Python and R",
    "section": "Histograms",
    "text": "Histograms\nThese histograms show the distribution of coastal giant salamander lenths.\n\nR Project RPython Python\n\n\n\nsalamander_R <- trout_salamander_R %>% \n  filter(species == 'Coastal giant salamander')\n\nggplot(data = salamander_R, aes(x = length_1_mm)) +\n  geom_histogram(fill = 'blue', bins = 25) +\n  labs(x = \"lenth (mm)\",\n       title = 'Distribution of Coastal Giant Salamander Length')\n\n\n\n\n\n\n\nsalamander_py = trout_salamander_py[ (trout_salamander_py['species'] == 'Coastal giant salamander') ]\n\n\nsalamander_py['length_1_mm'].hist(bins=25, color='green')\nplt.title('Distribution of Coastal Giant Salamander Length')\nplt.xlabel('length (mm)')\nplt.show()"
  },
  {
    "objectID": "code_samples/data-wrangling-with-python-and-R/index.html#bar-plots",
    "href": "code_samples/data-wrangling-with-python-and-R/index.html#bar-plots",
    "title": "Data Wrangling with Python and R",
    "section": "Bar Plots",
    "text": "Bar Plots\nThese bar plots show the averge salamander weight based on site code.\n\nR Project RPython Python\n\n\n\nsalamander_avg_weight_by_sitecode_R <- salamander_R %>% \n  group_by(sitecode) %>% \n  summarise(mean_weight = mean(weight_g, na.rm = TRUE))\n\nggplot(data = salamander_avg_weight_by_sitecode_R, aes(x = sitecode, y = mean_weight)) +\n  geom_col(fill = 'darkgreen') +\n  labs(y = 'weight (g)',\n       title = 'Average Coastal Giant Salamander Weight by Site')\n\n\n\n\n\n\n\nsalamander_avg_weight_by_sitecode_py = salamander_py.groupby('sitecode')['weight_g'].mean()\n\nsalamander_avg_weight_by_sitecode_py.plot(kind='bar', rot=0)\nplt.title('Average Coastal Giant Salamander Weight by Site')\nplt.ylabel('weight (g)')\nplt.show()"
  },
  {
    "objectID": "code_samples/data-wrangling-with-python-and-R/index.html#line-plots",
    "href": "code_samples/data-wrangling-with-python-and-R/index.html#line-plots",
    "title": "Data Wrangling with Python and R",
    "section": "Line Plots",
    "text": "Line Plots\nThese line plots show average salamander length over time.\n\nR Project RPython Python\n\n\n\nsalamander_avg_length_by_year_R <- salamander_R %>% \n  group_by(year) %>% \n  summarise(mean_length = mean(length_1_mm, na.rm = TRUE))\n\nggplot(data = salamander_avg_length_by_year_R, aes(x = year, y = mean_length)) +\n  geom_line(color = 'red') +\n  labs(x = 'year',\n       y = 'length (mm)',\n       title = 'Average Coastal Giant Salamander Length by Year')\n\n\n\n\n\n\n\nsalamander_avg_lenth_by_year_py = salamander_py.groupby('year')['length_1_mm'].mean()\n\nsalamander_avg_lenth_by_year_py.plot(x='year', y='length_1_mm', kind='line')\nplt.title('Average Coastal Giant Salamander Length by Year')\nplt.ylabel('length (mm)')\nplt.show()"
  },
  {
    "objectID": "code_samples/shiny-apps/index.html",
    "href": "code_samples/shiny-apps/index.html",
    "title": "Shiny Apps",
    "section": "",
    "text": "Snow Today Capstone Project\nVisualize 20 years of snow cover and albedo data from the Western US\n\n\n\nCalifornia Snow Depth Shiny App\nA Shiny App displaying California snow depth data\n\n\n\n\n\n\n\nCitationBibTeX citation:@online{rivers2022,\n  author = {Marie Rivers},\n  title = {Shiny {Apps}},\n  date = {2022-06-10},\n  url = {https://marierivers.github.io/code_samples/shiny-apps/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nMarie Rivers. 2022. “Shiny Apps.” June 10, 2022. https://marierivers.github.io/code_samples/shiny-apps/."
  },
  {
    "objectID": "posts/2021-11-03-how-i-made-this-visualization/index.html",
    "href": "posts/2021-11-03-how-i-made-this-visualization/index.html",
    "title": "How I made this visualization",
    "section": "",
    "text": "An assignment for my Metadata Standards, Data Modeling and Data Semantics class included using the metajam R package to download Alaskan household languages data and metadata from knb. After reviewing the metadata and reading the data into R, we were tasked with writing code to compute the percentage of Alaskan households that only speak English for the years 2009-2015 and visualizing these results…a straightforward task.\nBut…as I reviewed the numbers I thought about what I actually wanted to capture in the visualization. Did I want to focus on changes over time? There was a slight trend, but nothing significant. The data included State of Alaska Salmon and People Regions (SASAP) so I wanted to show any regional differences.\nHere’s the final visualization\nThe items below outline my data visualization process and helpful resources. Full code for data processing and visualization are included at the end."
  },
  {
    "objectID": "posts/2021-11-03-how-i-made-this-visualization/index.html#download-data-using-metajam",
    "href": "posts/2021-11-03-how-i-made-this-visualization/index.html#download-data-using-metajam",
    "title": "How I made this visualization",
    "section": "Download data using metajam",
    "text": "Download data using metajam\n\n# url to csv file\ndata_url <- \"https://knb.ecoinformatics.org/knb/d1/mn/v2/object/urn%3Auuid%3A7fc6f6db-c5ea-426a-a743-1f2edafb43b8\"\n\n# download the data and metadata to project folder\ndata_path <- metajam::download_d1_data(data_url, \"~/Documents/metajam_example\")\n\n# Read the data and metadata\nhh_list <- metajam::read_d1_files(data_path)\n\n# get the household data frame\nhh_data <- hh_list$data\n\n# get the attribute (columns) metadata\nhh_att_metadata <- hh_list$attribute_metadata"
  },
  {
    "objectID": "posts/2021-11-03-how-i-made-this-visualization/index.html#tidy-data",
    "href": "posts/2021-11-03-how-i-made-this-visualization/index.html#tidy-data",
    "title": "How I made this visualization",
    "section": "Tidy Data",
    "text": "Tidy Data\n\nCode used to process the data\nI adjusted the original data processing code as my plan for the visualization evolved\n\nhousehold_language <- read_csv(here(\"data\", \"doi_10.5063_F1CJ8BPH__household_language__csv\", \"household_language.csv\"))\n\nhh_data_english <- household_language %>%\n  filter(Year >= 2009) %>%\n  filter(total > 0) %>%\n  mutate(percent_only_english = (speak_only_english / total) * 100) %>%\n  relocate(percent_only_english, .before = german) %>%\n  mutate(SASAP.Region = fct_reorder(SASAP.Region, percent_only_english, .fun = mean)) %>%\n  group_by(SASAP.Region, Year) %>%\n  summarise(avg_percent_english = mean(percent_only_english))"
  },
  {
    "objectID": "posts/2021-11-03-how-i-made-this-visualization/index.html#ggplot",
    "href": "posts/2021-11-03-how-i-made-this-visualization/index.html#ggplot",
    "title": "How I made this visualization",
    "section": "ggplot",
    "text": "ggplot\nCode used to create the plot\n\nonly_english_plot <- ggplot(hh_data_english, aes(x = Year, y = SASAP.Region)) +\n  geom_tile(aes(fill = avg_percent_english), show.legend = TRUE) +\n  geom_text(aes(label = paste0(round(avg_percent_english, 0),\"%\")), color = \"white\", size = 3) +\n  scale_fill_gradientn(colors = c(\"antiquewhite3\", \"antiquewhite4\", \"steelblue4\", \"springgreen4\", \"indianred4\")) +\n  theme_minimal() +\n  theme(panel.grid.major = element_blank()) +\n  labs(x = \"Year\", y = NULL,\n       fill = \"Percent\",\n       title = \"Percent of Alaska Households that Only Speak English\",\n       subtitle = \"based on State of Alaska Salmon and People Region\",\n       caption = \"source: Jeanette Clark, Sharis Ochs, Derek Strong, and National Historic Geographic Information System. 2018.\\nLanguages used in Alaskan households, 1990-2015. urn:node:KNB. doi:10.5063/F1N58JPP\") +\n  theme(plot.caption = element_text(size = 8, hjust = 0),\n        plot.caption.position = \"plot\") +\n  theme(plot.title.position = \"plot\") +\n  scale_x_discrete(name = \"Year\", limits = c(2009, 2010, 2011, 2012, 2013, 2014, 2015))\n\nData citation: Jeanette Clark, Sharis Ochs, Derek Strong, and National Historic Geographic Information System. 2018. Languages used in Alaskan households, 1990-2015. urn:node:KNB. doi:10.5063/F1N58JPP.\nhttps://knb.ecoinformatics.org/knb/d1/mn/v2/object/urn%3Auuid%3A7fc6f6db-c5ea-426a-a743-1f2edafb43b8”"
  },
  {
    "objectID": "posts/2021-10-17-do-you-learn-about-the-chicken-or-the-egg-first/index.html",
    "href": "posts/2021-10-17-do-you-learn-about-the-chicken-or-the-egg-first/index.html",
    "title": "Do you learn about the chicken or the egg first",
    "section": "",
    "text": "When I decided to apply for an Environmental Data Science program I had zero coding experience. I knew that Python was a common language, but had never heard about R until learning about the UCSB Environmental Data Science program. Before starting classes I tried to learn more about this new world I’d be entering. I enrolled in an Intro to Programming class that focused on Python. In the first class we learned background info and advantages/disadvantages of C, Java, and Python, what a compiler and an interpreter are, general python syntax, and different data types (integers/floats/Booleans/strings). Over the next few weeks, we received formal definitions for value, variable, constant, expression, data strictures (arrays, queues, stacks, lists) and logical operators…All this info was useful, but we didn’t actually jump into much coding. The class dedicated more time to theoretical content than hands on coding.\nA big take away from the class wasn’t any specific coding knowledge, but rather an idea of the general mindset required to successfully learn how to code. I experienced the problem solving component of coding and the potentially frustrating cycle trying, failing, trying again, tweaking, consulting other references, and approaching from a different direction. More important than the lessons, I confirmed that I enjoy this type of work, my mind thinks in the logical way that code often requires, and I have the patience to work through debugging. An aspiring coder can’t be daunted when things don’t work the first, second, or third time. I also completed enough Codecademy R tutorials to confirm that yep, this coding thing was for me. I skimmed a few reference docs and watched a few intro R YouTube videos, but I didn’t have a great understanding of how R was used beyond tutorial world. A lot of this early context didn’t stick.\nOnce I arrived at UCSB, we started using R (and a little Python) straight away. From the beginning, our classes emphasized the importance of reproducible workflows, documenting code with comments, and version control. We spent more time typing code than just looking at code. Our classes include the right amount of repetition to retain essential content and appreciate its use in a variety of contexts.\nThere is no “right” way to being to learn how to code. Starting is the most important part. Don’t try to gain some arbitrary minimum knowledge base before starting. The first time you use a new package or function, you’re not going to remember all the details. The first times you read reference books, package documentations or blog posts they won’t make much sense. The more you learn, the more your capacity to ingest new information will increase. You will never know everything, but you will gain a larger understanding of what is possible (and where to go to learn how to do it). AND…you’ll start to understand R jokes. The best way to learn is by doing. This grad program has a good balance of formal instruction and, more importantly, plenty of opportunities to figure things out for yourself.\n\n\n\nCitationBibTeX citation:@online{rivers2021,\n  author = {Marie Rivers},\n  title = {Do You Learn about the Chicken or the Egg First},\n  date = {2021-10-17},\n  url = {https://marierivers.github.io/posts/2021-10-17-do-you-learn-about-the-chicken-or-the-egg-first/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nMarie Rivers. 2021. “Do You Learn about the Chicken or the Egg\nFirst.” October 17, 2021. https://marierivers.github.io/posts/2021-10-17-do-you-learn-about-the-chicken-or-the-egg-first/."
  },
  {
    "objectID": "posts/2022-06-07-the-importance-of-meaningfully-open-snow-data/index.html",
    "href": "posts/2022-06-07-the-importance-of-meaningfully-open-snow-data/index.html",
    "title": "The Importance of Meaningfully Open Snow Data",
    "section": "",
    "text": "For the past 6 months I’ve worked with the Snow Today capstone group. Our initial goal was to develop web-based visualizations of snow cover and albedo data. We worked with output files from a model called SPIReS (Snow Property Inversion from Remote Sensing) that was developed by researchers with the UCSB Earth Research Institute (Bair 2021). SPIReS uses MODIS satellite imagery to estimate snow cover and albedo. MODIS (Moderate Resolution Imaging Spectroradiometer) is a NASA satellite instrument that has collected daily data on Earth’s land, oceans, and atmosphere since the year 2000. MODIS also collects data on the cryosphere, the frozen parts of earth covered by snow and ice. The Snow Today group quickly began brainstorming visualization and website platform strategies, but soon realized we had to take a step back. We had no idea how to even open the datasets.\nThe datasets used in the Snow Today project could be improved by incorporating FAIR data principles (Wilkinson 2016). FAIR stands for Findable, Accessible, Interoperable, and Reusable. The metadata associated with the snow cover and albedo datasets was hard to find and once it was found, it was hard to interpret. The metadata told us the map projection of the data, but this information wasn’t attached in a standard format that could be recognized by common mapping software and spatial packages. This meant that we couldn’t plot the data on a map in the correct location. This also meant that researchers or water managers faced a significant barrier if trying to use the dataset to learn about their local water supply.\nAs we worked through the challenges of the snow cover and albedo datasets, our goals shifted towards creating an open-source workflow to make the data more meaningfully open. While the data used for our project is available online for anyone to download (if you know where to find it), insights can be hard or near impossible to gather without specialized training. After many conversations with the dataset creators, we were able to develop a workflow around the metadata challenges. To see the final product of the Snow Today capstone group, including tutorials to guide others through the steps of repeating our workflow, visit our interactive web app\nQuantifying snow cover area is important because much of the world’s population, from the Western US to High Mountain Asia, relies on winter snowpacks for year-round drinking water, but…\n\nWhy do we care about albedo?\nAlbedo is a measure of how much solar energy is reflected from a surface. Albedo has important climate implications because it determines how much radiation the planet absorbs. Dark surfaces like soil and vegetation have low albedo values while lighter surfaces such as snow have higher albedo values. Dirty snow absorbs more solar radiation and therefore melts faster than clean snow. Since spring snowmelt contributes to drinking water reservoirs in drier months, earlier snowmelt can leave less water in the summer when it’s needed most. A layer of fresh snow increases albedo for that area, which can result in local cooling. When snow melts, it reveals darker surfaces with lower albedo which increases local temperatures and encourages more melting in a feedback loop where the surface absorbs more solar radiation. \n\n\n\n\nCitationBibTeX citation:@online{rivers2022,\n  author = {Marie Rivers},\n  title = {The {Importance} of {Meaningfully} {Open} {Snow} {Data}},\n  date = {2022-06-07},\n  url = {https://marierivers.github.io/posts/2022-06-07-the-importance-of-meaningfully-open-snow-data/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nMarie Rivers. 2022. “The Importance of Meaningfully Open Snow\nData.” June 7, 2022. https://marierivers.github.io/posts/2022-06-07-the-importance-of-meaningfully-open-snow-data/."
  },
  {
    "objectID": "posts/2021-08-18-a-new-perspective-on-data/index.html",
    "href": "posts/2021-08-18-a-new-perspective-on-data/index.html",
    "title": "A New Perspective on Data",
    "section": "",
    "text": "Throughout my career as an environmental engineer, the bulk of my experience with data had been with excel spreadsheets and modeling software. Spreadsheets were my primary tool to complete bulk calculations and create graphs. I’ve collected many tips and tricks and consider my excel skills to be very strong. When I want to try something I’ve never tried before, I have a reasonably good intuition for if it’s something excel is capable of and how to find the solution using help features or internet searches.\nDuring college I used excel for everything inside and outside of class. My classmates and I would joke about using solver to answer everything from mundane domestic questions to meaning of life type queries. While I haven’t used solver since finding the optimal 2012 US Olympic gymnastics team based on various scoring scenarios, spreadsheets are still prevalent in my life:\n\n– Planning a group vacation…shared Google sheet!\n– Outlining an epic fantasy novel…no need for scrivener\n– Any major life decision…better back that choice up with excel\n\nAnd my works days were full of engineering calcs, budgets, cost estimates, asset management analyses, and graphs. From summary statistics, if statements, and well placed $ signs to conditional formatting and pivot tables, I thought my data storage, analysis, and visualization skills were top of their game…until I discovered the Masters of Environmental Data Science (MEDS) program at UC Santa Barbara’s Bren school of Environmental Science & Management. I started to get a hunch that there were other alternatives when working with data.\nSince starting the MEDS program, I’ve begun to expand my data storage, analysis, and visualization outlook. I was very impressed with an article we read in class Data Organization in Spreadsheet by Karl W. Broman & Kara H. Woo. (Karl W. Broman & Kara H. Woo (2018) Data Organization in Spreadsheets, The American Statistician, 72:1, 2-10, DOI: 10.1080/00031305.2017.1375989). This paper articulates many of the logistical challenges I’ve faced with data and presented useful recommendations. I’ve worked with or created spreadsheets where each of the basic spreadsheet principles were not used.\nI have received and passed along complicated spreadsheets with equations linked to cells in tabs throughout the workbook. While my coworkers and I attempted to used consistent file names (even if only consistent with their own files) these efforts often fall short when unexpected complexities arise or deadlines get close. When you are desperately trying to get a deliverable out the door, intermediate file names are the least of your priorities. Challenges associated with combining or separating dates and addresses and entering zip codes or ID numbers that begin with 0 were expected parts of data manipulation. I’ve received data with no documentation for column headers, acronyms, or units; from these experiences I began to use perhaps overly lengthy variable names, often with a top row merged over several variables.\nThe concept of Tidy data was completely knew to me. At first, this data structure seemed drawn-out and a little redundant, but as we’ve worked with more data in class I now see the advantages. Taking the time to use functions such as pivot_wider and pivot_longer to get data in Tide format ultimately gives your subsequent analysis more flexibility and ease.\nAs I continue with the MEDS program, I’m looking forward to gaining the same intuition for R and Python that I have with excel. I’m sure this journey will be frustrating and time consuming, but know that learning new ways to store, analyze, and visual data will be rewarding!\nP.S. I never fully appreciated CSV files\n\n\n\nCitationBibTeX citation:@online{rivers2012,\n  author = {Marie Rivers},\n  title = {A {New} {Perspective} on {Data}},\n  date = {2012-08-18},\n  url = {https://marierivers.github.io/posts/2021-08-18-a-new-perspective-on-data/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nMarie Rivers. 2012. “A New Perspective on Data.” August 18,\n2012. https://marierivers.github.io/posts/2021-08-18-a-new-perspective-on-data/."
  },
  {
    "objectID": "posts/2022-02-08-how-to-make-a-dumbbell-schedule-with-r/index.html",
    "href": "posts/2022-02-08-how-to-make-a-dumbbell-schedule-with-r/index.html",
    "title": "How to make a dumbbell schedule with R",
    "section": "",
    "text": "Here is a tutorial to create a custom dot plot / dumbbell plot schedule with R and geom_dumbbell and geom_segment. With this template you can also color items by categories such as deliverable, status or quarter.\nOriginal inspiration: and guidance on using geom_dumbbell.\n\nRead and view the data\nI started from an excel sheet.\n\nschedule <- read_excel(here(\"posts\", \"2022-02-08-how-to-make-a-dumbbell-schedule-with-r\", \"schedule.xlsx\"), sheet = \"Sheet1\")\n\n\nkable(schedule) %>% \n  kable_paper(full_width = TRUE) %>% \n  row_spec(0, bold = T) %>% \n  kable_styling(latex_options = \"HOLD_position\")\n\n\n\n \n  \n    milestone \n    deliverable \n    start_date \n    due_date \n    status \n    quarter \n  \n \n\n  \n    Draft Feasibility Study \n    Feasibility Study \n    2022-01-12 \n    2022-02-07 \n    complete \n    Q1 \n  \n  \n    Final Feasibility Study \n    Feasibility Study \n    2022-02-12 \n    2022-03-11 \n    in progress \n    Q1 \n  \n  \n    Draft Design Drawings \n    Design Drawings \n    2022-01-24 \n    2022-03-18 \n    in progress \n    Q1 \n  \n  \n    Final Design Drawings \n    Design Drawings \n    2022-04-12 \n    2022-05-20 \n    not started \n    Q2 \n  \n  \n    Draft Technical Documentation \n    Technical Documentation \n    2022-03-01 \n    2022-03-18 \n    not started \n    Q1 \n  \n  \n    Final Technical Documentation \n    Technical Documentation \n    2022-03-28 \n    2022-04-22 \n    not started \n    Q2 \n  \n  \n    Stakeholder Presentation \n    Presentation \n    NA \n    2022-03-25 \n    not started \n    Q1 \n  \n  \n    Draft Web Application \n    Web Application \n    2022-02-18 \n    2022-03-16 \n    not started \n    Q1 \n  \n  \n    Final Web Application \n    Web Application \n    2022-03-28 \n    2022-05-27 \n    not started \n    Q2 \n  \n  \n    Client Presentation \n    Presentation \n    NA \n    2022-06-03 \n    not started \n    Q2 \n  \n\n\n\n\n\n\n\nClean the data\nPay attention to date formats with class(schedule$start_date). Cleaning steps created a field for month abbreviation + day of month to use as labels. The deliverable category was reordered based on due date using fct_reorder.\n\nschedule <- schedule %>% \n  mutate(start_month_num = month(start_date)) %>% \n  mutate(end_month_num = month(due_date)) %>% \n  mutate(start_month_name = case_when(\n    start_month_num == 1 ~ \"Jan\",\n    start_month_num == 2  ~ \"Feb\",\n    start_month_num == 3 ~ \"Mar\",\n    start_month_num == 4 ~ \"Apr\",\n    start_month_num == 5 ~ \"May\",\n    start_month_num == 6 ~ \"Jun\",\n  )) %>% \n  mutate(end_month_name = case_when(\n    end_month_num == 1 ~ \"Jan\",\n    end_month_num == 2 ~ \"Feb\",\n    end_month_num == 3 ~ \"Mar\",\n    end_month_num == 4 ~ \"Apr\",\n    end_month_num == 5 ~ \"May\",\n    end_month_num == 6 ~ \"Jun\",\n  )) %>% \n  mutate(start_label = paste(start_month_name, day(start_date))) %>% \n  mutate(end_label = paste(end_month_name, day(due_date))) %>% \n  mutate(milestone = as_factor(milestone)) %>% \n  mutate(milestone = fct_reorder(milestone, as.numeric(due_date), .desc = TRUE))\n\nThis step was used for the bounds of the rectangle highlighting the second quarter.\n\nQ2 <- schedule %>% \n  filter(quarter == \"Q2\")\nxmin_Q2 <- as.POSIXct(min(Q2$start_date, na.rm = TRUE))\nxmax_Q2 <- as.POSIXct(max(Q2$due_date, na.rm = TRUE))\n\nThis step was used to set the x-axis limits when using scale_x_datetime.\n\nmin <- as.POSIXct(\"2022-1-1\")\nmax <- as.POSIXct(\"2022-6-15\")\n\nI originally used geom_dumbbell for the start and end dots, but using geom_point twice gave more flexibility for colors and made it easy add transparency to the start dots. The geom_dumbbell code is include for reference.\n\ntime_plot <- ggplot(data = schedule, aes(y = milestone)) +\n  geom_rect(aes(xmin = xmin_Q2, ymin = -Inf,\n                xmax = xmax_Q2, ymax = Inf),\n                fill = \"grey80\", alpha = 0.5) +\n  # create a thick line between x and xend instead of using default provided by geom_dumbbell\n  geom_segment(aes(x = start_date, xend = due_date, y = milestone, yend = milestone), \n               color = \"grey50\", \n               size = 1.5) +\n  # geom_dumbbell(color = \"grey80\", size_x = 5, size_xend = 5,\n  #               colour_x = \"blue\", colour_xend = \"red\") +\n  geom_point(data = schedule, aes(x = start_date, y = milestone,\n                              color = deliverable), size = 5, alpha = 0.5,\n             show.legend = FALSE) +\n  # scale_colour_paletteer_d(\"Redmonder::qMSOPap\") +\n  # scale_colour_paletteer_d(\"Redmonder::qMSOPap\") +\n  # scale_colour_paletteer_d(\"Redmonder::qMSOMed\") +\n  # scale_colour_paletteer_d(\"palettetown::pelipper\") +\n  scale_colour_manual(values = c(\"#007a76\", \"#b7245c\", \"#ca7f0e\", \"#0d4fbd\", \"#785ceb\")) + # manually specify colors or use an existing palette\n  geom_point(data = schedule, aes(x = due_date, y = milestone,\n                              color = deliverable), size = 5,\n             show.legend = FALSE) +\n  labs(x = NULL, y = NULL,\n       title = \"Milestone Timeline\") +\n  geom_text(color = \"black\", size = 3, hjust = 1.5,\n            aes(x = start_date, label = start_label)) +\n  geom_text(color = \"black\", size = 3, hjust = -0.5,\n            aes(x = due_date, label = end_label)) +\n  theme_minimal() +\n  theme(panel.grid = element_blank()) +\n  theme(plot.title.position = \"plot\") +\n  scale_x_datetime(limits = c(min, max)) +\n  annotate(\"text\", x = as.POSIXct(\"2022-5-1\"), y = schedule$milestone[1], label = \"Q2\", color = \"black\", size = 5)\ntime_plot\n\nWarning: Removed 2 rows containing missing values (geom_segment).\n\n\nWarning: Removed 2 rows containing missing values (geom_point).\n\n\nWarning: Removed 2 rows containing missing values (geom_text).\n\n\n\n\n\n\n\n\n\nCitationBibTeX citation:@online{rivers2022,\n  author = {Marie Rivers},\n  title = {How to Make a Dumbbell Schedule with {R}},\n  date = {2022-02-08},\n  url = {https://marierivers.github.io/posts/2022-02-08-how-to-make-a-dumbbell-schedule-with-r/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nMarie Rivers. 2022. “How to Make a Dumbbell Schedule with\nR.” February 8, 2022. https://marierivers.github.io/posts/2022-02-08-how-to-make-a-dumbbell-schedule-with-r/."
  },
  {
    "objectID": "posts/2021-12-02-environmental-quality-population-change/index.html",
    "href": "posts/2021-12-02-environmental-quality-population-change/index.html",
    "title": "Does Environmental Quality Influence Where People Live?",
    "section": "",
    "text": "This post discusses a statistical analysis used to answer the question: Does environmental quality influence where people live in the United States?"
  },
  {
    "objectID": "posts/2021-12-02-environmental-quality-population-change/index.html#background",
    "href": "posts/2021-12-02-environmental-quality-population-change/index.html#background",
    "title": "Does Environmental Quality Influence Where People Live?",
    "section": "Background",
    "text": "Background\nThe Environmental Quality Index (EQI), developed by the U.S. Environmental Protection Agency (EPA) provides a county level snapshop of environmental conditions throughout the country. EPA first released EQIs for the period 2000-2005 and updated these indexes for 2006-2010. This statistical evaluation focuses on the 2006-2010 EQI. The purpose of the EQI is to use (1) as an indicator of ambient conditions/exposure in environmental health and modeling and (2) as a covariate to adjust for ambient conditions in environmental models (EPA 2020). Previous studies have used the EQI to evaluate relationships between environmental quality and public health outcomes such as cancer incidence, asthma, obesity, and infant mortality.\nThe EQI is developed from five domains each with identified environmental constructs as shown in Table 1. Each county has an overall environmental index and a domain specific index. Indexes were also stratified by rural-urban continuum codes (RUCCs) for counties classified as metropolitan urbanized, non-metro urbanized, less urbanized, and thinly populated.\n\n\n\n\n\n\n\nEQI Environmental Domains and Constructs\n \n  \n    Domain \n    Constructs \n  \n \n\n  \n    air \n    criteria air pollutants and hazardous air pollutants \n  \n  \n    water \n    overall water quality, general water contamination, domestic use, atmospheric deposition, drought, chemical contamination, and drinking water quality \n  \n  \n    land \n    agriculture, pesticides, facilities, radon, and mining activity \n  \n  \n    built \n    roads, highway/road safety, commuting behavior, housing environment, walkability, and green space \n  \n  \n    sociodemographic \n    crime, socioeconomic, political character, and creative class representation \n  \n\n\n\n\n\n\nLimitations\nWhile the EQI can identify counties with higher environmental burdens, it may not identify environmental injustices at the local community level. The EQI cannot quantify environmental exposure for individuals and reflects only outside environmental conditions, not indoor conditions. The EQI can be used to identify locations for future research, but is not intended for regulatory purposes or as a diagnostics tool. Due to changes in methodology and datasets, the 2000-2005 and 2006-2010 EQIs should not be directly compared."
  },
  {
    "objectID": "posts/2021-12-02-environmental-quality-population-change/index.html#the-data",
    "href": "posts/2021-12-02-environmental-quality-population-change/index.html#the-data",
    "title": "Does Environmental Quality Influence Where People Live?",
    "section": "The Data",
    "text": "The Data\n\nEnvironmental Quality Index\nTo develop the EQI, variables were identified from available data to represent each environmental domain and assessed for collinearity so redundant variables could be excluded. Variables were standardized based on geographic space or on a per capita rate, as appropriate and transformations such as log-transformations were performed as needed based on the normality of each variable. Data gaps were evaluated to distinguish between missing data and meaningful zeros. Where applicable, spatial kriging was used to interpolate values when data was not available for all counties. Principal component analysis was used to aggregate variables into domain specific indexes. The domain indexes were then aggregated into overall indexes for each county. A result of this method is that each domain does not equally influence the overall EQI value for a given county. The EQI is developed to be normally distributed with mean=0 and standard deviation=1. Higher EQI values correspond with worse environmental quality. Lower (more negative) EQI values correspond with better environmental quality.\n\n\nCensus Population Data\nCounty level population data was obtained from the U.S. Census Bureau’s county intercensal datasets for 2000-2010. Percent population change was calculated for 2006-2010 then winsorized to remove outliers above the 99.9th percentile (ie. counties with population change above 28.5%)."
  },
  {
    "objectID": "posts/2021-12-02-environmental-quality-population-change/index.html#statistical-analysis",
    "href": "posts/2021-12-02-environmental-quality-population-change/index.html#statistical-analysis",
    "title": "Does Environmental Quality Influence Where People Live?",
    "section": "Statistical Analysis",
    "text": "Statistical Analysis\nKalawao County, Hawaii had the lowest EQI value (highest environmental quality) and greatest decrease in population (-17.4%). Falls Church, Virginia had the highest EQI (lowest environmental quality) and a population change of 15.4% which falls is the 99th percentile. The summary statistics in Table 2 suggest that population growth is correlated with worse environmental characteristics.\n\n\n\n\nEQI summary statistics for counties based on population change between 2006 and 2010\n \n  \n    Population Change \n    Min \n    Max \n    Mean \n    Standard Deviation \n    Variance \n    Number of Counties \n  \n \n\n  \n    negative \n    -5.88 \n    2.59 \n    -0.24 \n    0.93 \n    0.87 \n    1088 \n  \n  \n    positive \n    -5.05 \n    2.85 \n    0.13 \n    1.01 \n    1.02 \n    2052 \n  \n  \n    all counties \n    -5.88 \n    2.85 \n    0.00 \n    1.00 \n    1.00 \n    3140 \n  \n\n\n\n\n\n\nHypothesis Testing\nA different means test was completed to determine if mean environmental quality was statistically different for counties that experienced positive vs. negative population change between 2006-2010. The figure below shows a histogram of EQI values for counties with negative and positive population change.\n\n\n\n\n\nnull hypothesis: There is no difference in mean EQI for counties with positive and negative population change.\n\\[H_{0}: \\mu_{posPopChange} - \\mu_{negPopChange} = 0\\]\nalternative hypothesis: There is a difference in mean EQI for counties with positive and negative population change.\n\\[H_{A}: \\mu_{posPopChange} - \\mu_{negPopChange} \\neq 0\\]\n\n\n\n\n\n\n\\[\\text{point estimate} = \\mu_{posPopChange} - \\mu_{negPopChange} =0.131 - -0.245 = 0.376\\]\nThe standard error for the difference in means is:\n\\[SE = \\sqrt{\\frac{s_1^2}{n_1} + \\frac{s^2_2}{n_2}} = \\sqrt{\\frac{1.009^2}{2052} + \\frac{0.933^2}{1088}} = 0.036\\]\nThe z-score for hypothesis testing is:\n\\[z = \\frac{\\text{point estimate - null}}{SE} = \\frac{0.376 - 0}{0.036} = 10.443\\]\n\n\n\nThe p-value, the probability of getting a point estimate at least as extreme as calculated if the null hypothesis were true, is:\n\\[p\\text {-value }=\\operatorname{Pr}(Z<-|z| \\text { or } Z>|z|)=2 * \\operatorname{Pr}(Z>|z|) = 1.5809578\\times 10^{-25}\\]\n\n\n\nSince the p-value is < 0.001 we reject the null that there is no difference in EQI for counties with positive population change versus negative population change. There is a statistically significant difference (at the 0.1% significance level) in EQI across the two population change groups. The 95% confidence interval ranges from 0.31 to 0.45. This means that there is a 95% chance that this interval includes the true difference in mean EQI between counties with positive and negative percent population change.\n\n\nLinear Regression\nLinear regression was used to model the relationship between population change and environmental quality using the overall EQI value and each domain specific EQI to determine if a particular domain was a stronger predictor of population change.\n\\[\\text{percent population change}_i=\\beta_{0}+\\beta_{1} \\cdot EQI_i + \\varepsilon_i\\]\n\n\n\n\n\nFirst, hypothesis testing was used to test whether the slope coefficient for the percent population change rate is equal to zero or not.\nnull hypothesis: The slope coefficient is equal to zero\n\\[H_{0}: \\beta_{1} = 0\\] alternative hypothesis: The slope coefficient is NOT equal to zero\n\\[H_{A}: \\beta_{1} \\neq 0\\] ::: {.cell}\n:::\nThe point estimate, \\(\\beta_1\\) = 0.991 and the standard error, SE = 0.077.\n\\[z = \\frac{\\text{point estimate - null}}{SE} = \\frac{{0.991 - 0}}{0.077} = 12.817\\]\n\\[p\\text {-value }=\\operatorname{Pr}(Z<-|z| \\text { or } Z>|z|)=2 * \\operatorname{Pr}(Z>|z|) = 1.0849376\\times 10^{-36}\\]\n\n\n\nSince the p-value for the slope coefficient was < 0.001, we reject the null hypothesis that EQI has no influence on population change at the 0.1% level. There is a statistically significant relationship between EQI and percent population change and the coefficient is significantly different from zero. Based on value of \\(\\beta_1\\), for each one unit increase in EQI, the percent population change increases by 0.991. The 95% confidence interval for the slope coefficient ranges from 0.84 to 1.143. This means that there is a 95% chance that this interval includes the true county level rate of change for percent population change for each one unit change in EQI.\n\n\nDomain Specific Linear Models\n\\[\\text{percent population change}_i=\\beta_{0,domain}+\\beta_{1,domain} \\cdot EQI_{i,domain} + \\varepsilon_i\\] ::: {.cell}\n:::\n\n\n\n\n\n\n\n\nWhile a partly manual method was used above, statistical functions in R were used to test for the significance of domain models. Table 3 presents coefficients for each domain specific model. The numbers in [brackets] are the 95% confidence intervals for each estimated coefficient.\n\n\n\n\nSummary of EQI Domain Slope Coefficients\n \n  \n    coefficient \n    overall EQI \n    air model \n    water model \n    land model \n    built model \n    sociodem model \n  \n \n\n  \n    Intercept \n    1.868 *** \n    1.869 *** \n    1.868 *** \n    1.868 *** \n    1.867 *** \n    1.868 *** \n  \n  \n     \n    [1.716, 2.019] \n    [1.717, 2.020] \n    [1.714, 2.023] \n    [1.713, 2.024] \n    [1.713, 2.021] \n    [1.718, 2.019] \n  \n  \n    EQI \n    0.991 *** \n     \n     \n     \n     \n     \n  \n  \n     \n    [0.840, 1.143] \n     \n     \n     \n     \n     \n  \n  \n    air EQI \n     \n    0.998 *** \n     \n     \n     \n     \n  \n  \n     \n     \n    [0.847, 1.150] \n     \n     \n     \n     \n  \n  \n    water EQI \n     \n     \n    0.485 *** \n     \n     \n     \n  \n  \n     \n     \n     \n    [0.331, 0.640] \n     \n     \n     \n  \n  \n    land EQI \n     \n     \n     \n    -0.314 *** \n     \n     \n  \n  \n     \n     \n     \n     \n    [-0.469, -0.158] \n     \n     \n  \n  \n    built EQI \n     \n     \n     \n     \n    0.635 *** \n     \n  \n  \n     \n     \n     \n     \n     \n    [0.481, 0.790] \n     \n  \n  \n    sociodem EQI \n     \n     \n     \n     \n     \n    1.068 *** \n  \n  \n     \n     \n     \n     \n     \n     \n    [0.917, 1.219] \n  \n  \n    n \n    3140 \n    3140 \n    3140 \n    3140 \n    3140 \n    3140 \n  \n  \n    R2 \n    0.050 \n    0.051 \n    0.012 \n    0.005 \n    0.020 \n    0.058 \n  \n\n\n\n\n\nThe figure below provides a visual comparison of each model result. The bold portion of the line represents the 90% confidence interval and the full line represents the 95% confidence interval for each estimate.\n\n\n\n\n\nThe p-value on the slope coefficient was < 0.001 for all domain specific linear models which indicates a statistically significant relationship at the 0.01% level. Based on the \\(R^2\\) values and slope coefficients, the air and sociodemographic domains account for most of the overall relationship between population change and EQI. All domains except land are positively correlated with population change. Since higher EQI values indicate poorer environmental quality, these models show that population increased more in counties with worse environmental conditions. For a one unit increase in sociodemogrpahic EQI, the percent population change increases by 1.068. For a one unit increase in air EQI, the percent population change increases by 0.998. The \\(R^2\\) terms represent the variance in percent population change that can be explained by EQI. For the overall EQI value, 5% of the variance in percent population change is explained by environmental conditions. The sociodemographic EQI explains 5.8% of the variance in population change while the air EQI explains 5.1%."
  },
  {
    "objectID": "posts/2021-12-02-environmental-quality-population-change/index.html#conclusions",
    "href": "posts/2021-12-02-environmental-quality-population-change/index.html#conclusions",
    "title": "Does Environmental Quality Influence Where People Live?",
    "section": "Conclusions",
    "text": "Conclusions\nThe identified relationships between population change and environmental quality are noteworthy for their public health and environmental justice implications. Positive population trends in areas with worse environmental conditions could result in increased incidences of cancer, asthma, obesity, and infant mortality. While this project did not evaluate economic variables, locations with higher environmental quality could also have higher living costs which drive people to move to more affordable places. If economic factors contribute to population growth in counties with poor environmental quality, then this could negatively affect the health of vulnerable populations and perpetuate social inequalities. Further analysis could evaluate trends in mean household income to determine if there is growing income inequality between counties with better and worse environmental quality. Economic variables and other factors influencing demographic shifts from rural areas to cities may be stronger predictors of population change than environmental quality.\nData availability:\nEPA Datasets and files from the EQI county data from 2006-2010\nU.S. Census Bureau County Intercensal Tables: 2000-2010"
  },
  {
    "objectID": "posts/2021-12-02-environmental-quality-population-change/index.html#references",
    "href": "posts/2021-12-02-environmental-quality-population-change/index.html#references",
    "title": "Does Environmental Quality Influence Where People Live?",
    "section": "References",
    "text": "References\n\nU.S. EPA. Environmental Quality Index - Technical Report (2006-2010) (Final, 2020). U.S. Environmental Protection Agency, Washington, DC, EPA/600/R-20/367, 2020.\nU.S. Census Bureau. County Intercensal Datasets: 2000-2010. https://www.census.gov/data/datasets/time-series/demo/popest/intercensal-2000-2010-counties.html"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Marie T. Rivers",
    "section": "",
    "text": "this website is under construction xxxx\n\n  \n    \n\n    \n  \n      LinkedIn\n  \n  \n      Twitter\n  \n  \n      GitHub\n  \n\n  \n  \n\nEducation\n\n\nMaster of Environmental Data Science\n\nBren School of Environmental Science & Management University of California, Santa Barbara (2022)\n\nMS in Environmental Engineering\n\nDepartment of Civil and Environmental Engineering University of Massachusetts, Amherst (2011)\n\nBS in Environmental Engineering\n\nDepartment of Civil and Environmental Engineering University of Delaware (2009)\n\n\n\n\n\nMarie is a graduate of UC Santa Barbara’s Bren School of Environmental Science and Management inaugural environmental data science cohort. She is also a registered professional civil water resources engineer with 10 years of experience modeling and designing municipal drinking water distribution systems. During this time, she observed the growing need for reproducible data analysis and visualization skills to solve current and emerging environmental problems. Marie is interested in using geospatial analyses to understand interactions between human and natural systems so that these insights can advance renewable energy technologies that support climate change goals."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "presentations/wind_resource_presentation.html#data-workflow",
    "href": "presentations/wind_resource_presentation.html#data-workflow",
    "title": "Wind Resource Temporal Variability",
    "section": "Data Workflow",
    "text": "Data Workflow\n\n\n\nExplored diurnal and monthly variability of wind resources at Mount Washington in NH\nUsed the NREL Wind Integration National Dataset (WIND) Toolkit\nAccessed data with the h5pyd Python package and NREL Highly Scalable Data Service (HSDS)\nSubset the for the windspeed_100m, dataset and year 2012\nConverted to pandas dataframe\nAggregated data by hourly and monthly groupings to calculate mean, standard deviation, and quartiles"
  },
  {
    "objectID": "presentations/wind_resource_presentation.html#visualization-of-full-time-series",
    "href": "presentations/wind_resource_presentation.html#visualization-of-full-time-series",
    "title": "Wind Resource Temporal Variability",
    "section": "Visualization of full time series",
    "text": "Visualization of full time series\n\ngraphcode\n\n\n\n\n\n                        \n                                            \n\n\n\n\n\nfig = go.Figure([\n    go.Scatter(x = windspeed_100m_df.index, y = windspeed_100m_df['windspeed_100m'], \n              mode = 'lines', legendrank = 1, \n              name = 'hourly', line=dict(color='blue', width=0.75)),\n    go.Scatter(x = moving_averages_24hr.index, y = moving_averages_24hr['windspeed_100m'], \n              mode = 'lines', legendrank = 1,\n              name = '24 hour avg', line=dict(color='green', width=1), visible='legendonly'),\n    go.Scatter(x = moving_averages_10day.index, y = moving_averages_10day['windspeed_100m'], \n              mode = 'lines', legendrank = 1, \n              name = '10 day avg', line=dict(color='red', width=1), visible='legendonly'),\n    go.Scatter(x = moving_averages_30day.index, y = moving_averages_30day['windspeed_100m'], \n              mode = 'lines', legendrank = 1, \n              name = '30 day avg', line=dict(color='yellow', width=3), visible='legendonly')\n])\n\nfig.update_layout(\n    margin=dict(l=20, r=20, t=30, b=20),\n    paper_bgcolor=\"#FFFFFF\",\n    plot_bgcolor='#f5f5f5',\n    yaxis=dict(\n        title_text=\"windspeed (m/s)\",\n        titlefont=dict(size=16)),\n    title={\n        'text': \"Hourly Wind Speed\",\n        'y':0.99,\n        'x':0.5,\n        'xanchor': 'center',\n        'yanchor': 'top'}\n)"
  },
  {
    "objectID": "presentations/wind_resource_presentation.html#diurnal-and-monthly-variability",
    "href": "presentations/wind_resource_presentation.html#diurnal-and-monthly-variability",
    "title": "Wind Resource Temporal Variability",
    "section": "Diurnal and Monthly Variability",
    "text": "Diurnal and Monthly Variability\n\nBoth wind speed and electricity demands fluctuate throughout the day and seasonally\nWind and electricity patterns may not match\nUnlike water resources, electricity can be challenging to store during low demands\nWhen selecting sites for utility scale wind power it is important to have adequate wind speeds at the same time as peak electricity demands\nStatistics such as the interquartile range and standard deviation can help quantify the spread of data"
  },
  {
    "objectID": "presentations/wind_resource_presentation.html#diurnal-and-monthly-variability-1",
    "href": "presentations/wind_resource_presentation.html#diurnal-and-monthly-variability-1",
    "title": "Wind Resource Temporal Variability",
    "section": "Diurnal and Monthly Variability",
    "text": "Diurnal and Monthly Variability\n\n\n\nHourly average\n\ngraphcode\n\n\n\n\n\n                        \n                                            \n\n\n\n\n\nfig = go.Figure([\n    go.Scatter(name = 'mean', y = hourly_avg['mean'], x = hourly_avg['hour'], mode = 'lines',\n              line = dict(color = \"blue\", width = 4),\n              error_y = dict(type = 'data', array = hourly_avg['std'], visible = True)),\n    go.Scatter(\n        name = 'IQR 75', y = hourly_avg['quantile75'], x = hourly_avg['hour'],\n        mode='lines',\n        marker=dict(color=\"#444\"),\n        line=dict(width=0),\n        #legendgroup = 'IQR',\n        showlegend = False\n    ),\n    # Create IQR 25 fill color\n    go.Scatter(\n        name='IQR', y = hourly_avg['quantile25'], x = hourly_avg['hour'],\n        marker=dict(color=\"#444\"),\n        line=dict(width=0),\n        mode='lines',\n        fillcolor='rgba(68, 68, 68, 0.3)',\n        fill='tonexty', # fill to next y\n        legendgroup = 'IQR',\n        showlegend = True\n    )\n])\nfig.update_layout(\n    xaxis=dict(\n        title_text=\"hour (UTC)\",\n        titlefont=dict(size=16),\n        dtick = 2),\n    yaxis=dict(\n        title_text=\"windspeed (m/s)\",\n        titlefont=dict(size=16)),\n    title={\n        'text': \"Average Hourly Wind Speed for the Year 2012\",\n        'y':0.99,\n        'x':0.5,\n        'xanchor': 'center',\n        'yanchor': 'top'},\n    margin=dict(l=20, r=20, t=30, b=20),\n    paper_bgcolor=\"#FFFFFF\",\n    plot_bgcolor='#f5f5f5'\n)\n\n\n\n\n\nMonthly average\n\ngraphcode\n\n\n\n\n\n                        \n                                            \n\n\n\n\n\nfig = go.Figure([\n    go.Scatter(name = 'mean', y = monthly_avg['mean'], x = monthly_avg['month'], \n              mode = 'lines', line = dict(color = \"blue\", width = 4),\n              error_y = dict(type = 'data', array = monthly_avg['std'], visible = True)),\n    go.Scatter(\n        name = 'IQR 75', y = monthly_avg['quantile75'], x = monthly_avg['month'],\n        mode='lines', marker=dict(color=\"#444\"), line=dict(width=0),\n        showlegend = False\n    ),\n\n    # Create IQR 25 fill color\n    go.Scatter(\n        name='IQR', y = monthly_avg['quantile25'], x = monthly_avg['month'],\n        marker=dict(color=\"#444\"), line=dict(width=0), mode='lines',\n        fillcolor='rgba(68, 68, 68, 0.3)',\n        fill='tonexty', # fill to next y\n        legendgroup = 'IQR',\n        showlegend = True)\n])\nfig.update_layout(\n    xaxis=dict(\n        title_text=\"month\",\n        titlefont=dict(size=16),\n        dtick = 1),\n    yaxis=dict(\n        title_text=\"windspeed (m/s)\",\n        titlefont=dict(size=16)),\n    title={\n        'text': \"Average Monthly Wind Speed for the Year 2012\",\n        'y':0.99,\n        'x':0.5,\n        'xanchor': 'center',\n        'yanchor': 'top'},\n    margin=dict(l=20, r=20, t=30, b=20),\n    paper_bgcolor=\"#FFFFFF\",\n    plot_bgcolor='#f5f5f5'\n)"
  },
  {
    "objectID": "presentations/wind_resource_presentation.html#diurnal-and-monthly-variability-2",
    "href": "presentations/wind_resource_presentation.html#diurnal-and-monthly-variability-2",
    "title": "Wind Resource Temporal Variability",
    "section": "Diurnal and Monthly Variability",
    "text": "Diurnal and Monthly Variability\n\ngraphcode\n\n\n\n\n\n                        \n                                            \n\n\n\n\n\nfig = go.Figure([\n    go.Scatter(y = hourly_avg_by_month['1'], x = hourly_avg_by_month.index, \n              mode = 'lines', legendrank = 1, \n              name = 'January', line=dict(color='#DC050C', width=2)),\n    go.Scatter(y = hourly_avg_by_month['2'], x = hourly_avg_by_month.index,\n              mode = 'lines+markers', legendrank = 2, \n              name = 'February', line=dict(color='#E8601c', width=2)),\n    go.Scatter(y = hourly_avg_by_month['3'], x = hourly_avg_by_month.index, \n              mode = 'lines', legendrank = 3, \n              name = 'March', line=dict(color='#f4a736', width=2)),\n    go.Scatter(y = hourly_avg_by_month['4'], x = hourly_avg_by_month.index, \n              mode = 'lines+markers', legendrank = 4, \n              name = 'April', line=dict(color='#f7f056', width=2)),\n    go.Scatter(y = hourly_avg_by_month['5'], x = hourly_avg_by_month.index, \n              mode = 'lines', legendrank = 5, \n              name = 'May', line=dict(color='#cae0ab', width=2)),\n    go.Scatter(y = hourly_avg_by_month['6'], x = hourly_avg_by_month.index, \n              mode = 'lines+markers', legendrank = 6, \n              name = 'June', line=dict(color='#4eb265', width=2)),\n    go.Scatter(y = hourly_avg_by_month['7'], x = hourly_avg_by_month.index, \n              mode = 'lines', legendrank = 7, \n              name = 'July', line=dict(color='#7bafde', width=2)),\n    go.Scatter(y = hourly_avg_by_month['8'], x = hourly_avg_by_month.index, \n              mode = 'lines+markers', legendrank = 8, \n              name = 'August', line=dict(color='#5289c7', width=2)),\n    go.Scatter(y = hourly_avg_by_month['9'], x = hourly_avg_by_month.index, \n              mode = 'lines', legendrank = 9, \n              name = 'September', line=dict(color='#1965b0', width=2)),\n    go.Scatter(y = hourly_avg_by_month['10'], x = hourly_avg_by_month.index, \n              mode = 'lines+markers', legendrank = 10, \n              name = 'October', line=dict(color='#882e72', width=2)),\n    go.Scatter(y = hourly_avg_by_month['11'], x = hourly_avg_by_month.index, \n              mode = 'lines', legendrank = 11, \n              name = 'November', line=dict(color='#ae76a3', width=2)),\n    go.Scatter(y = hourly_avg_by_month['12'], x = hourly_avg_by_month.index, \n              mode = 'lines+markers', legendrank = 12, \n              name = 'December', line=dict(color='#d1bbd7', width=2)),\n    go.Scatter(name = 'annual mean', y = hourly_avg['mean'], x = hourly_avg['hour'], mode = 'lines',\n              line = dict(color = \"black\", width = 5))\n\n])\n\nvariables_to_hide = ['February', 'March', 'April', 'May', 'June', 'July',\n                    'August', 'September', 'October', 'November', 'December']\nfig.for_each_trace(lambda trace: trace.update(visible=\"legendonly\") \n                   if trace.name in variables_to_hide else ())\n                   \nfig.update_layout(\n    xaxis=dict(\n        title_text=\"hour (UTC)\",\n        titlefont=dict(size=16),\n        dtick = 4),\n    yaxis=dict(\n        title_text=\"windspeed (m/s)\",\n        titlefont=dict(size=16)),\n    title={\n        'text': \"Average Hourly Wind Speed by Month\",\n        'y':0.99,\n        'x':0.5,\n        'xanchor': 'center',\n        'yanchor': 'top'},\n    margin=dict(l=20, r=20, t=30, b=20),\n    paper_bgcolor=\"#FFFFFF\",\n    plot_bgcolor='#f5f5f5'\n)"
  },
  {
    "objectID": "presentations/wind_resource_presentation.html#diurnal-and-monthly-variability-3",
    "href": "presentations/wind_resource_presentation.html#diurnal-and-monthly-variability-3",
    "title": "Wind Resource Temporal Variability",
    "section": "Diurnal and Monthly Variability",
    "text": "Diurnal and Monthly Variability\n\ngraphcode\n\n\n\n\n\n                        \n                                            \n\n\n\n\n\nheatmap_month = hourly_avg_by_month.columns.tolist()\nheatmap_hour = hourly_avg_by_month.index.tolist()\nheatmap_windspeed = hourly_avg_by_month.values.tolist()\n\ntrace = go.Heatmap(\n   x = heatmap_month,\n   y = heatmap_hour,\n   z = heatmap_windspeed,\n   type = 'heatmap',\n   #colorscale = [(0,\"blue\"), (1,\"red\")],\n   colorscale = 'mint',\n   colorbar=dict(title='Wind Speed (m/s)')\n)\ndata = [trace]\nfig = go.Figure(data = data)\n\nfig.update_layout(\n    #width=1000,\n    height=650,\n    xaxis=dict(\n        title_text=\"month\",\n        titlefont=dict(size=16),\n        #dtick = 1,\n        tickmode = 'array',\n        # Set tick intervals to correspond with months\n        tickvals = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12],\n        ticktext = ['January', 'February', 'March', 'April', \n                    'May', 'June', 'July', 'August', \n                    'September', 'October', 'November', 'December'],\n        tickfont = dict(size=16)),\n    yaxis=dict(\n        title_text=\"hour (UTC)\",\n        titlefont=dict(size=16),\n        dtick = 1,\n        tickfont = dict(size=16)),\n    title={\n        'text': \"Average Wind Speed by Month and Hour\",\n        'y':0.99,\n        'x':0.5,\n        'xanchor': 'center',\n        'yanchor': 'top'},\n    margin=dict(l=20, r=20, t=30, b=20),\n)"
  },
  {
    "objectID": "presentations/wind_resource_presentation.html#standard-deviation",
    "href": "presentations/wind_resource_presentation.html#standard-deviation",
    "title": "Wind Resource Temporal Variability",
    "section": "Standard Deviation",
    "text": "Standard Deviation\n\ngraphcode\n\n\n\n\n\n                        \n                                            \n\n\n\n\n\nstd_heatmap_month = hourly_std_by_month.columns.tolist()\nstd_heatmap_hour = hourly_std_by_month.index.tolist()\nstd_heatmap_windspeed = hourly_std_by_month.values.tolist()\n\ntrace = go.Heatmap(\n   x = std_heatmap_month,\n   y = std_heatmap_hour,\n   z = std_heatmap_windspeed,\n   type = 'heatmap',\n   colorscale = 'Blues',\n   colorbar=dict(title='Standard Deviation (m/s)')\n)\ndata = [trace]\nfig = go.Figure(data = data)\n\nfig.update_layout(\n    #width=1000,\n    height=650,\n    xaxis=dict(\n        titlefont=dict(size=16),\n        #dtick = 1,\n        tickmode = 'array',\n        # Set tick intervals to correspond with months\n        tickvals = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12],\n        ticktext = ['January', 'February', 'March', 'April', \n                    'May', 'June', 'July', 'August', \n                    'September', 'October', 'November', 'December'],\n        tickfont = dict(size=16)),\n    yaxis=dict(\n        title_text=\"hour (UTC)\",\n        titlefont=dict(size=16),\n        dtick = 1,\n        tickfont = dict(size=16)),\n    title={\n        'text': \"Wind Speed Standard Deviation by Month and Hour\",\n        'y':0.99,\n        'x':0.5,\n        'xanchor': 'center',\n        'yanchor': 'top'},\n    margin=dict(l=20, r=20, t=30, b=20),\n)"
  },
  {
    "objectID": "presentations/wind_resource_presentation.html#statistical-summary",
    "href": "presentations/wind_resource_presentation.html#statistical-summary",
    "title": "Wind Resource Temporal Variability",
    "section": "Statistical Summary",
    "text": "Statistical Summary\n\n\n\n\nmin wind speed (m/s)\nmax wind speed (m/s)\n\n\n\n\nhourly\n0.1\n36.7\n\n\nhourly average\n10.7\n12.5\n\n\nmonthly average\n8.8\n15.8\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\nsmallest variability\ngreatest variability\nsmallest average wind speed\ngreatest average wind speed\n\n\n\n\nmonthly\nJuly\nNovember\nAugust\nFebruary\n\n\nhourly\n19\n13\n16\n1"
  },
  {
    "objectID": "presentations/wind_resource_presentation.html#conclusions",
    "href": "presentations/wind_resource_presentation.html#conclusions",
    "title": "Wind Resource Temporal Variability",
    "section": "Conclusions",
    "text": "Conclusions\n\nGreatest monthly variability in November\nSmallest monthly variability in July\nHighest wind speeds in February\nLowest wind speeds in August\nSlower wind speeds mid-day than at night\nBased on the seasonal variability, this site would be better at meeting high winter demands than summer demands\nThis site may not be ideal for meeting all daytime demands."
  },
  {
    "objectID": "presentations/wind_resource_presentation.html#expanded-geographic-scale",
    "href": "presentations/wind_resource_presentation.html#expanded-geographic-scale",
    "title": "Wind Resource Temporal Variability",
    "section": "Expanded Geographic Scale",
    "text": "Expanded Geographic Scale\nCreated a parameterized report using Quarto as a tool to allow users to generate summary reports based on specified inputs from CLI or with render function for multiple sites.\n\n\nModify by specifying parameters for:\n\nsite name\nsite latitude\nsite longitude\nstart date\nend date\nturbine cut-in speed\nturbine cut-out speed\nrequired annual average wind speed\n\n\nDefault parameters are:\n\nstart date: ‘2012-01-01’\nend date: ‘2013-01-01’\ncut-in speed: 3.6 m/s\ncut-out speed: 24.6 m/s\nrequired annual avg speed: 5.8 m/s"
  },
  {
    "objectID": "presentations/wind_resource_presentation.html#expanded-geographic-scale-1",
    "href": "presentations/wind_resource_presentation.html#expanded-geographic-scale-1",
    "title": "Wind Resource Temporal Variability",
    "section": "Expanded Geographic Scale",
    "text": "Expanded Geographic Scale\nIn the CLI the example code below renders a report for NYC in 2010 using default values for cut-in speed, cut-out speed, and required annual average wind speed.\n\nquarto render report.qmd -P site_name:“New York City” -P site_lat:40.7128 -P site_lon:-74.0059 -P start_date:2010-01-01 -P end_date:2011-01-01 --output new_york_city_report.pdf (>)\n\nThis function generates multiples reports from a dataframe of parameters for different sites.\n\nrender_fun <- function(param_df){\n  quarto::quarto_render(\n    input = \"report.qmd\",\n    execute_params = list(site_name = param_df$site_name,\n                          site_lat = param_df$site_lat,\n                          site_lon = param_df$site_lon,\n                          start_date = param_df$start_date,\n                          end_date = param_df$end_date),\n    output_file = glue::glue(\"{param_df$site_name}-report.pdf\"))}\n\nparam_list <- split(report_parameters, seq(nrow(report_parameters))) %>% \n  purrr::walk(render_fun)"
  },
  {
    "objectID": "presentations/wind_resource_presentation.html#report",
    "href": "presentations/wind_resource_presentation.html#report",
    "title": "Wind Resource Temporal Variability",
    "section": "Report",
    "text": "Report\nFor the specified site, the report answers the following questions:\n\nIs the annual average wind speed at least 13 mph (5.8 m/s)? 1\nHow often the wind is below the cut-in speed of 8 mph (3.6 m/s)? 2\nHow often the wind exceed the cut-out speed of 55 mph (24.6 m/s)?\n\nUniversity of Delaware\nUMass Amherst\nUC Santa Barbara\nNREL - Golden, CO\nThe U.S. Energy Information Administration recommends an annual average wind speed of at least 9 mph (4 m/s) for small wind turbines and 13 mph (5.8 m/s) for utility-scale turbines. https://www.eia.gov/energyexplained/wind/where-wind-power-is-harnessed.php#:~:text=Good%20places%20for%20wind%20turbines,)%20for%20utility%2Dscale%20turbines.The Office of Energy Efficiency & Renewable Energy notes a typical cut-in speed of 6 to 9 mpg and cut-out speed of 55 mph. https://www.energy.gov/eere/articles/how-do-wind-turbines-survive-severe-storms"
  },
  {
    "objectID": "presentations/wind_resource_presentation.html#citations",
    "href": "presentations/wind_resource_presentation.html#citations",
    "title": "Wind Resource Temporal Variability",
    "section": "Citations",
    "text": "Citations\nDraxl, C., B.M. Hodge, A. Clifton, and J. McCaa. 2015. Overview and Meteorological Validation of the Wind Integration National Dataset Toolkit (Technical Report, NREL/TP-5000-61740). Golden, CO: National Renewable Energy Laboratory.\nDraxl, C., B.M. Hodge, A. Clifton, and J. McCaa. 2015. “The Wind Integration National Dataset (WIND) Toolkit.” Applied Energy 151: 355366.\nKing, J., A. Clifton, and B.M. Hodge. 2014. Validation of Power Output for the WIND Toolkit (Technical Report, NREL/TP-5D00-61714). Golden, CO: National Renewable Energy Laboratory.\nhttps://www.eia.gov/electricity/gridmonitor/dashboard/electric_overview/US48/US48\nhttps://www.eia.gov/energyexplained/wind/where-wind-power-is-harnessed.php#:~:text=Good%20places%20for%20wind%20turbines,)%20for%20utility%2Dscale%20turbines.\n\n\nhttps://marierivers.github.io/wind_resource_temporal_variability/"
  },
  {
    "objectID": "presentations/power_outage_presentation.html#background",
    "href": "presentations/power_outage_presentation.html#background",
    "title": "Houston Power Outages",
    "section": "Background",
    "text": "Background\n\n\nIn February 2021 the Houston, TX metropolitan area experienced wide scale power outages due to electrical infrastructure failure during winter storms and extreme cold temperatures\n\n\n\n\n1.4 million customers without power"
  },
  {
    "objectID": "presentations/power_outage_presentation.html#approach",
    "href": "presentations/power_outage_presentation.html#approach",
    "title": "Houston Power Outages",
    "section": "Approach",
    "text": "Approach\nUse geospatial and statistical methods to quantify:\n\nNumber of residential homes without power\nSocioeconomic differences of areas with and without power\n\n…by using data from:\n\nSatellite imagery of nighttime lights\nOpenStreetMaps roadways and buidings\nCensus tract level race, age and income variables"
  },
  {
    "objectID": "presentations/power_outage_presentation.html#project-overview",
    "href": "presentations/power_outage_presentation.html#project-overview",
    "title": "Houston Power Outages",
    "section": "Project overview",
    "text": "Project overview\n\n\n\n\nLoad satellite imagery\n\n\n\n\nCreate blackout mask\n\n\n\n\nIdentify residential buildings within blackout area\n\n\n\n\nSpatially join census data to blackout areas\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n+144,000 households without power"
  },
  {
    "objectID": "presentations/power_outage_presentation.html#section",
    "href": "presentations/power_outage_presentation.html#section",
    "title": "Houston Power Outages",
    "section": "",
    "text": "Percent white\n\n\n\n\n\nPercent bipoc"
  },
  {
    "objectID": "presentations/power_outage_presentation.html#statistical-analysis",
    "href": "presentations/power_outage_presentation.html#statistical-analysis",
    "title": "Houston Power Outages",
    "section": "Statistical Analysis",
    "text": "Statistical Analysis\nLinear regression models of percent of households without power vs. census variables\n\n\n\n\n\n\n\nRace\n\npercent white\npercent black\npercent Native American\npercent Asian\npercent Hispanic / Latino\n\n\n\nAge\n\n65 and older\nchildren under 18\n\n\n\nIncome\n\npercent households below poverty\nmedian income"
  },
  {
    "objectID": "presentations/power_outage_presentation.html#statistical-analysis-1",
    "href": "presentations/power_outage_presentation.html#statistical-analysis-1",
    "title": "Houston Power Outages",
    "section": "Statistical Analysis",
    "text": "Statistical Analysis\n\n\ngraph\ncode\n\n\n\n\n\n\n\n\n\n\n\n# linear regression model\nmodel_pct_white <- lm(data = blackout_census_data, pct_houses_that_lost_power ~ pct_white)\n\n#plot\nplot_model_pct_white <- ggplot(data = blackout_census_data, aes(x = pct_white, y = pct_houses_that_lost_power)) +\n  geom_point(size = 0.5) +\n  geom_smooth(method = lm, formula = y~x, se = FALSE) +\n  theme_classic() +\n  labs(x = \"% white\", y = \"% of houses that lost power\",\n       title = \"Linear regression of % households without power vs. % population white\")\nplot_model_pct_white"
  },
  {
    "objectID": "presentations/power_outage_presentation.html#statistical-analysis-2",
    "href": "presentations/power_outage_presentation.html#statistical-analysis-2",
    "title": "Houston Power Outages",
    "section": "Statistical Analysis",
    "text": "Statistical Analysis\n\n\ngraph\ncode\n\n\n\n\n\n\n\n\n\n\n\n# linear regression model\nmodel_pct_black <- lm(data = blackout_census_data, pct_houses_that_lost_power ~ pct_black)\n\n# plot\nplot_model_pct_black <- ggplot(data = blackout_census_data, aes(x = pct_black, y = pct_houses_that_lost_power)) +\n  geom_point(size = 0.5) +\n  geom_smooth(method = lm, formula = y~x, se = FALSE) +\n  theme_classic() +\n  labs(x = \"% black\", y = \"% of houses that lost power\",\n       title = \"Linear regression of % households without power vs. % population black\")\nplot_model_pct_black"
  },
  {
    "objectID": "presentations/power_outage_presentation.html#results",
    "href": "presentations/power_outage_presentation.html#results",
    "title": "Houston Power Outages",
    "section": "Results",
    "text": "Results"
  },
  {
    "objectID": "presentations/power_outage_presentation.html#conclusions",
    "href": "presentations/power_outage_presentation.html#conclusions",
    "title": "Houston Power Outages",
    "section": "Conclusions",
    "text": "Conclusions\n\n\n\nWhile race, age and income accounted for small portions of the overall variance in residential power outages, this analysis suggests some racial and economic inequality.\nElectric utilities should evaluate infrastructure and asset management plans in areas with higher proportions of people of color and poverty\nThere is a need for more equitable responses to natural disasters\n\n\n image source1\n\n\n\n\nhttps://marierivers.github.io/blackout_analysis/\n\nhttps://appliedsciences.nasa.gov/our-impact/news/extreme-winter-weather-causes-us-blackouts"
  },
  {
    "objectID": "presentations/snow_today_presentation.html#section",
    "href": "presentations/snow_today_presentation.html#section",
    "title": "Snow Today",
    "section": "",
    "text": "Knowing the spatial extent of snow cover is critical for water management and winter recreation. Climate change will affect the variability of frozen water resources."
  },
  {
    "objectID": "presentations/snow_today_presentation.html#section-1",
    "href": "presentations/snow_today_presentation.html#section-1",
    "title": "Snow Today",
    "section": "",
    "text": "Snow Science: UCSB CUES Field Station Site Visit"
  },
  {
    "objectID": "presentations/snow_today_presentation.html#albedo-importance",
    "href": "presentations/snow_today_presentation.html#albedo-importance",
    "title": "Snow Today",
    "section": "Albedo Importance",
    "text": "Albedo Importance\n\n\n\nRegulates the Earth’s temperature by reflecting solar radiation  \nInfluences rate of snow melt  \nParticularly important in the Western US  \nAccurate estimates critical for climate models and predicting water storage"
  },
  {
    "objectID": "presentations/snow_today_presentation.html#snow-today",
    "href": "presentations/snow_today_presentation.html#snow-today",
    "title": "Snow Today",
    "section": "Snow Today",
    "text": "Snow Today\n\n\n\nScientific analysis website that provides data on snow conditions from satellite and surface measurements  \nUsed by scientists, water managers, and outdoor enthusiasts for snow observations  \nSpatial products offered include measures of snow cover extent and albedo"
  },
  {
    "objectID": "presentations/snow_today_presentation.html#snow-today-usability",
    "href": "presentations/snow_today_presentation.html#snow-today-usability",
    "title": "Snow Today",
    "section": "Snow Today: Usability",
    "text": "Snow Today: Usability\n\n\n\n\nSnow Today can be hard to navigate for new users unfamiliar with the website’s layout\nVisualizations are of current snow conditions, and have limited customization options\nSnow cover and albedo files are hard to find\nData format may be challenging for new users\nSnow metadata is stored in a non-standardized format which is difficult for some software to interpret the data\nUsers may have trouble processing and analyzing snow data without the help"
  },
  {
    "objectID": "presentations/snow_today_presentation.html#snow-today-visualizations",
    "href": "presentations/snow_today_presentation.html#snow-today-visualizations",
    "title": "Snow Today",
    "section": "Snow Today: Visualizations",
    "text": "Snow Today: Visualizations"
  },
  {
    "objectID": "presentations/snow_today_presentation.html#objectives",
    "href": "presentations/snow_today_presentation.html#objectives",
    "title": "Snow Today",
    "section": "Objectives",
    "text": "Objectives\n\n\n\n\nCreate an open source workflow for processing and visualizing snow data\n\n\n\nProvide recommendations for the Snow Today website\n\n\n\n\n\nCreate interactive visualizations\n\n\n\n\nImprove data usability through tutorials in Python"
  },
  {
    "objectID": "presentations/snow_today_presentation.html#website-recommendations",
    "href": "presentations/snow_today_presentation.html#website-recommendations",
    "title": "Snow Today",
    "section": "Website Recommendations",
    "text": "Website Recommendations\nwebsite architecture"
  },
  {
    "objectID": "presentations/snow_today_presentation.html#website-recommendations-1",
    "href": "presentations/snow_today_presentation.html#website-recommendations-1",
    "title": "Snow Today",
    "section": "Website Recommendations",
    "text": "Website Recommendations"
  },
  {
    "objectID": "presentations/snow_today_presentation.html#objectives-1",
    "href": "presentations/snow_today_presentation.html#objectives-1",
    "title": "Snow Today",
    "section": "Objectives",
    "text": "Objectives\n\n\n\n\nCreate an open source workflow for processing and visualizing snow data\n\nProvide recommendations for the Snow Today website\n\n2. Create interactive visualizations\n\nImprove data usability through tutorials in Python"
  },
  {
    "objectID": "presentations/snow_today_presentation.html#interactive-visualizations",
    "href": "presentations/snow_today_presentation.html#interactive-visualizations",
    "title": "Snow Today",
    "section": "Interactive Visualizations",
    "text": "Interactive Visualizations\n\n\nPrototype Web Application\n  Daily maps of snow cover and albedo for any selected date"
  },
  {
    "objectID": "presentations/snow_today_presentation.html#interactive-visualizations-1",
    "href": "presentations/snow_today_presentation.html#interactive-visualizations-1",
    "title": "Snow Today",
    "section": "Interactive Visualizations",
    "text": "Interactive Visualizations\n\n\nMonthly Average and Anomaly\n  Users can select a specific month, water year, and variable to view averages on anomalies."
  },
  {
    "objectID": "presentations/snow_today_presentation.html#interactive-visualizations-2",
    "href": "presentations/snow_today_presentation.html#interactive-visualizations-2",
    "title": "Snow Today",
    "section": "Interactive Visualizations",
    "text": "Interactive Visualizations\nAnnual Comparisons"
  },
  {
    "objectID": "presentations/snow_today_presentation.html#interactive-visualizations-3",
    "href": "presentations/snow_today_presentation.html#interactive-visualizations-3",
    "title": "Snow Today",
    "section": "Interactive Visualizations",
    "text": "Interactive Visualizations\n\ngraphcode\n\n\n\n\n                        \n                                            \n\n\n\n\n\nsnow_cover_df = pd.read_csv('data/snow_cover_df.csv')\nsnow_cover_df = snow_cover_df.fillna(0)\n\n# Create empty list to input with for loop\nIQR_25 = []\nIQR_75 = []\nIQR_50 = []\ndays = []\nfor i in range(len(snow_cover_df)): \n    #Takes the IQR of each day (25, 50, 75)\n    Q1 = np.percentile(snow_cover_df.iloc[i], 25)\n    Q2 = np.percentile(snow_cover_df.iloc[i], 50)\n    Q3 = np.percentile(snow_cover_df.iloc[i], 75)\n    #appends list with IQR outputs\n    IQR_25.append(Q1)\n    IQR_50.append(Q2)\n    IQR_75.append(Q3)\n    #Creates day list to append dataset with\n    days.append(i + 1)\n    \n# Next, need to create a single column of mean values. \nsnow_cover_df['Average Snow Cover'] = snow_cover_df.mean(axis = 1)\n\n#Appends list for loop lists\nsnow_cover_df['IQR_25'] = IQR_25\nsnow_cover_df['IQR_75'] = IQR_75\nsnow_cover_df['IQR_50'] = IQR_50\nsnow_cover_df['days'] = days\n\nmonth_day = [31, 30, 31, 31, 28, 31, 30, 31, 30, 31, 31, 30]\nnew_list = []\n\nj = 0 \nfor i in range(0,len(month_day)):\n    j+=month_day[i]\n    new_list.append(j)\n    \n# Create a list of years to graph. legend rank allows lets you order where the lines are located on the chart. \nfor i in range(len(snow_cover_df)):\n    print(\"\"\"go.Scatter(\"\"\"\n        \"\"\"name = '\"\"\" + str(i + 2001) + \"\"\"', \"\"\"\n        \"\"\"y = snow_cover_df['\"\"\"+ str(i + 2001) + \"\"\"'], x = snow_cover_df['days'], \"\"\"\n        \"\"\"mode = 'lines', legendrank = \"\"\" + str(19-i) + \"\"\"),\"\"\"\n    )\n#Plot the figure. \nfig = go.Figure([\n\n#create median line\ngo.Scatter(\n    #Name that appears on legend\n    name = 'Median',\n    # y-dim\n    y = snow_cover_df['IQR_50'],\n    # x-dim\n    x = snow_cover_df['days'],\n    # type of plot\n    mode = 'lines',\n    # Include to select/deselect multiple variables at once\n    legendgroup = 'IQR',\n    # Name of legend group on legend\n    legendgrouptitle_text=\"<b>Interquartile Range</b>\",\n    # Legend position\n    legendrank = 20,\n    # Line color\n    line=dict(color='rgb(31, 119, 180)'),\n),\n#Create IQR 75 line\ngo.Scatter(\n        name = 'IQR 75', y = snow_cover_df['IQR_75'], x = snow_cover_df['days'],\n        mode='lines', marker=dict(color=\"#444\"), line=dict(width=0),\n        legendgroup = 'IQR', showlegend = False\n        # Here we 'hide' the name from appearing on the legend since it's lumped in with the legendgroup 'IQR'\n    ),\n    #Create IQR 25 fill color\n    go.Scatter(\n        name='IQR 25', y = snow_cover_df['IQR_25'], x = snow_cover_df['days'],\n        marker=dict(color=\"#444\"), line=dict(width=0),  mode='lines',\n        fillcolor='rgba(68, 68, 68, 0.3)', fill='tonexty',\n        legendgroup = 'IQR', showlegend = False\n    ),\n    #Create mean line\n    go.Scatter(\n        name = 'Average Snow Cover',  y = snow_cover_df['Average Snow Cover'], x = snow_cover_df['days'],\n        mode = 'lines', legendgroup = 'Average',\n        legendgrouptitle_text = '<b>Average</b>', legendrank = 21\n    ),\n#Create lines for each respective year\ngo.Scatter(name = '2001', y = snow_cover_df['2001'], x = snow_cover_df['days'], mode = 'lines', legendrank = 19),\ngo.Scatter(name = '2002', y = snow_cover_df['2002'], x = snow_cover_df['days'], mode = 'lines', legendrank = 18),\ngo.Scatter(name = '2003', y = snow_cover_df['2003'], x = snow_cover_df['days'], mode = 'lines', legendrank = 17),\ngo.Scatter(name = '2004', y = snow_cover_df['2004'], x = snow_cover_df['days'], mode = 'lines', legendrank = 16),\ngo.Scatter(name = '2005', y = snow_cover_df['2005'], x = snow_cover_df['days'], mode = 'lines', legendrank = 15),\ngo.Scatter(name = '2006', y = snow_cover_df['2006'], x = snow_cover_df['days'], mode = 'lines', legendrank = 14),\ngo.Scatter(name = '2007', y = snow_cover_df['2007'], x = snow_cover_df['days'], mode = 'lines', legendrank = 13),\ngo.Scatter(name = '2008', y = snow_cover_df['2008'], x = snow_cover_df['days'], mode = 'lines', legendrank = 12),\ngo.Scatter(name = '2009', y = snow_cover_df['2009'], x = snow_cover_df['days'], mode = 'lines', legendrank = 11),\ngo.Scatter(name = '2010', y = snow_cover_df['2010'], x = snow_cover_df['days'], mode = 'lines', legendrank = 10),\ngo.Scatter(name = '2011', y = snow_cover_df['2011'], x = snow_cover_df['days'], mode = 'lines', legendrank = 9),\ngo.Scatter(name = '2012', y = snow_cover_df['2012'], x = snow_cover_df['days'], mode = 'lines', legendrank = 8),\ngo.Scatter(name = '2013', y = snow_cover_df['2013'], x = snow_cover_df['days'], mode = 'lines', legendrank = 7),\ngo.Scatter(name = '2014', y = snow_cover_df['2014'], x = snow_cover_df['days'], mode = 'lines', legendrank = 6),\ngo.Scatter(name = '2015', y = snow_cover_df['2015'], x = snow_cover_df['days'], mode = 'lines', legendrank = 5),\ngo.Scatter(name = '2016', y = snow_cover_df['2016'], x = snow_cover_df['days'], mode = 'lines', legendrank = 4),\ngo.Scatter(name = '2017', y = snow_cover_df['2017'], x = snow_cover_df['days'], mode = 'lines', legendrank = 3),\ngo.Scatter(name = '2018', y = snow_cover_df['2018'], x = snow_cover_df['days'], mode = 'lines', legendrank = 2),\ngo.Scatter(name = '2019', y = snow_cover_df['2019'], x = snow_cover_df['days'], mode = 'lines', legendrank = 1)\n\n])\n# Can change default \"off\" variables. Right now, the only variable visible is year_2019 and IQR\nvariables_to_hide = ['2001', '2002', '2003', '2004', '2005', '2006', '2007', \n'2008', '2009', '2010', '2011', '2012', '2013', '2014', '2015', '2016', '2017', '2018',\n'Average Snow Cover']\nfig.for_each_trace(lambda trace: trace.update(visible=\"legendonly\") \n                   if trace.name in variables_to_hide else ())\nfig.update_layout(\n    title = \"<b> Annual Snow Cover Area: Sierra Nevada Region </b> <br> <sup>2001-2019</sup></br>\",\n    legend_title=\"<b>Year</b>\",\n    autosize=False,\n    width=1200,\n    height=700,\n    template = 'none',\n    font=dict(\n        size=16),\nxaxis = dict(\n        tickmode = 'array',\n        tickvals = [1, 31, 61, 92, 123, 151, 182, 212, 243, 273, 304, 335, 365],\n        ticktext = ['<b>October</b>', '<b>November</b>', '<b>December</b>', '<b>January</b>', '<b>February</b>', '<b>March</b>', '<b>April</b>', '<b>May</b>', \n        '<b>June</b>', '<b>July', '<b>August</b>', \"<b>September</b>\", \"<b>October</b>\"],\n        tickfont = dict(size=12))\n)\n\nfig.update_xaxes(title_text = \"\", gridcolor = 'lightgrey', gridwidth = 0.1)\nfig.update_yaxes(title_text = \"<b> Area (Thousands of Square Kilometers) </b>\", \n    title_font = {\"size\": 15}, gridcolor = 'lightgrey', gridwidth = 0.1)"
  },
  {
    "objectID": "presentations/snow_today_presentation.html#objectives-2",
    "href": "presentations/snow_today_presentation.html#objectives-2",
    "title": "Snow Today",
    "section": "Objectives",
    "text": "Objectives\n\n\n\n\nCreate an open source workflow for processing and visualizing snow data\n\nProvide recommendations for the Snow Today website\nCreate interactive visualizations\n\n3. Improve data usability through tutorials in Python"
  },
  {
    "objectID": "presentations/snow_today_presentation.html#tutorials",
    "href": "presentations/snow_today_presentation.html#tutorials",
    "title": "Snow Today",
    "section": "Tutorials",
    "text": "Tutorials\n\n\n 1. Download and Explore Datasets     2. Process and Format Data     3. Analyze and Visualize Snow Data"
  },
  {
    "objectID": "presentations/snow_today_presentation.html#tutorials-1",
    "href": "presentations/snow_today_presentation.html#tutorials-1",
    "title": "Snow Today",
    "section": "Tutorials",
    "text": "Tutorials\n\n\n 1. Download and Explore Datasets     2. Process and Format Data     3. Analyze and Visualize Snow Data\n\n\nDownload snow cover and albedo datasets\nOpen datasets and view metadata\nCreate basic visualizations of each dataset"
  },
  {
    "objectID": "presentations/snow_today_presentation.html#tutorials-2",
    "href": "presentations/snow_today_presentation.html#tutorials-2",
    "title": "Snow Today",
    "section": "Tutorials",
    "text": "Tutorials\n\n\n 1. Download and Explore Datasets     2. Process and Format Data     3. Analyze and Visualize Snow Data\n\n\nProcess and subset datasets\nCalculate monthly and yearly snow cover and albedo averages and anomalies\nCreate interactive maps of processed data\nConvert processed data to GeoTiff and NetCDF formats"
  },
  {
    "objectID": "presentations/snow_today_presentation.html#tutorials-3",
    "href": "presentations/snow_today_presentation.html#tutorials-3",
    "title": "Snow Today",
    "section": "Tutorials",
    "text": "Tutorials\n\n\n 1. Download and Explore Datasets     2. Process and Format Data     3. Analyze and Visualize Snow Data\n\n\nCalculate total snow cover area and average albedo for entire spatial domain\nPerform basic statistical analysis of datasets\nDevelop interactive charts to compare snow cover area and albedo percentages for each water year"
  },
  {
    "objectID": "presentations/snow_today_presentation.html#contributions",
    "href": "presentations/snow_today_presentation.html#contributions",
    "title": "Snow Today",
    "section": "Contributions",
    "text": "Contributions"
  },
  {
    "objectID": "presentations/snow_today_presentation.html#section-3",
    "href": "presentations/snow_today_presentation.html#section-3",
    "title": "Snow Today",
    "section": "",
    "text": "As water resources become harder to manage due to climate change, implementing these tools will open a valuable dataset to a wider audience"
  },
  {
    "objectID": "presentations/snow_today_presentation.html#acknowledgements",
    "href": "presentations/snow_today_presentation.html#acknowledgements",
    "title": "Snow Today",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nFaculty Advisors\nSam Stevenson, UCSB Bren School\nAllison Horst, UCSB Bren School\nClients\nTimbo Stillinger, UCSB Earth Research Institute\nNed Bair, UCSB Earth Research Institute\nKarl Rittger, CU Boulder Institute of Arctic & Alpine Research\nExternal Advisors\nJames Frew, UCSB Bren School\nNiklas Griessbaum, UCSB Bren School\nKat Le, UCSB Bren School\nMichael Colee, UCSB Geography & Earth Research Institute\nBren School Faculty and Staff and the MEDS 2022 cohort\n\n\nhttps://shiny.snow.ucsb.edu/snow_today_shiny_app/>"
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Posts",
    "section": "",
    "text": "The Importance of Meaningfully Open Snow Data\n\n\n\n\n\n\n\nopen science\n\n\n\n\nCommunicating climate and water resource data to a wider audience\n\n\n\n\n\n\nJun 7, 2022\n\n\nMarie Rivers\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow to make a dumbbell schedule with R\n\n\n\n\n\n\n\nR\n\n\ndata visualization\n\n\n\n\nA modern update to the Gantt chart\n\n\n\n\n\n\nFeb 8, 2022\n\n\nMarie Rivers\n\n\n\n\n\n\n  \n\n\n\n\nDoes Environmental Quality Influence Where People Live?\n\n\n\n\n\n\n\nR\n\n\n\n\nPopulation Change vs. the EPA Environmental Quality Index (EQI): a statistical analysis\n\n\n\n\n\n\nDec 2, 2021\n\n\nMarie Rivers\n\n\n\n\n\n\n  \n\n\n\n\nHow I made this visualization\n\n\n\n\n\n\n\nR\n\n\ndata visualization\n\n\n\n\nVisualization of Alaska household languages data\n\n\n\n\n\n\nNov 3, 2021\n\n\nMarie Rivers\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDo you learn about the chicken or the egg first\n\n\n\n\n\n\n\nPython\n\n\nR\n\n\nMEDS\n\n\n\n\nIf you are new to coding, what first steps should you take?\n\n\n\n\n\n\nOct 17, 2021\n\n\nMarie Rivers\n\n\n\n\n\n\n  \n\n\n\n\nA New Perspective on Data\n\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nMEDS\n\n\n\n\nlife beyond excel spreadsheets\n\n\n\n\n\n\nAug 18, 2012\n\n\nMarie Rivers\n\n\n\n\n\n\nNo matching items"
  }
]